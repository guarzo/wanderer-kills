<repomix><file_summary>This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where content has been formatted for parsing in xml style.<purpose>This file contains a packed representation of the entire repository&apos;s contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.</purpose><file_format>The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file</file_format><usage_guidelines>- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.</usage_guidelines><notes>- Some files may have been excluded based on .gitignore rules and Repomix&apos;s configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*
- Files matching these patterns are excluded: priv/**/*, **/*.svg, .notes/**/*, .cursor/**/*, _build/**/*, feedback.md, test.results
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been formatted for parsing in xml style
- Files are sorted by Git change count (files with more changes are at the bottom)</notes></file_summary><directory_structure>.devcontainer/
  devcontainer.json
  docker-compose.yml
  Dockerfile
.github/
  workflows/
    ci.yml
  CODEOWNERS
config/
  config.exs
  dev.exs
  test.exs
docs/
  api-reference.md
  integration-guide.md
  README.md
lib/
  wanderer_kills/
    behaviours/
      data_fetcher.ex
      esi_client.ex
      http_client.ex
    cache/
      helper.ex
    esi/
      client.ex
      data_fetcher.ex
    http/
      client_provider.ex
      client.ex
    infrastructure/
      batch_processor.ex
      clock.ex
      config.ex
      error.ex
      retry.ex
    killmails/
      coordinator.ex
      enricher.ex
      parser.ex
      store.ex
      zkb_client.ex
    observability/
      health_checks.ex
      monitoring.ex
      telemetry.ex
    ship_types/
      csv.ex
      info.ex
      updater.ex
    application.ex
    client_behaviour.ex
    client.ex
    core.ex
    kill_store.ex
    preloader.ex
    redisq.ex
    subscription_manager.ex
    types.ex
  wanderer_kills_web/
    api/
      helpers.ex
      killfeed_controller.ex
    controllers/
      kills_controller.ex
      subscriptions_controller.ex
    api.ex
  wanderer_kills_web.ex
  wanderer_kills.ex
plans/
  interface.md
  zkb_service_interfaces.md
test/
  external/
    esi_cache_test.exs
  fetcher/
    zkb_service_test.exs
  fixtures/
    .gitkeep
  integration/
    api_helpers_test.exs
    api_smoke_test.exs
    api_test.exs
    cache_migration_test.exs
  killmails/
    store_test.exs
  shared/
    cache_key_test.exs
    cache_test.exs
    csv_test.exs
    http_util_test.exs
  support/
    helpers.ex
  wanderer_kills/
    killmails/
      enricher_test.exs
  test_helper.exs
  wanderer_kills_test.exs
.coderabbit.yaml
.coveralls.exs
.dockerignore
.formatter.exs
.gitignore
docker-compose.yml
DOCKER.md
Dockerfile
mix.exs
README.md
repomix.config.json</directory_structure><files>This section contains the contents of the repository&apos;s files.<file path=".github/CODEOWNERS"># This file defines code ownership and review requirements for the WandererKills project.

# Default owners for everything in the repo
* @project-maintainer

# Core application code
/lib/ @core-team
/test/ @core-team

# Configuration files
/config/ @devops-team
/.github/ @devops-team

# Documentation
/docs/ @documentation-team
*.md @documentation-team

# Docker and deployment
/Dockerfile* @devops-team
/docker-compose* @devops-team

# Dependencies
/mix.exs @core-team
/mix.lock @core-team

# Tests
/test/ @core-team
/spec/ @core-team

# CI/CD
/.github/workflows/ @devops-team</file><file path="docs/api-reference.md"># WandererKills API Reference

## Base URL

```
http://localhost:4004/api/v1
```

## Authentication

No authentication required for current version.

## Endpoints

### Kill Data

| Method | Endpoint                    | Description                 |
| ------ | --------------------------- | --------------------------- |
| GET    | `/kills/system/{system_id}` | Get kills for a system      |
| POST   | `/kills/systems`            | Bulk fetch multiple systems |
| GET    | `/kills/cached/{system_id}` | Get cached kills only       |
| GET    | `/killmail/{killmail_id}`   | Get specific killmail       |
| GET    | `/kills/count/{system_id}`  | Get kill count for system   |

### Subscriptions

| Method | Endpoint                         | Description         |
| ------ | -------------------------------- | ------------------- |
| POST   | `/subscriptions`                 | Create subscription |
| DELETE | `/subscriptions/{subscriber_id}` | Remove subscription |
| GET    | `/subscriptions`                 | List subscriptions  |

### System

| Method | Endpoint  | Description    |
| ------ | --------- | -------------- |
| GET    | `/health` | Health check   |
| GET    | `/status` | Service status |

## Request Parameters

### GET /kills/system/{system_id}

- `since_hours` (required) - Hours to look back
- `limit` (optional) - Max kills to return

### POST /kills/systems

```json
{
  &quot;system_ids&quot;: [30000142, 30000144],
  &quot;since_hours&quot;: 24,
  &quot;limit&quot;: 50
}
```

### POST /subscriptions

```json
{
  &quot;subscriber_id&quot;: &quot;my-service&quot;,
  &quot;system_ids&quot;: [30000142],
  &quot;callback_url&quot;: &quot;https://my-service.com/webhook&quot;
}
```

## Response Format

### Success Response

```json
{
  &quot;data&quot;: { ... },
  &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;,
  &quot;error&quot;: null
}
```

### Error Response

```json
{
  &quot;data&quot;: null,
  &quot;error&quot;: &quot;Error message&quot;,
  &quot;code&quot;: &quot;ERROR_CODE&quot;,
  &quot;details&quot;: { ... },
  &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;
}
```

## Kill Object Structure

```json
{
  &quot;killmail_id&quot;: 123456789,
  &quot;kill_time&quot;: &quot;2024-01-15T14:30:00Z&quot;,
  &quot;solar_system_id&quot;: 30000142,
  &quot;victim&quot;: {
    &quot;character_id&quot;: 987654321,
    &quot;corporation_id&quot;: 123456789,
    &quot;alliance_id&quot;: 456789123,
    &quot;ship_type_id&quot;: 671,
    &quot;damage_taken&quot;: 2847
  },
  &quot;attackers&quot;: [
    {
      &quot;character_id&quot;: 111222333,
      &quot;corporation_id&quot;: 444555666,
      &quot;ship_type_id&quot;: 17918,
      &quot;weapon_type_id&quot;: 2456,
      &quot;damage_done&quot;: 2847,
      &quot;final_blow&quot;: true
    }
  ],
  &quot;zkb&quot;: {
    &quot;location_id&quot;: 50000001,
    &quot;hash&quot;: &quot;abc123def456&quot;,
    &quot;fitted_value&quot;: 150000000.0,
    &quot;total_value&quot;: 152000000.0,
    &quot;points&quot;: 15,
    &quot;npc&quot;: false,
    &quot;solo&quot;: true,
    &quot;awox&quot;: false
  }
}
```

## HTTP Status Codes

| Code | Description    |
| ---- | -------------- |
| 200  | Success        |
| 400  | Bad Request    |
| 404  | Not Found      |
| 429  | Rate Limited   |
| 500  | Internal Error |

## Error Codes

| Code                | Description               |
| ------------------- | ------------------------- |
| `INVALID_PARAMETER` | Invalid request parameter |
| `NOT_FOUND`         | Resource not found        |
| `RATE_LIMITED`      | Rate limit exceeded       |
| `INTERNAL_ERROR`    | Server error              |
| `TIMEOUT`           | Request timeout           |

## Webhook Payload

### Kill Update

```json
{
  &quot;type&quot;: &quot;detailed_kill_update&quot;,
  &quot;data&quot;: {
    &quot;solar_system_id&quot;: 30000142,
    &quot;kills&quot;: [...],
    &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;
  }
}
```

### Count Update

```json
{
  &quot;type&quot;: &quot;kill_count_update&quot;,
  &quot;data&quot;: {
    &quot;solar_system_id&quot;: 30000142,
    &quot;count&quot;: 48,
    &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;
  }
}
```

## PubSub Topics (Elixir Apps)

### Global Topics

- `zkb:kills:updated` - Kill count updates
- `zkb:detailed_kills:updated` - Detailed kill updates

### System-Specific Topics

- `zkb:system:#{system_id}` - All updates for system
- `zkb:system:#{system_id}:detailed` - Detailed kills for system

## Rate Limits

- **Per-IP**: 1000 requests/minute
- **Burst**: 100 requests/10 seconds
- **WebSocket**: 10 connections/IP

## cURL Examples

### Get System Kills

```bash
curl &quot;http://localhost:4004/api/v1/kills/system/30000142?since_hours=24&amp;limit=50&quot;
```

### Bulk Fetch

```bash
curl -X POST http://localhost:4004/api/v1/kills/systems \
  -H &quot;Content-Type: application/json&quot; \
  -d &apos;{&quot;system_ids&quot;:[30000142,30000144],&quot;since_hours&quot;:24,&quot;limit&quot;:50}&apos;
```

### Create Subscription

```bash
curl -X POST http://localhost:4004/api/v1/subscriptions \
  -H &quot;Content-Type: application/json&quot; \
  -d &apos;{&quot;subscriber_id&quot;:&quot;test&quot;,&quot;system_ids&quot;:[30000142],&quot;callback_url&quot;:&quot;https://example.com/hook&quot;}&apos;
```

### Health Check

```bash
curl http://localhost:4004/health
```</file><file path="docs/integration-guide.md"># WandererKills Integration Guide

## Overview

The WandererKills service provides real-time EVE Online killmail data through multiple integration patterns. This guide covers all available integration methods and provides practical examples for consuming the service.

## Quick Start

The service runs on `http://localhost:4004` by default and provides:

- **REST API** - Fetch kill data and manage subscriptions
- **Real-time Updates** - Phoenix PubSub for internal applications
- **Webhooks** - HTTP callbacks for external services
- **Client Library** - Elixir behaviour for direct integration

## Authentication

Currently, the service does not require authentication for read operations. Subscription management endpoints may require API keys in production deployments.

## REST API Integration

### Base URL

```
http://localhost:4004/api/v1
```

### Kill Data Endpoints

#### Fetch System Kills

Get recent kills for a specific solar system.

```http
GET /api/v1/kills/system/{system_id}?since_hours={hours}&amp;limit={limit}
```

**Parameters:**

- `system_id` (required) - EVE Online solar system ID
- `since_hours` (required) - Hours to look back for kills
- `limit` (optional) - Maximum kills to return (default: 100)

**Example Request:**

```bash
curl &quot;http://localhost:4004/api/v1/kills/system/30000142?since_hours=24&amp;limit=50&quot;
```

**Example Response:**

```json
{
  &quot;data&quot;: {
    &quot;kills&quot;: [
      {
        &quot;killmail_id&quot;: 123456789,
        &quot;kill_time&quot;: &quot;2024-01-15T14:30:00Z&quot;,
        &quot;solar_system_id&quot;: 30000142,
        &quot;victim&quot;: {
          &quot;character_id&quot;: 987654321,
          &quot;corporation_id&quot;: 123456789,
          &quot;alliance_id&quot;: 456789123,
          &quot;ship_type_id&quot;: 671,
          &quot;damage_taken&quot;: 2847
        },
        &quot;attackers&quot;: [
          {
            &quot;character_id&quot;: 111222333,
            &quot;corporation_id&quot;: 444555666,
            &quot;ship_type_id&quot;: 17918,
            &quot;weapon_type_id&quot;: 2456,
            &quot;damage_done&quot;: 2847,
            &quot;final_blow&quot;: true
          }
        ],
        &quot;zkb&quot;: {
          &quot;location_id&quot;: 50000001,
          &quot;hash&quot;: &quot;abc123def456&quot;,
          &quot;fitted_value&quot;: 150000000.0,
          &quot;total_value&quot;: 152000000.0,
          &quot;points&quot;: 15,
          &quot;npc&quot;: false,
          &quot;solo&quot;: true,
          &quot;awox&quot;: false
        }
      }
    ],
    &quot;cached&quot;: false
  },
  &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;,
  &quot;error&quot;: null
}
```

#### Bulk Fetch Multiple Systems

Get kills for multiple systems in a single request.

```http
POST /api/v1/kills/systems
Content-Type: application/json

{
  &quot;system_ids&quot;: [30000142, 30000144, 30000145],
  &quot;since_hours&quot;: 24,
  &quot;limit&quot;: 50
}
```

**Example Response:**

```json
{
  &quot;data&quot;: {
    &quot;systems_kills&quot;: {
      &quot;30000142&quot;: [...],
      &quot;30000144&quot;: [...],
      &quot;30000145&quot;: [...]
    }
  },
  &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;,
  &quot;error&quot;: null
}
```

#### Get Cached Kills

Retrieve cached kills without triggering a fresh fetch.

```http
GET /api/v1/kills/cached/{system_id}
```

#### Get Specific Killmail

Fetch details for a specific killmail.

```http
GET /api/v1/killmail/{killmail_id}
```

#### Get System Kill Count

Get the current kill count for a system.

```http
GET /api/v1/kills/count/{system_id}
```

**Example Response:**

```json
{
  &quot;data&quot;: {
    &quot;system_id&quot;: 30000142,
    &quot;count&quot;: 47,
    &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;
  }
}
```

### Subscription Management

#### Create Subscription

Subscribe to real-time updates for specific systems.

```http
POST /api/v1/subscriptions
Content-Type: application/json

{
  &quot;subscriber_id&quot;: &quot;my-service-v1&quot;,
  &quot;system_ids&quot;: [30000142, 30000144],
  &quot;callback_url&quot;: &quot;https://my-service.com/webhooks/kills&quot;
}
```

**Example Response:**

```json
{
  &quot;data&quot;: {
    &quot;subscription_id&quot;: &quot;abc123def456&quot;,
    &quot;status&quot;: &quot;active&quot;
  },
  &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;,
  &quot;error&quot;: null
}
```

#### Remove Subscription

Cancel an existing subscription.

```http
DELETE /api/v1/subscriptions/{subscriber_id}
```

#### List Active Subscriptions

Get all active subscriptions.

```http
GET /api/v1/subscriptions
```

## Webhook Integration

When you create a subscription with a `callback_url`, the service will send HTTP POST requests to your endpoint when new kills are detected.

### Webhook Payload Formats

#### Kill Update Notification

```json
{
  &quot;type&quot;: &quot;detailed_kill_update&quot;,
  &quot;data&quot;: {
    &quot;solar_system_id&quot;: 30000142,
    &quot;kills&quot;: [
      {
        &quot;killmail_id&quot;: 123456789,
        &quot;kill_time&quot;: &quot;2024-01-15T14:30:00Z&quot;,
        &quot;solar_system_id&quot;: 30000142,
        &quot;victim&quot;: {...},
        &quot;attackers&quot;: [...],
        &quot;zkb&quot;: {...}
      }
    ],
    &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;
  }
}
```

#### Kill Count Update

```json
{
  &quot;type&quot;: &quot;kill_count_update&quot;,
  &quot;data&quot;: {
    &quot;solar_system_id&quot;: 30000142,
    &quot;count&quot;: 48,
    &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;
  }
}
```

### Webhook Endpoint Requirements

Your webhook endpoint should:

- Accept HTTP POST requests
- Respond with 2xx status codes for successful processing
- Handle timeouts gracefully (10-second timeout)
- Implement idempotency (same kill may be sent multiple times)

**Example Webhook Handler (Python Flask):**

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route(&apos;/webhooks/kills&apos;, methods=[&apos;POST&apos;])
def handle_kill_webhook():
    data = request.get_json()

    if data[&apos;type&apos;] == &apos;detailed_kill_update&apos;:
        for kill in data[&apos;data&apos;][&apos;kills&apos;]:
            process_kill(kill)
    elif data[&apos;type&apos;] == &apos;kill_count_update&apos;:
        update_system_count(
            data[&apos;data&apos;][&apos;solar_system_id&apos;],
            data[&apos;data&apos;][&apos;count&apos;]
        )

    return jsonify({&apos;status&apos;: &apos;received&apos;}), 200

def process_kill(kill):
    # Your kill processing logic here
    print(f&quot;New kill: {kill[&apos;killmail_id&apos;]} in system {kill[&apos;solar_system_id&apos;]}&quot;)

def update_system_count(system_id, count):
    # Your count update logic here
    print(f&quot;System {system_id} now has {count} kills&quot;)
```

## Real-time Integration (Elixir Applications)

For Elixir applications running in the same environment, you can subscribe directly to Phoenix PubSub topics.

### PubSub Topics

```elixir
# Subscribe to all kill updates
Phoenix.PubSub.subscribe(WandererKills.PubSub, &quot;zkb:kills:updated&quot;)
Phoenix.PubSub.subscribe(WandererKills.PubSub, &quot;zkb:detailed_kills:updated&quot;)

# Subscribe to specific system updates
Phoenix.PubSub.subscribe(WandererKills.PubSub, &quot;zkb:system:#{system_id}&quot;)
Phoenix.PubSub.subscribe(WandererKills.PubSub, &quot;zkb:system:#{system_id}:detailed&quot;)
```

### Message Handling

```elixir
defmodule MyApp.KillSubscriber do
  use GenServer

  def start_link(_) do
    GenServer.start_link(__MODULE__, %{}, name: __MODULE__)
  end

  def init(state) do
    # Subscribe to Jita system kills
    Phoenix.PubSub.subscribe(WandererKills.PubSub, &quot;zkb:system:30000142&quot;)
    {:ok, state}
  end

  def handle_info(%{type: :detailed_kill_update, solar_system_id: system_id, kills: kills}, state) do
    IO.puts(&quot;Received #{length(kills)} new kills for system #{system_id}&quot;)
    # Process kills...
    {:noreply, state}
  end

  def handle_info(%{type: :kill_count_update, solar_system_id: system_id, kills: count}, state) do
    IO.puts(&quot;System #{system_id} kill count updated to #{count}&quot;)
    # Update your local state...
    {:noreply, state}
  end
end
```

## Client Library Integration (Elixir)

For direct integration within Elixir applications, implement the `WandererKills.ClientBehaviour`.

### Using the Built-in Client

```elixir
# Add to your application&apos;s dependencies
{:wanderer_kills, path: &quot;../wanderer_kills&quot;}

# Use the client directly
alias WandererKills.Client

# Fetch system kills
{:ok, kills} = Client.fetch_system_kills(30000142, 24, 100)

# Fetch multiple systems
{:ok, systems_kills} = Client.fetch_systems_kills([30000142, 30000144], 24, 50)

# Get cached data
cached_kills = Client.fetch_cached_kills(30000142)

# Manage subscriptions
{:ok, subscription_id} = Client.subscribe_to_kills(
  &quot;my-app&quot;,
  [30000142],
  &quot;https://my-app.com/webhooks&quot;
)

:ok = Client.unsubscribe_from_kills(&quot;my-app&quot;)
```

### Implementing Your Own Client

```elixir
defmodule MyApp.KillsClient do
  @behaviour WandererKills.ClientBehaviour

  @impl true
  def fetch_system_kills(system_id, since_hours, limit) do
    # Your implementation using the REST API
    url = &quot;http://wanderer-kills:4004/api/v1/kills/system/#{system_id}&quot;
    params = %{since_hours: since_hours, limit: limit}

    case HTTPoison.get(url, [], params: params) do
      {:ok, %{status_code: 200, body: body}} -&gt;
        %{&quot;data&quot; =&gt; %{&quot;kills&quot; =&gt; kills}} = Jason.decode!(body)
        {:ok, kills}
      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  # Implement other callbacks...
end
```

## Error Handling

The service returns standardized error responses:

```json
{
  &quot;data&quot;: null,
  &quot;error&quot;: &quot;Invalid system ID&quot;,
  &quot;code&quot;: &quot;INVALID_PARAMETER&quot;,
  &quot;details&quot;: {
    &quot;parameter&quot;: &quot;system_id&quot;,
    &quot;value&quot;: &quot;invalid&quot;
  },
  &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;
}
```

### Common Error Codes

- `INVALID_PARAMETER` - Invalid request parameters
- `NOT_FOUND` - Resource not found
- `RATE_LIMITED` - Rate limit exceeded
- `INTERNAL_ERROR` - Server error
- `TIMEOUT` - Request timeout

### Error Handling Best Practices

1. **Implement Retry Logic** - Use exponential backoff for transient errors
2. **Handle Rate Limits** - Respect 429 responses and retry-after headers
3. **Validate Parameters** - Check parameters client-side before requests
4. **Log Errors** - Include request context in error logs

## Rate Limiting

The service implements rate limiting to ensure fair usage:

- **Per-IP Limits**: 1000 requests per minute
- **Burst Limit**: 100 requests in 10 seconds
- **WebSocket Connections**: 10 concurrent connections per IP

### Rate Limit Headers

```
X-RateLimit-Limit: 1000
X-RateLimit-Remaining: 987
X-RateLimit-Reset: 1642258800
```

## Health and Monitoring

### Health Check

```http
GET /health
```

**Response:**

```json
{
  &quot;status&quot;: &quot;ok&quot;,
  &quot;timestamp&quot;: &quot;2024-01-15T15:00:00Z&quot;
}
```

### Service Status

```http
GET /status
```

**Response:**

```json
{
  &quot;cache_stats&quot;: {
    &quot;hit_rate&quot;: 0.85,
    &quot;size&quot;: 15420
  },
  &quot;active_subscriptions&quot;: 42,
  &quot;websocket_connected&quot;: true,
  &quot;last_kill_received&quot;: &quot;2024-01-15T14:58:30Z&quot;
}
```

## Integration Examples

### Node.js Application

```javascript
const axios = require(&quot;axios&quot;);

class WandererKillsClient {
  constructor(baseUrl = &quot;http://localhost:4004/api/v1&quot;) {
    this.baseUrl = baseUrl;
  }

  async getSystemKills(systemId, sinceHours = 24, limit = 100) {
    try {
      const response = await axios.get(
        `${this.baseUrl}/kills/system/${systemId}`,
        { params: { since_hours: sinceHours, limit } }
      );
      return response.data.data.kills;
    } catch (error) {
      console.error(&quot;Failed to fetch kills:&quot;, error.response?.data);
      throw error;
    }
  }

  async createSubscription(subscriberId, systemIds, callbackUrl) {
    const response = await axios.post(`${this.baseUrl}/subscriptions`, {
      subscriber_id: subscriberId,
      system_ids: systemIds,
      callback_url: callbackUrl,
    });
    return response.data.data;
  }
}

// Usage
const client = new WandererKillsClient();
const kills = await client.getSystemKills(30000142, 24, 50);
console.log(`Found ${kills.length} kills`);
```

### Python Application

```python
import requests
import json

class WandererKillsClient:
    def __init__(self, base_url=&apos;http://localhost:4004/api/v1&apos;):
        self.base_url = base_url

    def get_system_kills(self, system_id, since_hours=24, limit=100):
        url = f&quot;{self.base_url}/kills/system/{system_id}&quot;
        params = {&apos;since_hours&apos;: since_hours, &apos;limit&apos;: limit}

        response = requests.get(url, params=params)
        response.raise_for_status()

        return response.json()[&apos;data&apos;][&apos;kills&apos;]

    def create_subscription(self, subscriber_id, system_ids, callback_url=None):
        url = f&quot;{self.base_url}/subscriptions&quot;
        data = {
            &apos;subscriber_id&apos;: subscriber_id,
            &apos;system_ids&apos;: system_ids,
            &apos;callback_url&apos;: callback_url
        }

        response = requests.post(url, json=data)
        response.raise_for_status()

        return response.json()[&apos;data&apos;]

# Usage
client = WandererKillsClient()
kills = client.get_system_kills(30000142, since_hours=24, limit=50)
print(f&quot;Found {len(kills)} kills&quot;)

# Subscribe to updates
subscription = client.create_subscription(
    &quot;my-python-app&quot;,
    [30000142, 30000144],
    &quot;https://my-app.com/webhooks/kills&quot;
)
print(f&quot;Created subscription: {subscription[&apos;subscription_id&apos;]}&quot;)
```

## Best Practices

### Performance

- **Batch Requests** - Use bulk endpoints for multiple systems
- **Cache Results** - Implement client-side caching with appropriate TTLs
- **Use Cached Endpoints** - Use `/cached/` endpoints for frequently accessed data
- **Limit Request Size** - Keep system lists under 50 systems per request

### Reliability

- **Implement Circuit Breakers** - Fail fast when service is unavailable
- **Handle Duplicates** - Same kill may be delivered multiple times
- **Graceful Degradation** - Fallback to cached data when possible
- **Health Monitoring** - Regular health checks in production

### Security

- **Validate Webhooks** - Verify webhook authenticity in production
- **Rate Limiting** - Implement client-side rate limiting
- **HTTPS Only** - Use HTTPS in production environments
- **API Keys** - Implement proper authentication for production

## Troubleshooting

### Common Issues

1. **No Kills Returned**

   - Check if system ID is valid
   - Verify time range (some systems have no recent activity)
   - Check if service is properly fetching from zKillboard

2. **Webhook Not Receiving Data**

   - Verify callback URL is accessible from service
   - Check webhook endpoint returns 2xx status codes
   - Review logs for HTTP errors

3. **High Latency**

   - Use bulk endpoints for multiple systems
   - Implement client-side caching
   - Consider using cached endpoints

4. **Rate Limiting**
   - Implement exponential backoff
   - Reduce request frequency
   - Use WebSocket connections for real-time data

### Debug Information

Enable debug logging to troubleshoot issues:

```elixir
# In config/config.exs
config :logger, level: :debug

# View logs
docker logs wanderer-kills-container -f
```

## Support

For issues and questions:

- **GitHub Issues**: [Create an issue](https://github.com/wanderer-industries/wanderer-kills/issues)
- **Documentation**: Check the `/docs` directory
- **Health Endpoint**: Monitor service status via `/health`

## API Versioning

The current API version is `v1`. Future versions will be released with backward compatibility guarantees:

- **URL Versioning**: `/api/v1/`, `/api/v2/`
- **Deprecation Notice**: 6 months advance notice
- **Migration Guide**: Provided for breaking changes</file><file path="docs/README.md"># WandererKills Documentation

Welcome to the WandererKills service documentation. This directory contains comprehensive guides for integrating with the killmail data service.

## Documentation Overview

### 📖 [Integration Guide](integration-guide.md)

**Complete integration documentation** - Start here for comprehensive information on integrating with WandererKills service.

**Covers:**

- REST API integration with examples
- Webhook configuration and handling
- Real-time PubSub integration (Elixir apps)
- Client library usage
- Error handling and best practices
- Code examples in Python, Node.js, and Elixir

### 📋 [API Reference](api-reference.md)

**Quick reference documentation** - Ideal for developers who need quick lookups during implementation.

**Includes:**

- Endpoint summaries
- Request/response formats
- Error codes
- cURL examples
- Data structure definitions

## Quick Start

1. **For HTTP/REST integration**: Start with the [Integration Guide](integration-guide.md#rest-api-integration)
2. **For webhook subscriptions**: See [Webhook Integration](integration-guide.md#webhook-integration)
3. **For Elixir applications**: Review [Real-time Integration](integration-guide.md#real-time-integration-elixir-applications)
4. **For quick reference**: Use the [API Reference](api-reference.md)

## Service Overview

The WandererKills service provides:

- **REST API** - HTTP endpoints for fetching killmail data
- **Subscriptions** - Webhook notifications for real-time updates
- **PubSub** - Direct message broadcasting for Elixir applications
- **Client Library** - Behaviour-based integration for Elixir projects

## Common Integration Patterns

### 1. Polling Integration

Periodically fetch kills using REST endpoints. Good for:

- Batch processing
- Systems with relaxed real-time requirements
- Simple integrations

### 2. Webhook Integration

Subscribe to receive HTTP callbacks when new kills are detected. Good for:

- Real-time applications
- External services
- Event-driven architectures

### 3. PubSub Integration

Direct subscription to internal message broadcasts. Good for:

- Elixir applications in the same environment
- Low-latency requirements
- High-throughput scenarios

### 4. Client Library Integration

Use the provided Elixir behaviour for type-safe integration. Good for:

- Elixir applications
- Compile-time interface validation
- Consistent API across implementations

## Getting Help

- **Issues**: Report bugs or request features via GitHub issues
- **API Reference**: Quick lookups in [api-reference.md](api-reference.md)
- **Examples**: Comprehensive examples in [integration-guide.md](integration-guide.md)
- **Health Check**: Monitor service status at `GET /health`

## Service Information

- **Default Port**: 4004
- **API Version**: v1
- **Base URL**: `http://localhost:4004/api/v1`
- **Health Endpoint**: `http://localhost:4004/health`
- **Status Endpoint**: `http://localhost:4004/status`

## External Dependencies

The service integrates with:

- **zKillboard RedisQ** - Real-time killmail stream
- **EVE ESI API** - Killmail details and validation

Rate limiting and caching are implemented to ensure reliable operation while respecting external service limits.</file><file path="lib/wanderer_kills/behaviours/data_fetcher.ex">defmodule WandererKills.Behaviours.DataFetcher do
  @moduledoc &quot;&quot;&quot;
  Behaviour for data fetching implementations.

  This behaviour standardizes data fetching operations for ESI, ZKB,
  and other external data sources.
  &quot;&quot;&quot;

  alias WandererKills.Infrastructure.Error

  @type fetch_args :: term()
  @type fetch_result :: {:ok, term()} | {:error, Error.t()}

  @callback fetch(fetch_args()) :: fetch_result()
  @callback fetch_many([fetch_args()]) :: [fetch_result()]
  @callback supports?(fetch_args()) :: boolean()
end</file><file path="lib/wanderer_kills/behaviours/esi_client.ex">defmodule WandererKills.Behaviours.ESIClient do
  @moduledoc &quot;&quot;&quot;
  Behaviour for ESI (EVE Swagger Interface) client implementations.

  This behaviour standardizes interactions with the EVE Online ESI API.
  &quot;&quot;&quot;

  alias WandererKills.Infrastructure.Error

  @type entity_id :: pos_integer()
  @type entity_data :: map()
  @type esi_result :: {:ok, entity_data()} | {:error, Error.t()}

  # Character operations
  @callback get_character(entity_id()) :: esi_result()
  @callback get_character_batch([entity_id()]) :: [esi_result()]

  # Corporation operations
  @callback get_corporation(entity_id()) :: esi_result()
  @callback get_corporation_batch([entity_id()]) :: [esi_result()]

  # Alliance operations
  @callback get_alliance(entity_id()) :: esi_result()
  @callback get_alliance_batch([entity_id()]) :: [esi_result()]

  # Type operations
  @callback get_type(entity_id()) :: esi_result()
  @callback get_type_batch([entity_id()]) :: [esi_result()]

  # Group operations
  @callback get_group(entity_id()) :: esi_result()
  @callback get_group_batch([entity_id()]) :: [esi_result()]

  # System operations
  @callback get_system(entity_id()) :: esi_result()
  @callback get_system_batch([entity_id()]) :: [esi_result()]
end</file><file path="lib/wanderer_kills/behaviours/http_client.ex">defmodule WandererKills.Behaviours.HttpClient do
  @moduledoc &quot;&quot;&quot;
  Behaviour for HTTP client implementations.

  This behaviour standardizes HTTP operations across ESI, ZKB, and other
  external service clients. Currently only GET operations are used.
  &quot;&quot;&quot;

  alias WandererKills.Infrastructure.Error

  @type url :: String.t()
  @type headers :: [{String.t(), String.t()}]
  @type options :: keyword()
  @type response :: {:ok, map()} | {:error, Error.t()}

  @callback get(url(), headers(), options()) :: response()
  @callback get_with_rate_limit(url(), options()) :: response()
end</file><file path="lib/wanderer_kills/cache/helper.ex">defmodule WandererKills.Cache.Helper do
  @moduledoc &quot;&quot;&quot;
  Helper module for namespaced cache access using a single Cachex instance.

  Instead of multiple cache instances, this uses a single Cachex instance
  with namespaced keys and appropriate TTLs per namespace.
  &quot;&quot;&quot;

  require Logger
  alias WandererKills.Infrastructure.Config

  @cache_name :wanderer_cache

  # Cache namespaces and their corresponding config keys for TTL
  @namespaces %{
    &quot;esi&quot; =&gt; :esi,
    &quot;ship_types&quot; =&gt; :esi,
    &quot;systems&quot; =&gt; :system,
    &quot;characters&quot; =&gt; :esi,
    &quot;corporations&quot; =&gt; :esi,
    &quot;alliances&quot; =&gt; :esi,
    &quot;killmails&quot; =&gt; :killmails
  }

  @doc &quot;&quot;&quot;
  Get a value from the cache using a namespaced key.

  ## Examples
      iex&gt; WandererKills.Cache.Helper.get(&quot;characters&quot;, &quot;12345&quot;)
      {:ok, character_data}

      iex&gt; WandererKills.Cache.Helper.get(&quot;systems&quot;, &quot;30000142&quot;)
      {:ok, system_data}
  &quot;&quot;&quot;
  def get(namespace, key) do
    namespaced_key = build_key(namespace, key)
    Cachex.get(@cache_name, namespaced_key)
  end

  @doc &quot;&quot;&quot;
  Put a value in the cache using a namespaced key with appropriate TTL.

  ## Examples
      iex&gt; WandererKills.Cache.Helper.put(&quot;characters&quot;, &quot;12345&quot;, character_data)
      {:ok, true}
  &quot;&quot;&quot;
  def put(namespace, key, value) do
    namespaced_key = build_key(namespace, key)
    ttl_ms = get_ttl_for_namespace(namespace)
    Cachex.put(@cache_name, namespaced_key, value, ttl: ttl_ms)
  end

  @doc &quot;&quot;&quot;
  Delete a value from the cache using a namespaced key.
  &quot;&quot;&quot;
  def delete(namespace, key) do
    namespaced_key = build_key(namespace, key)
    Cachex.del(@cache_name, namespaced_key)
  end

  @doc &quot;&quot;&quot;
  Check if a key exists in the cache.
  &quot;&quot;&quot;
  def exists?(namespace, key) do
    namespaced_key = build_key(namespace, key)

    case Cachex.exists?(@cache_name, namespaced_key) do
      {:ok, exists} -&gt; exists
      _ -&gt; false
    end
  end

  @doc &quot;&quot;&quot;
  Get or set a value using a fallback function if the key doesn&apos;t exist.

  Similar to Cachex.fetch but with namespaced keys.
  &quot;&quot;&quot;
  def fetch(namespace, key, fallback_fn) do
    namespaced_key = build_key(namespace, key)
    Cachex.fetch(@cache_name, namespaced_key, fallback_fn)
  end

  @doc &quot;&quot;&quot;
  Get a value with error handling for domain-specific use.

  Returns {:error, :not_found} if the key doesn&apos;t exist instead of {:ok, nil}.
  &quot;&quot;&quot;
  def get_with_error(namespace, key) do
    case get(namespace, key) do
      {:ok, nil} -&gt; {:error, :not_found}
      {:ok, value} -&gt; {:ok, value}
      {:error, reason} -&gt; {:error, reason}
    end
  end

  @doc &quot;&quot;&quot;
  Get or set using a fallback function with proper error handling.

  This wraps the fallback function to handle exceptions and provides
  consistent return values.
  &quot;&quot;&quot;
  def get_or_set(namespace, key, fallback_fn) do
    case fetch(namespace, key, fn _key -&gt;
           try do
             {:commit, fallback_fn.()}
           rescue
             error -&gt;
               {:ignore, error}
           end
         end) do
      {:ok, value} -&gt; {:ok, value}
      {:commit, value} -&gt; {:ok, value}
      {:error, reason} -&gt; {:error, reason}
    end
  end

  @doc &quot;&quot;&quot;
  Get cache statistics for monitoring.
  &quot;&quot;&quot;
  def stats do
    Cachex.stats(@cache_name)
  end

  @doc &quot;&quot;&quot;
  Stream entries for a specific namespace pattern.
  &quot;&quot;&quot;
  def stream(namespace, pattern) do
    namespaced_pattern = build_key(namespace, pattern)

    case Cachex.stream(@cache_name, namespaced_pattern) do
      {:ok, stream} -&gt; {:ok, stream}
      {:error, reason} -&gt; {:error, reason}
    end
  end

  @doc &quot;&quot;&quot;
  Clear all entries for a specific namespace.
  &quot;&quot;&quot;
  def clear_namespace(namespace) do
    pattern = build_key(namespace, &quot;*&quot;)

    try do
      case Cachex.stream(@cache_name, pattern) do
        {:ok, stream} -&gt;
          stream
          |&gt; Stream.each(fn {key, _value} -&gt; Cachex.del(@cache_name, key) end)
          |&gt; Stream.run()

          :ok

        {:error, reason} -&gt;
          {:error, reason}
      end
    rescue
      error -&gt;
        {:error, error}
    end
  end

  # ============================================================================
  # Domain-Specific Convenience Functions
  # ============================================================================

  @doc &quot;&quot;&quot;
  Character cache operations.
  &quot;&quot;&quot;
  def character_get(id), do: get_with_error(&quot;characters&quot;, to_string(id))
  def character_put(id, data), do: put(&quot;characters&quot;, to_string(id), data)

  def character_get_or_set(id, fallback_fn),
    do: get_or_set(&quot;characters&quot;, to_string(id), fallback_fn)

  def character_delete(id), do: delete(&quot;characters&quot;, to_string(id))

  @doc &quot;&quot;&quot;
  Corporation cache operations.
  &quot;&quot;&quot;
  def corporation_get(id), do: get_with_error(&quot;corporations&quot;, to_string(id))
  def corporation_put(id, data), do: put(&quot;corporations&quot;, to_string(id), data)

  def corporation_get_or_set(id, fallback_fn),
    do: get_or_set(&quot;corporations&quot;, to_string(id), fallback_fn)

  def corporation_delete(id), do: delete(&quot;corporations&quot;, to_string(id))

  @doc &quot;&quot;&quot;
  Alliance cache operations.
  &quot;&quot;&quot;
  def alliance_get(id), do: get_with_error(&quot;alliances&quot;, to_string(id))
  def alliance_put(id, data), do: put(&quot;alliances&quot;, to_string(id), data)

  def alliance_get_or_set(id, fallback_fn),
    do: get_or_set(&quot;alliances&quot;, to_string(id), fallback_fn)

  def alliance_delete(id), do: delete(&quot;alliances&quot;, to_string(id))

  @doc &quot;&quot;&quot;
  Ship type cache operations.
  &quot;&quot;&quot;
  def ship_type_get(id), do: get_with_error(&quot;ship_types&quot;, to_string(id))
  def ship_type_put(id, data), do: put(&quot;ship_types&quot;, to_string(id), data)

  def ship_type_get_or_set(id, fallback_fn),
    do: get_or_set(&quot;ship_types&quot;, to_string(id), fallback_fn)

  def ship_type_delete(id), do: delete(&quot;ship_types&quot;, to_string(id))

  @doc &quot;&quot;&quot;
  System cache operations.
  &quot;&quot;&quot;
  def system_get(id), do: get_with_error(&quot;systems&quot;, to_string(id))
  def system_put(id, data), do: put(&quot;systems&quot;, to_string(id), data)
  def system_get_or_set(id, fallback_fn), do: get_or_set(&quot;systems&quot;, to_string(id), fallback_fn)
  def system_delete(id), do: delete(&quot;systems&quot;, to_string(id))

  @doc &quot;&quot;&quot;
  System-specific complex cache operations.
  &quot;&quot;&quot;
  def system_get_killmails(system_id) do
    case get(&quot;systems&quot;, &quot;killmails:#{system_id}&quot;) do
      {:ok, nil} -&gt;
        {:error, :not_found}

      {:ok, killmail_ids} when is_list(killmail_ids) -&gt;
        {:ok, killmail_ids}

      {:ok, _invalid_data} -&gt;
        # Clean up corrupted data
        delete(&quot;systems&quot;, &quot;killmails:#{system_id}&quot;)
        {:error, :invalid_data}

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  def system_put_killmails(system_id, killmail_ids) when is_list(killmail_ids) do
    put(&quot;systems&quot;, &quot;killmails:#{system_id}&quot;, killmail_ids)
  end

  def system_add_killmail(system_id, killmail_id) do
    case system_get_killmails(system_id) do
      {:ok, existing_ids} -&gt;
        if killmail_id in existing_ids do
          {:ok, true}
        else
          new_ids = [killmail_id | existing_ids]
          system_put_killmails(system_id, new_ids)
        end

      {:error, :not_found} -&gt;
        system_put_killmails(system_id, [killmail_id])

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  def system_is_active?(system_id) do
    case get(&quot;systems&quot;, &quot;active:#{system_id}&quot;) do
      {:ok, nil} -&gt; {:ok, false}
      {:ok, _timestamp} -&gt; {:ok, true}
      {:error, reason} -&gt; {:error, reason}
    end
  end

  def system_add_active(system_id) do
    case system_is_active?(system_id) do
      {:ok, true} -&gt;
        {:ok, :already_exists}

      {:ok, false} -&gt;
        timestamp = DateTime.utc_now()

        case put(&quot;systems&quot;, &quot;active:#{system_id}&quot;, timestamp) do
          {:ok, true} -&gt; {:ok, :added}
          {:error, reason} -&gt; {:error, reason}
        end

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  def system_get_fetch_timestamp(system_id) do
    case get(&quot;systems&quot;, &quot;fetch_timestamp:#{system_id}&quot;) do
      {:ok, nil} -&gt;
        {:error, :not_found}

      {:ok, timestamp} when is_struct(timestamp, DateTime) -&gt;
        {:ok, timestamp}

      {:ok, _invalid_data} -&gt;
        # Clean up corrupted data
        delete(&quot;systems&quot;, &quot;fetch_timestamp:#{system_id}&quot;)
        {:error, :invalid_data}

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  def system_set_fetch_timestamp(system_id, timestamp \\ nil) do
    timestamp = timestamp || DateTime.utc_now()

    case put(&quot;systems&quot;, &quot;fetch_timestamp:#{system_id}&quot;, timestamp) do
      {:ok, true} -&gt; {:ok, :set}
      {:error, reason} -&gt; {:error, reason}
    end
  end

  def system_get_kill_count(system_id) do
    case get(&quot;systems&quot;, &quot;kill_count:#{system_id}&quot;) do
      {:ok, nil} -&gt;
        {:ok, 0}

      {:ok, count} when is_integer(count) -&gt;
        {:ok, count}

      {:ok, _invalid_data} -&gt;
        # Clean up corrupted data and return 0
        delete(&quot;systems&quot;, &quot;kill_count:#{system_id}&quot;)
        {:ok, 0}

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  def system_increment_kill_count(system_id) do
    case system_get_kill_count(system_id) do
      {:ok, current_count} -&gt;
        new_count = current_count + 1

        case put(&quot;systems&quot;, &quot;kill_count:#{system_id}&quot;, new_count) do
          {:ok, true} -&gt; {:ok, new_count}
          {:error, reason} -&gt; {:error, reason}
        end

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  def system_get_active_systems do
    try do
      case stream(&quot;systems&quot;, &quot;active:*&quot;) do
        {:ok, stream} -&gt;
          # Convert stream to list and process entries
          entries = Enum.to_list(stream)
          Logger.debug(&quot;Stream entries count: #{length(entries)}&quot;)

          system_ids =
            entries
            |&gt; Enum.map(fn entry -&gt;
              case entry do
                # Handle the standard {key, value} format
                {key, _value} when is_binary(key) -&gt;
                  extract_system_id_from_key(key)

                # Handle if it&apos;s just a key
                key when is_binary(key) -&gt;
                  extract_system_id_from_key(key)

                # Handle any other format
                other -&gt;
                  Logger.debug(&quot;Unexpected stream entry format: #{inspect(other)}&quot;)
                  nil
              end
            end)
            |&gt; Enum.reject(&amp;is_nil/1)
            |&gt; Enum.sort()

          Logger.debug(&quot;Found #{length(system_ids)} active systems&quot;)
          {:ok, system_ids}

        {:error, :invalid_match} -&gt;
          # This happens when there are no keys matching the pattern
          Logger.debug(&quot;No active systems found (no matching keys)&quot;)
          {:ok, []}

        {:error, reason} -&gt;
          Logger.error(&quot;Failed to create systems stream: #{inspect(reason)}&quot;)
          {:error, reason}
      end
    rescue
      error -&gt;
        Logger.error(&quot;Error processing active systems stream: #{inspect(error)}&quot;)
        {:error, error}
    end
  end

  def system_recently_fetched?(system_id, threshold_hours \\ 1) do
    case system_get_fetch_timestamp(system_id) do
      {:ok, timestamp} -&gt;
        cutoff_time = DateTime.utc_now() |&gt; DateTime.add(-threshold_hours * 3600, :second)
        is_recent = DateTime.compare(timestamp, cutoff_time) == :gt
        {:ok, is_recent}

      {:error, :not_found} -&gt;
        {:ok, false}

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  @doc &quot;&quot;&quot;
  Killmail cache operations.
  &quot;&quot;&quot;
  def killmail_get(id), do: get_with_error(&quot;killmails&quot;, to_string(id))
  def killmail_put(id, data), do: put(&quot;killmails&quot;, to_string(id), data)

  def killmail_get_or_set(id, fallback_fn),
    do: get_or_set(&quot;killmails&quot;, to_string(id), fallback_fn)

  def killmail_delete(id), do: delete(&quot;killmails&quot;, to_string(id))

  @doc &quot;&quot;&quot;
  Caches killmails for a specific system.

  This function:
  1. Updates the system&apos;s fetch timestamp
  2. Caches individual killmails by ID
  3. Associates killmail IDs with the system
  4. Adds the system to the active systems list

  ## Parameters
  - `system_id` - The solar system ID
  - `killmails` - List of killmail maps

  ## Returns
  - `:ok` on success
  - `{:error, :cache_exception}` on failure
  &quot;&quot;&quot;
  @spec cache_killmails_for_system(integer(), [map()]) :: :ok | {:error, term()}
  def cache_killmails_for_system(system_id, killmails) when is_list(killmails) do
    # Update fetch timestamp
    case system_set_fetch_timestamp(system_id, DateTime.utc_now()) do
      {:ok, _} -&gt; :ok
      # Continue anyway
      {:error, _reason} -&gt; :ok
    end

    # Extract killmail IDs and cache individual killmails
    killmail_ids =
      killmails
      |&gt; Enum.map(fn killmail -&gt;
        killmail_id = Map.get(killmail, &quot;killmail_id&quot;) || Map.get(killmail, &quot;killID&quot;)

        if killmail_id do
          # Cache the individual killmail
          killmail_put(killmail_id, killmail)
          killmail_id
        else
          nil
        end
      end)
      |&gt; Enum.filter(&amp;(&amp;1 != nil))

    # Add each killmail ID to system&apos;s killmail list
    Enum.each(killmail_ids, fn killmail_id -&gt;
      system_add_killmail(system_id, killmail_id)
    end)

    # Add system to active list
    system_add_active(system_id)

    :ok
  rescue
    _error -&gt; {:error, :cache_exception}
  end

  # Private functions

  defp build_key(namespace, key) do
    &quot;#{namespace}:#{key}&quot;
  end

  defp get_ttl_for_namespace(namespace) do
    cache_config = Config.cache()

    case Map.get(@namespaces, namespace, :esi) do
      :esi -&gt; cache_config.esi_ttl * 1_000
      :system -&gt; cache_config.system_ttl * 1_000
      :killmails -&gt; cache_config.killmails_ttl * 1_000
      _ -&gt; cache_config.esi_ttl * 1_000
    end
  end

  # Helper function to extract system ID from cache key
  defp extract_system_id_from_key(key) do
    # Extract system_id from &quot;systems:active:12345&quot; format
    case String.split(key, &quot;:&quot;) do
      [&quot;systems&quot;, &quot;active&quot;, system_id_str] -&gt;
        case Integer.parse(system_id_str) do
          {system_id, &quot;&quot;} -&gt; system_id
          _ -&gt; nil
        end

      _ -&gt;
        nil
    end
  end
end</file><file path="lib/wanderer_kills/esi/client.ex">defmodule WandererKills.ESI.Client do
  @moduledoc &quot;&quot;&quot;
  ESI (EVE Swagger Interface) API client coordinator.

  This module acts as the main interface for ESI operations, delegating
  to specialized fetcher modules for different types of data.
  &quot;&quot;&quot;

  require Logger
  alias WandererKills.Infrastructure.Config
  alias WandererKills.ESI.DataFetcher
  alias WandererKills.Behaviours.ESIClient

  @behaviour ESIClient

  @doc &quot;&quot;&quot;
  Gets ESI base URL from configuration.
  &quot;&quot;&quot;
  def base_url, do: Config.services().esi_base_url

  # ============================================================================
  # ESIClient Behaviour Implementation
  # ============================================================================

  @impl ESIClient
  def get_character(character_id), do: DataFetcher.get_character(character_id)

  @impl ESIClient
  def get_character_batch(character_ids), do: DataFetcher.get_character_batch(character_ids)

  @impl ESIClient
  def get_corporation(corporation_id), do: DataFetcher.get_corporation(corporation_id)

  @impl ESIClient
  def get_corporation_batch(corporation_ids),
    do: DataFetcher.get_corporation_batch(corporation_ids)

  @impl ESIClient
  def get_alliance(alliance_id), do: DataFetcher.get_alliance(alliance_id)

  @impl ESIClient
  def get_alliance_batch(alliance_ids), do: DataFetcher.get_alliance_batch(alliance_ids)

  @impl ESIClient
  def get_type(type_id), do: DataFetcher.get_type(type_id)

  @impl ESIClient
  def get_type_batch(type_ids), do: DataFetcher.get_type_batch(type_ids)

  @impl ESIClient
  def get_group(group_id), do: DataFetcher.get_group(group_id)

  @impl ESIClient
  def get_group_batch(group_ids), do: DataFetcher.get_group_batch(group_ids)

  @impl ESIClient
  def get_system(system_id), do: DataFetcher.get_system(system_id)

  @impl ESIClient
  def get_system_batch(system_ids), do: DataFetcher.get_system_batch(system_ids)

  # ============================================================================
  # Killmail Operations
  # ============================================================================

  @doc &quot;&quot;&quot;
  Fetches a killmail from ESI using killmail ID and hash.
  &quot;&quot;&quot;
  def get_killmail(killmail_id, killmail_hash) do
    DataFetcher.get_killmail(killmail_id, killmail_hash)
  end

  @doc &quot;&quot;&quot;
  Fetches a killmail directly from ESI API (raw implementation).

  This provides direct access to the ESI API for killmail fetching,
  which is used by the parser when full killmail data is needed.
  &quot;&quot;&quot;
  @spec get_killmail_raw(integer(), String.t()) :: {:ok, map()} | {:error, term()}
  def get_killmail_raw(killmail_id, killmail_hash) do
    DataFetcher.get_killmail_raw(killmail_id, killmail_hash)
  end

  @doc &quot;&quot;&quot;
  Fetches multiple killmails concurrently.
  &quot;&quot;&quot;
  def get_killmails_batch(killmail_specs) do
    DataFetcher.get_killmails_batch(killmail_specs)
  end

  # ============================================================================
  # Ship Type Operations
  # ============================================================================

  @doc &quot;&quot;&quot;
  Returns the default ship group IDs.
  &quot;&quot;&quot;
  def ship_group_ids, do: DataFetcher.ship_group_ids()

  @doc &quot;&quot;&quot;
  Returns the source name for this ESI client.
  &quot;&quot;&quot;
  def source_name, do: &quot;ESI&quot;

  @doc &quot;&quot;&quot;
  General update function that delegates to update_ship_groups.

  This provides compatibility for modules that expect a general update function.

  ## Options
  - `opts` - Keyword list of options
    - `group_ids` - List of group IDs to fetch (optional)

  ## Examples
      iex&gt; ESI.Client.update()
      :ok

      iex&gt; ESI.Client.update(group_ids: [23, 16])
      :ok
  &quot;&quot;&quot;
  def update(opts \\ []) do
    group_ids = Keyword.get(opts, :group_ids)
    update_ship_groups(group_ids)
  end

  @doc &quot;&quot;&quot;
  Updates ship groups by fetching fresh data from ESI.
  &quot;&quot;&quot;
  def update_ship_groups(group_ids \\ nil) do
    group_ids = group_ids || DataFetcher.ship_group_ids()
    DataFetcher.update_ship_groups(group_ids)
  end

  @doc &quot;&quot;&quot;
  Fetches ship types for specific groups.
  &quot;&quot;&quot;
  def fetch_ship_types_for_groups(group_ids \\ nil) do
    group_ids = group_ids || DataFetcher.ship_group_ids()
    DataFetcher.fetch_ship_types_for_groups(group_ids)
  end

  # ============================================================================
  # Private Functions
  # ============================================================================
end</file><file path="lib/wanderer_kills/esi/data_fetcher.ex">defmodule WandererKills.ESI.DataFetcher do
  @moduledoc &quot;&quot;&quot;
  Unified ESI data fetcher that consolidates all ESI API interactions.

  This module replaces the individual fetcher modules (CharacterFetcher, TypeFetcher,
  KillmailFetcher) with a single, clean implementation that handles all ESI data types.
  &quot;&quot;&quot;

  require Logger
  alias WandererKills.Behaviours.{DataFetcher, ESIClient}
  alias WandererKills.Cache.Helper
  alias WandererKills.Infrastructure.{Config, Error}

  @behaviour ESIClient
  @behaviour DataFetcher

  # Default ship group IDs that contain ship types
  @ship_group_ids [6, 7, 9, 11, 16, 17, 23]

  # ============================================================================
  # ESIClient Implementation
  # ============================================================================

  @impl ESIClient
  def get_character(character_id) when is_integer(character_id) do
    Helper.character_get_or_set(character_id, fn -&gt;
      fetch_from_api(:character, character_id)
    end)
  end

  @impl ESIClient
  def get_character_batch(character_ids) when is_list(character_ids) do
    fetch_batch(:character, character_ids)
  end

  @impl ESIClient
  def get_corporation(corporation_id) when is_integer(corporation_id) do
    Helper.corporation_get_or_set(corporation_id, fn -&gt;
      fetch_from_api(:corporation, corporation_id)
    end)
  end

  @impl ESIClient
  def get_corporation_batch(corporation_ids) when is_list(corporation_ids) do
    fetch_batch(:corporation, corporation_ids)
  end

  @impl ESIClient
  def get_alliance(alliance_id) when is_integer(alliance_id) do
    Helper.alliance_get_or_set(alliance_id, fn -&gt;
      fetch_from_api(:alliance, alliance_id)
    end)
  end

  @impl ESIClient
  def get_alliance_batch(alliance_ids) when is_list(alliance_ids) do
    fetch_batch(:alliance, alliance_ids)
  end

  @impl ESIClient
  def get_type(type_id) when is_integer(type_id) do
    Helper.ship_type_get_or_set(type_id, fn -&gt;
      fetch_from_api(:type, type_id)
    end)
  end

  @impl ESIClient
  def get_type_batch(type_ids) when is_list(type_ids) do
    fetch_batch(:type, type_ids)
  end

  @impl ESIClient
  def get_group(group_id) when is_integer(group_id) do
    Helper.get_or_set(&quot;groups&quot;, to_string(group_id), fn -&gt;
      fetch_from_api(:group, group_id)
    end)
  end

  @impl ESIClient
  def get_group_batch(group_ids) when is_list(group_ids) do
    fetch_batch(:group, group_ids)
  end

  @impl ESIClient
  def get_system(system_id) when is_integer(system_id) do
    result = fetch_from_api(:system, system_id)
    {:ok, result}
  rescue
    error -&gt;
      {:error, error}
  end

  @impl ESIClient
  def get_system_batch(system_ids) when is_list(system_ids) do
    fetch_batch(:system, system_ids)
  end

  # ============================================================================
  # DataFetcher Implementation
  # ============================================================================

  @impl DataFetcher
  def fetch({:character, character_id}), do: get_character(character_id)
  def fetch({:corporation, corporation_id}), do: get_corporation(corporation_id)
  def fetch({:alliance, alliance_id}), do: get_alliance(alliance_id)
  def fetch({:type, type_id}), do: get_type(type_id)
  def fetch({:group, group_id}), do: get_group(group_id)
  def fetch({:system, system_id}), do: get_system(system_id)
  def fetch({:killmail, killmail_id, killmail_hash}), do: get_killmail(killmail_id, killmail_hash)
  def fetch(_), do: {:error, Error.esi_error(:unsupported, &quot;Unsupported fetch operation&quot;)}

  @impl DataFetcher
  def fetch_many(fetch_args) when is_list(fetch_args) do
    Enum.map(fetch_args, &amp;fetch/1)
  end

  @impl DataFetcher
  def supports?({:character, _}), do: true
  def supports?({:corporation, _}), do: true
  def supports?({:alliance, _}), do: true
  def supports?({:type, _}), do: true
  def supports?({:group, _}), do: true
  def supports?({:system, _}), do: true
  def supports?({:killmail, _, _}), do: true
  def supports?(_), do: false

  # ============================================================================
  # Killmail-specific functions
  # ============================================================================

  @doc &quot;&quot;&quot;
  Fetches a killmail from ESI using killmail ID and hash.
  &quot;&quot;&quot;
  def get_killmail(killmail_id, killmail_hash)
      when is_integer(killmail_id) and is_binary(killmail_hash) do
    Helper.killmail_get_or_set(killmail_id, fn -&gt;
      fetch_killmail_from_api(killmail_id, killmail_hash)
    end)
  end

  @doc &quot;&quot;&quot;
  Fetches multiple killmails concurrently.
  &quot;&quot;&quot;
  def get_killmails_batch(killmail_specs) when is_list(killmail_specs) do
    killmail_specs
    |&gt; Task.async_stream(
      fn {killmail_id, killmail_hash} -&gt;
        get_killmail(killmail_id, killmail_hash)
      end,
      max_concurrency: Config.batch().concurrency_esi,
      timeout: Config.timeouts().esi_request_ms
    )
    |&gt; Enum.to_list()
    |&gt; Enum.map(fn
      {:ok, result} -&gt;
        result

      {:exit, reason} -&gt;
        {:error, Error.esi_error(:timeout, &quot;Killmail fetch timeout&quot;, false, %{reason: reason})}
    end)
  end

  # ============================================================================
  # Ship type utilities
  # ============================================================================

  @doc &quot;&quot;&quot;
  Returns the default ship group IDs.
  &quot;&quot;&quot;
  def ship_group_ids, do: @ship_group_ids

  @doc &quot;&quot;&quot;
  Updates ship groups by fetching fresh data from ESI.
  &quot;&quot;&quot;
  def update_ship_groups(group_ids \\ @ship_group_ids) when is_list(group_ids) do
    Logger.info(&quot;Updating ship groups from ESI&quot;, group_ids: group_ids)

    results = Enum.map(group_ids, &amp;get_group/1)
    errors = Enum.filter(results, &amp;match?({:error, _}, &amp;1))

    if length(errors) &gt; 0 do
      Logger.error(&quot;Failed to update some ship groups&quot;,
        error_count: length(errors),
        total_groups: length(group_ids)
      )

      {:error, {:partial_failure, errors}}
    else
      Logger.info(&quot;Successfully updated all ship groups&quot;)
      :ok
    end
  end

  @doc &quot;&quot;&quot;
  Fetches types for specific groups and returns parsed ship data.
  &quot;&quot;&quot;
  def fetch_ship_types_for_groups(group_ids \\ @ship_group_ids) when is_list(group_ids) do
    Logger.info(&quot;Fetching ship types for groups&quot;, group_ids: group_ids)

    with {:ok, groups} &lt;- fetch_groups(group_ids),
         {:ok, ship_types} &lt;- extract_and_fetch_types(groups) do
      {:ok, ship_types}
    else
      {:error, reason} -&gt;
        Logger.error(&quot;Failed to fetch ship types: #{inspect(reason)}&quot;)
        {:error, reason}
    end
  end

  @doc &quot;&quot;&quot;
  Fetches a killmail directly from ESI API (raw implementation).

  This provides direct access to the ESI API for killmail fetching,
  bypassing the cache layer. Used by the parser when fresh killmail data is needed.
  &quot;&quot;&quot;
  @spec get_killmail_raw(integer(), String.t()) :: {:ok, map()} | {:error, term()}
  def get_killmail_raw(killmail_id, killmail_hash) do
    url = &quot;#{esi_base_url()}/killmails/#{killmail_id}/#{killmail_hash}/&quot;

    case WandererKills.Http.Client.get_with_rate_limit(url) do
      {:ok, %{body: body}} -&gt; {:ok, body}
      {:error, reason} -&gt; {:error, reason}
    end
  end

  # ============================================================================
  # Private Functions
  # ============================================================================

  defp fetch_groups(group_ids) do
    Logger.debug(&quot;Fetching groups from ESI&quot;, group_ids: group_ids)

    results = Enum.map(group_ids, &amp;get_group/1)

    errors = Enum.filter(results, &amp;match?({:error, _}, &amp;1))
    successes = Enum.filter(results, &amp;match?({:ok, _}, &amp;1))

    if length(errors) &gt; 0 do
      Logger.error(&quot;Failed to fetch some groups&quot;,
        error_count: length(errors),
        success_count: length(successes)
      )

      {:error, {:partial_failure, errors}}
    else
      groups = Enum.map(successes, fn {:ok, group} -&gt; group end)
      {:ok, groups}
    end
  end

  defp extract_and_fetch_types(groups) do
    Logger.debug(&quot;Extracting type IDs from groups&quot;)

    type_ids =
      groups
      |&gt; Enum.flat_map(fn group -&gt; Map.get(group, &quot;types&quot;, []) end)
      |&gt; Enum.uniq()

    Logger.debug(&quot;Fetching types&quot;, type_count: length(type_ids))

    results = Enum.map(type_ids, &amp;get_type/1)

    errors = Enum.filter(results, &amp;match?({:error, _}, &amp;1))
    successes = Enum.filter(results, &amp;match?({:ok, _}, &amp;1))

    if length(errors) &gt; 0 do
      Logger.error(&quot;Failed to fetch some types&quot;,
        error_count: length(errors),
        success_count: length(successes)
      )

      {:error, {:partial_failure, errors}}
    else
      types = Enum.map(successes, fn {:ok, type} -&gt; type end)
      {:ok, types}
    end
  end

  defp fetch_batch(entity_type, ids) when is_list(ids) do
    ids
    |&gt; Task.async_stream(
      fn id -&gt; fetch_from_api(entity_type, id) end,
      max_concurrency: Config.batch().concurrency_esi,
      timeout: Config.timeouts().esi_request_ms
    )
    |&gt; Enum.to_list()
    |&gt; Enum.map(fn
      {:ok, result} -&gt;
        result

      {:exit, reason} -&gt;
        {:error, Error.esi_error(:timeout, &quot;Batch fetch timeout&quot;, false, %{reason: reason})}
    end)
  end

  defp fetch_from_api(entity_type, entity_id) do
    url = build_url(entity_type, entity_id)

    case http_client().get(url, default_headers(), request_options()) do
      {:ok, response} -&gt;
        parse_response(entity_type, entity_id, response)

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to fetch #{entity_type} #{entity_id}: #{inspect(reason)}&quot;)

        raise Error.esi_error(:api_error, &quot;Failed to fetch #{entity_type} from ESI&quot;, false, %{
                entity_type: entity_type,
                entity_id: entity_id,
                reason: reason
              })
    end
  end

  defp fetch_killmail_from_api(killmail_id, killmail_hash) do
    url = &quot;#{esi_base_url()}/killmails/#{killmail_id}/#{killmail_hash}/&quot;

    Logger.debug(&quot;Fetching killmail from ESI&quot;,
      killmail_id: killmail_id,
      killmail_hash: String.slice(killmail_hash, 0, 8) &lt;&gt; &quot;...&quot;
    )

    case http_client().get(url, default_headers(), request_options()) do
      {:ok, response} -&gt;
        parse_killmail_response(killmail_id, killmail_hash, response)

      {:error, %{status: 404}} -&gt;
        raise Error.esi_error(:not_found, &quot;Killmail not found&quot;, false, %{
                killmail_id: killmail_id,
                killmail_hash: killmail_hash
              })

      {:error, %{status: 403}} -&gt;
        raise Error.esi_error(:forbidden, &quot;Killmail access forbidden&quot;, false, %{
                killmail_id: killmail_id,
                killmail_hash: killmail_hash
              })

      {:error, %{status: status}} when status &gt;= 500 -&gt;
        raise Error.esi_error(:server_error, &quot;ESI server error&quot;, false, %{
                killmail_id: killmail_id,
                killmail_hash: killmail_hash,
                status: status
              })

      {:error, reason} -&gt;
        raise Error.esi_error(:api_error, &quot;Failed to fetch killmail from ESI&quot;, false, %{
                killmail_id: killmail_id,
                killmail_hash: killmail_hash,
                reason: reason
              })
    end
  end

  defp build_url(:character, id), do: &quot;#{esi_base_url()}/characters/#{id}/&quot;
  defp build_url(:corporation, id), do: &quot;#{esi_base_url()}/corporations/#{id}/&quot;
  defp build_url(:alliance, id), do: &quot;#{esi_base_url()}/alliances/#{id}/&quot;
  defp build_url(:type, id), do: &quot;#{esi_base_url()}/universe/types/#{id}/&quot;
  defp build_url(:group, id), do: &quot;#{esi_base_url()}/universe/groups/#{id}/&quot;
  defp build_url(:system, id), do: &quot;#{esi_base_url()}/universe/systems/#{id}/&quot;

  defp parse_response(:character, id, %{body: body}) do
    %{
      &quot;character_id&quot; =&gt; id,
      &quot;name&quot; =&gt; Map.get(body, &quot;name&quot;),
      &quot;corporation_id&quot; =&gt; Map.get(body, &quot;corporation_id&quot;),
      &quot;alliance_id&quot; =&gt; Map.get(body, &quot;alliance_id&quot;),
      &quot;birthday&quot; =&gt; Map.get(body, &quot;birthday&quot;),
      &quot;gender&quot; =&gt; Map.get(body, &quot;gender&quot;),
      &quot;race_id&quot; =&gt; Map.get(body, &quot;race_id&quot;),
      &quot;bloodline_id&quot; =&gt; Map.get(body, &quot;bloodline_id&quot;),
      &quot;ancestry_id&quot; =&gt; Map.get(body, &quot;ancestry_id&quot;),
      &quot;security_status&quot; =&gt; Map.get(body, &quot;security_status&quot;)
    }
  end

  defp parse_response(:corporation, id, %{body: body}) do
    %{
      &quot;corporation_id&quot; =&gt; id,
      &quot;name&quot; =&gt; Map.get(body, &quot;name&quot;),
      &quot;ticker&quot; =&gt; Map.get(body, &quot;ticker&quot;),
      &quot;alliance_id&quot; =&gt; Map.get(body, &quot;alliance_id&quot;),
      &quot;ceo_id&quot; =&gt; Map.get(body, &quot;ceo_id&quot;),
      &quot;creator_id&quot; =&gt; Map.get(body, &quot;creator_id&quot;),
      &quot;date_founded&quot; =&gt; Map.get(body, &quot;date_founded&quot;),
      &quot;description&quot; =&gt; Map.get(body, &quot;description&quot;),
      &quot;faction_id&quot; =&gt; Map.get(body, &quot;faction_id&quot;),
      &quot;home_station_id&quot; =&gt; Map.get(body, &quot;home_station_id&quot;),
      &quot;member_count&quot; =&gt; Map.get(body, &quot;member_count&quot;),
      &quot;shares&quot; =&gt; Map.get(body, &quot;shares&quot;),
      &quot;tax_rate&quot; =&gt; Map.get(body, &quot;tax_rate&quot;),
      &quot;url&quot; =&gt; Map.get(body, &quot;url&quot;),
      &quot;war_eligible&quot; =&gt; Map.get(body, &quot;war_eligible&quot;)
    }
  end

  defp parse_response(:alliance, id, %{body: body}) do
    %{
      &quot;alliance_id&quot; =&gt; id,
      &quot;name&quot; =&gt; Map.get(body, &quot;name&quot;),
      &quot;ticker&quot; =&gt; Map.get(body, &quot;ticker&quot;),
      &quot;creator_corporation_id&quot; =&gt; Map.get(body, &quot;creator_corporation_id&quot;),
      &quot;creator_id&quot; =&gt; Map.get(body, &quot;creator_id&quot;),
      &quot;date_founded&quot; =&gt; Map.get(body, &quot;date_founded&quot;),
      &quot;executor_corporation_id&quot; =&gt; Map.get(body, &quot;executor_corporation_id&quot;),
      &quot;faction_id&quot; =&gt; Map.get(body, &quot;faction_id&quot;)
    }
  end

  defp parse_response(:type, id, %{body: body}) do
    %{
      &quot;type_id&quot; =&gt; id,
      &quot;name&quot; =&gt; Map.get(body, &quot;name&quot;),
      &quot;description&quot; =&gt; Map.get(body, &quot;description&quot;),
      &quot;group_id&quot; =&gt; Map.get(body, &quot;group_id&quot;),
      &quot;category_id&quot; =&gt; Map.get(body, &quot;category_id&quot;),
      &quot;published&quot; =&gt; Map.get(body, &quot;published&quot;),
      &quot;mass&quot; =&gt; Map.get(body, &quot;mass&quot;),
      &quot;volume&quot; =&gt; Map.get(body, &quot;volume&quot;),
      &quot;capacity&quot; =&gt; Map.get(body, &quot;capacity&quot;),
      &quot;portion_size&quot; =&gt; Map.get(body, &quot;portion_size&quot;),
      &quot;radius&quot; =&gt; Map.get(body, &quot;radius&quot;),
      &quot;graphic_id&quot; =&gt; Map.get(body, &quot;graphic_id&quot;),
      &quot;icon_id&quot; =&gt; Map.get(body, &quot;icon_id&quot;),
      &quot;market_group_id&quot; =&gt; Map.get(body, &quot;market_group_id&quot;),
      &quot;packaged_volume&quot; =&gt; Map.get(body, &quot;packaged_volume&quot;)
    }
  end

  defp parse_response(:group, id, %{body: body}) do
    %{
      &quot;group_id&quot; =&gt; id,
      &quot;name&quot; =&gt; Map.get(body, &quot;name&quot;),
      &quot;category_id&quot; =&gt; Map.get(body, &quot;category_id&quot;),
      &quot;published&quot; =&gt; Map.get(body, &quot;published&quot;),
      &quot;types&quot; =&gt; Map.get(body, &quot;types&quot;, [])
    }
  end

  defp parse_response(:system, id, %{body: body}) do
    %{
      &quot;system_id&quot; =&gt; id,
      &quot;name&quot; =&gt; Map.get(body, &quot;name&quot;),
      &quot;constellation_id&quot; =&gt; Map.get(body, &quot;constellation_id&quot;),
      &quot;security_class&quot; =&gt; Map.get(body, &quot;security_class&quot;),
      &quot;security_status&quot; =&gt; Map.get(body, &quot;security_status&quot;),
      &quot;star_id&quot; =&gt; Map.get(body, &quot;star_id&quot;),
      &quot;stargates&quot; =&gt; Map.get(body, &quot;stargates&quot;, []),
      &quot;stations&quot; =&gt; Map.get(body, &quot;stations&quot;, []),
      &quot;planets&quot; =&gt; Map.get(body, &quot;planets&quot;, [])
    }
  end

  defp parse_killmail_response(killmail_id, killmail_hash, %{body: body}) do
    body
    |&gt; Map.put(&quot;killmail_id&quot;, killmail_id)
    |&gt; Map.put(&quot;killmail_hash&quot;, killmail_hash)
  end

  defp esi_base_url, do: Config.services().esi_base_url
  defp http_client, do: Config.app().http_client

  defp default_headers do
    [
      {&quot;User-Agent&quot;, &quot;WandererKills/1.0&quot;},
      {&quot;Accept&quot;, &quot;application/json&quot;}
    ]
  end

  defp request_options do
    [
      timeout: Config.timeouts().esi_request_ms,
      recv_timeout: Config.timeouts().esi_request_ms
    ]
  end
end</file><file path="lib/wanderer_kills/http/client_provider.ex">defmodule WandererKills.Http.ClientProvider do
  @moduledoc &quot;&quot;&quot;
  Centralized HTTP client configuration and utilities provider.

  This module provides a single point for accessing HTTP client configuration,
  default headers, timeouts, and other HTTP-related utilities, eliminating
  the need for duplicate configurations across modules.

  ## Usage

  ```elixir
  alias WandererKills.Http.ClientProvider

  client = ClientProvider.get_client()
  headers = ClientProvider.default_headers()
  timeout = ClientProvider.default_timeout()
  ```
  &quot;&quot;&quot;

  alias WandererKills.Infrastructure.Config

  @user_agent &quot;(wanderer-kills@proton.me; +https://github.com/wanderer-industries/wanderer-kills)&quot;

  @doc &quot;&quot;&quot;
  Gets the configured HTTP client module.

  Returns the HTTP client configured in the application environment,
  defaulting to `WandererKills.Http.Client` if not specified.
  &quot;&quot;&quot;
  @spec get_client() :: module()
  def get_client do
    Config.app().http_client
  end

  @doc &quot;&quot;&quot;
  Gets default HTTP headers for API requests.

  ## Options
  - `:user_agent` - Custom user agent (defaults to application user agent)
  - `:accept` - Accept header (defaults to &quot;application/json&quot;)
  - `:encoding` - Accept-Encoding header (defaults to &quot;gzip&quot;)
  &quot;&quot;&quot;
  @spec default_headers(keyword()) :: [{String.t(), String.t()}]
  def default_headers(opts \\ []) do
    user_agent = Keyword.get(opts, :user_agent, @user_agent)
    accept = Keyword.get(opts, :accept, &quot;application/json&quot;)
    encoding = Keyword.get(opts, :encoding, &quot;gzip&quot;)

    [
      {&quot;User-Agent&quot;, user_agent},
      {&quot;Accept&quot;, accept},
      {&quot;Accept-Encoding&quot;, encoding}
    ]
  end

  @doc &quot;&quot;&quot;
  Gets EVE Online API specific headers.
  &quot;&quot;&quot;
  @spec eve_api_headers() :: [{String.t(), String.t()}]
  def eve_api_headers do
    default_headers()
  end

  @doc &quot;&quot;&quot;
  Gets default request timeout from configuration.
  &quot;&quot;&quot;
  @spec default_timeout() :: integer()
  def default_timeout do
    Config.timeouts().default_request_ms
  end

  @doc &quot;&quot;&quot;
  Gets ESI-specific timeout from configuration.
  &quot;&quot;&quot;
  @spec esi_timeout() :: integer()
  def esi_timeout do
    Config.timeouts().esi_request_ms
  end

  @doc &quot;&quot;&quot;
  Builds standard request options with defaults.

  ## Options
  - `:timeout` - Request timeout (defaults to configured default)
  - `:headers` - Additional headers (merged with defaults)
  - `:params` - Query parameters
  &quot;&quot;&quot;
  @spec build_request_opts(keyword()) :: keyword()
  def build_request_opts(opts \\ []) do
    timeout = Keyword.get(opts, :timeout, default_timeout())
    custom_headers = Keyword.get(opts, :headers, [])
    params = Keyword.get(opts, :params, [])

    headers = default_headers() ++ custom_headers

    [
      headers: headers,
      params: filter_params(params),
      timeout: timeout,
      recv_timeout: timeout
    ]
  end

  # Private functions

  @spec filter_params(keyword()) :: keyword()
  defp filter_params(params) do
    params
    |&gt; Enum.reject(fn {_key, value} -&gt; is_nil(value) end)
    |&gt; Enum.map(fn
      {key, true} -&gt; {key, &quot;true&quot;}
      {key, false} -&gt; {key, &quot;false&quot;}
      {key, value} when is_integer(value) -&gt; {key, Integer.to_string(value)}
      {key, value} -&gt; {key, value}
    end)
  end
end</file><file path="lib/wanderer_kills/infrastructure/batch_processor.ex">defmodule WandererKills.Infrastructure.BatchProcessor do
  @moduledoc &quot;&quot;&quot;
  Unified batch processing module for handling parallel operations.

  This module provides consistent patterns for:
  - Parallel task execution with configurable concurrency
  - Result aggregation and reporting
  - Timeout and retry management

  ## Configuration

  Batch processing uses the concurrency configuration:

  ```elixir
  config :wanderer_kills,
    batch: %{
      concurrency_default: 10,
      batch_size: 50
    },
    timeouts: %{
      default_request_ms: 30_000
    }
  ```

  ## Usage

  ```elixir
  # Parallel processing (recommended)
  items = [1, 2, 3, 4, 5]
  {:ok, results} = BatchProcessor.process_parallel(items, &amp;fetch_data/1)

  # With custom options
  {:ok, results} = BatchProcessor.process_parallel(items, &amp;fetch_data/1,
    max_concurrency: 5,
    timeout: 60_000,
    description: &quot;Fetching ship data&quot;
  )

  # Sequential processing (use regular Enum.map for simple cases)
  results = Enum.map(items, &amp;fetch_data/1)
  ```
  &quot;&quot;&quot;

  require Logger
  alias WandererKills.Infrastructure.Config

  @type task_result :: {:ok, term()} | {:error, term()}
  @type batch_result :: {:ok, [term()]} | {:partial, [term()], [term()]} | {:error, term()}
  @type batch_opts :: [
          max_concurrency: pos_integer(),
          timeout: pos_integer(),
          description: String.t(),
          supervisor: GenServer.name()
        ]

  @doc &quot;&quot;&quot;
  Processes items in parallel using Task.Supervisor with configurable concurrency.

  This is the main batch processing function that handles all parallel operations.
  For simple sequential processing, use `Enum.map/2` directly.

  ## Options
  - `:max_concurrency` - Maximum concurrent tasks (default: from config)
  - `:timeout` - Timeout per task in milliseconds (default: from config)
  - `:supervisor` - Task supervisor to use (default: WandererKills.TaskSupervisor)
  - `:description` - Description for logging (default: &quot;items&quot;)

  ## Returns
  - `{:ok, results}` - If all items processed successfully
  - `{:partial, results, failures}` - If some items failed
  - `{:error, reason}` - If processing failed entirely
  &quot;&quot;&quot;
  @spec process_parallel([term()], (term() -&gt; task_result()), batch_opts()) :: batch_result()
  def process_parallel(items, process_fn, opts \\ []) when is_list(items) do
    max_concurrency = Keyword.get(opts, :max_concurrency, Config.batch().concurrency_default)
    timeout = Keyword.get(opts, :timeout, Config.timeouts().default_request_ms)
    supervisor = Keyword.get(opts, :supervisor, WandererKills.TaskSupervisor)
    description = Keyword.get(opts, :description, &quot;items&quot;)

    Logger.info(
      &quot;Processing #{length(items)} #{description} in parallel &quot; &lt;&gt;
        &quot;(max_concurrency: #{max_concurrency}, timeout: #{timeout}ms)&quot;
    )

    start_time = System.monotonic_time()

    results =
      Task.Supervisor.async_stream_nolink(
        supervisor,
        items,
        process_fn,
        max_concurrency: max_concurrency,
        timeout: timeout
      )
      |&gt; Enum.to_list()

    duration = System.monotonic_time() - start_time
    duration_ms = System.convert_time_unit(duration, :native, :millisecond)

    process_batch_results(results, length(items), description, duration_ms)
  end

  @doc &quot;&quot;&quot;
  Executes a list of async tasks with timeout and error aggregation.

  ## Options
  - `:timeout` - Timeout for all tasks in milliseconds (default: from config)
  - `:description` - Description for logging (default: &quot;tasks&quot;)

  ## Returns
  - `{:ok, results}` - If all tasks succeed
  - `{:partial, results, failures}` - If some tasks failed
  - `{:error, reason}` - If tasks failed entirely
  &quot;&quot;&quot;
  @spec await_tasks([Task.t()], batch_opts()) :: batch_result()
  def await_tasks(tasks, opts \\ []) when is_list(tasks) do
    timeout = Keyword.get(opts, :timeout, Config.timeouts().default_request_ms)
    description = Keyword.get(opts, :description, &quot;tasks&quot;)

    Logger.info(&quot;Awaiting #{length(tasks)} #{description} (timeout: #{timeout}ms)&quot;)

    start_time = System.monotonic_time()

    try do
      results = Task.await_many(tasks, timeout)

      duration = System.monotonic_time() - start_time
      duration_ms = System.convert_time_unit(duration, :native, :millisecond)

      Logger.info(&quot;Completed #{length(tasks)} #{description} in #{duration_ms}ms&quot;)
      {:ok, results}
    rescue
      error -&gt;
        duration = System.monotonic_time() - start_time
        duration_ms = System.convert_time_unit(duration, :native, :millisecond)

        Logger.error(&quot;Task await failed after #{duration_ms}ms&quot;,
          tasks: length(tasks),
          description: description,
          error: inspect(error)
        )

        {:error, error}
    end
  end

  # Private Functions

  @spec process_batch_results([term()], integer(), String.t(), integer()) :: batch_result()
  defp process_batch_results(results, total_count, description, duration_ms) do
    {successes, failures} = categorize_results(results)

    success_count = length(successes)
    failure_count = length(failures)

    case {success_count, failure_count} do
      {^total_count, 0} -&gt;
        Logger.info(&quot;Successfully processed #{success_count} #{description} in #{duration_ms}ms&quot;)
        {:ok, successes}

      {0, ^total_count} -&gt;
        Logger.error(&quot;Failed to process all #{total_count} #{description} in #{duration_ms}ms&quot;)
        {:error, &quot;All items failed to process&quot;}

      {_, _} -&gt;
        Logger.warning(
          &quot;Partially processed #{description} in #{duration_ms}ms: &quot; &lt;&gt;
            &quot;#{success_count} succeeded, #{failure_count} failed&quot;
        )

        {:partial, successes, failures}
    end
  end

  @spec categorize_results([term()]) :: {[term()], [term()]}
  defp categorize_results(results) do
    Enum.reduce(results, {[], []}, fn
      {:ok, result}, {successes, failures} -&gt;
        {[result | successes], failures}

      {:exit, reason}, {successes, failures} -&gt;
        {successes, [{:exit, reason} | failures]}

      {:error, reason}, {successes, failures} -&gt;
        {successes, [{:error, reason} | failures]}

      other, {successes, failures} -&gt;
        Logger.warning(&quot;Unexpected async_stream result format: #{inspect(other)}&quot;)
        {successes, [{:unexpected, other} | failures]}
    end)
  end
end</file><file path="lib/wanderer_kills/infrastructure/clock.ex">defmodule WandererKills.Infrastructure.Clock do
  @moduledoc &quot;&quot;&quot;
  Unified time and clock utilities for WandererKills.

  This module provides a clean, simple API for time operations without
  complex configuration overrides that were previously used for testing.

  ## Usage

  ```elixir
  # Get current time
  now = Clock.now()

  # Get milliseconds since epoch
  ms = Clock.now_milliseconds()

  # Get time N hours ago
  past = Clock.hours_ago(2)

  # Parse killmail times
  {:ok, datetime} = Clock.parse_time(&quot;2025-01-01T00:00:00Z&quot;)
  ```

  ## Testing

  For testing time-dependent behavior, use libraries like `ExMachina` or
  inject time values directly into your test functions rather than relying
  on global configuration overrides.
  &quot;&quot;&quot;

  require Logger

  @type killmail :: map()
  @type time_result :: {:ok, DateTime.t()} | {:error, term()}
  @type validation_result :: {:ok, {killmail(), DateTime.t()}} | :older | :skip

  # ============================================================================
  # Current Time Functions
  # ============================================================================

  @doc &quot;&quot;&quot;
  Returns the current `DateTime` in UTC.
  &quot;&quot;&quot;
  @spec now() :: DateTime.t()
  def now do
    DateTime.utc_now()
  end

  @doc &quot;&quot;&quot;
  Returns the current time in **milliseconds** since Unix epoch.
  &quot;&quot;&quot;
  @spec now_milliseconds() :: integer()
  def now_milliseconds do
    System.system_time(:millisecond)
  end

  @doc &quot;&quot;&quot;
  Returns the current system time in the specified `unit`.
  &quot;&quot;&quot;
  @spec system_time(System.time_unit()) :: integer()
  def system_time(unit) do
    System.system_time(unit)
  end

  @doc &quot;&quot;&quot;
  Returns the current time as an ISO8601 string.
  &quot;&quot;&quot;
  @spec now_iso8601() :: String.t()
  def now_iso8601 do
    now() |&gt; DateTime.to_iso8601()
  end

  # ============================================================================
  # Relative Time Functions
  # ============================================================================

  @doc &quot;&quot;&quot;
  Returns a `DateTime` that is `seconds` seconds before the current `now()`.
  &quot;&quot;&quot;
  @spec seconds_ago(non_neg_integer()) :: DateTime.t()
  def seconds_ago(seconds) do
    now() |&gt; DateTime.add(-seconds, :second)
  end

  @doc &quot;&quot;&quot;
  Returns a `DateTime` that is `hours` hours before the current `now()`.
  &quot;&quot;&quot;
  @spec hours_ago(non_neg_integer()) :: DateTime.t()
  def hours_ago(hours) do
    now() |&gt; DateTime.add(-hours * 3_600, :second)
  end

  @doc &quot;&quot;&quot;
  Converts a DateTime to Unix timestamp in milliseconds.
  &quot;&quot;&quot;
  @spec to_unix(DateTime.t()) :: integer()
  def to_unix(%DateTime{} = dt) do
    DateTime.to_unix(dt, :millisecond)
  end

  # ============================================================================
  # Time Parsing Functions (from TimeHandler)
  # ============================================================================

  @doc &quot;&quot;&quot;
  Parses timestamps in a killmail.
  &quot;&quot;&quot;
  @spec parse_times(killmail()) :: {:ok, killmail()} | {:error, term()}
  def parse_times(killmail) do
    with {:ok, kill_time} &lt;- parse_kill_time(Map.get(killmail, &quot;killTime&quot;)),
         {:ok, zkb_time} &lt;- parse_zkb_time(get_in(killmail, [&quot;zkb&quot;, &quot;time&quot;])) do
      killmail = Map.put(killmail, &quot;killTime&quot;, kill_time)
      killmail = put_in(killmail, [&quot;zkb&quot;, &quot;time&quot;], zkb_time)
      {:ok, killmail}
    else
      error -&gt;
        Logger.error(&quot;Failed to parse times in killmail: #{inspect(error)}&quot;)
        error
    end
  end

  @doc &quot;&quot;&quot;
  Validates and attaches a killmail&apos;s timestamp against a cutoff.
  Returns:
    - `{:ok, {km_with_time, dt}}` if valid
    - `:older` if timestamp is before cutoff
    - `:skip` if timestamp is missing or unparseable
  &quot;&quot;&quot;
  @spec validate_killmail_time(killmail(), DateTime.t()) :: validation_result()
  def validate_killmail_time(km, cutoff_dt) do
    case get_killmail_time(km) do
      {:ok, km_dt} -&gt;
        if older_than_cutoff?(km_dt, cutoff_dt) do
          :older
        else
          km_with_time = Map.put(km, &quot;kill_time&quot;, km_dt)
          {:ok, {km_with_time, km_dt}}
        end

      {:error, reason} -&gt;
        Logger.warning(
          &quot;[Clock] Failed to parse time for killmail #{inspect(Map.get(km, &quot;killmail_id&quot;))}: #{inspect(reason)}&quot;
        )

        :skip
    end
  end

  @doc &quot;&quot;&quot;
  Gets the killmail time from any supported format.
  Returns `{:ok, DateTime.t()}` or `{:error, reason}`.
  &quot;&quot;&quot;
  @spec get_killmail_time(killmail()) :: time_result()
  def get_killmail_time(%{&quot;killmail_time&quot; =&gt; value}), do: parse_time(value)
  def get_killmail_time(%{&quot;killTime&quot; =&gt; value}), do: parse_time(value)
  def get_killmail_time(%{&quot;zkb&quot; =&gt; %{&quot;time&quot; =&gt; value}}), do: parse_time(value)
  def get_killmail_time(_), do: {:error, :missing_time}

  @doc &quot;&quot;&quot;
  Parses a time value from various formats into a DateTime.
  &quot;&quot;&quot;
  @spec parse_time(String.t() | DateTime.t() | any()) :: time_result()
  def parse_time(dt) when is_struct(dt, DateTime), do: {:ok, dt}

  def parse_time(time_str) when is_binary(time_str) do
    case DateTime.from_iso8601(time_str) do
      {:ok, dt, _offset} -&gt;
        {:ok, DateTime.shift_zone!(dt, &quot;Etc/UTC&quot;)}

      {:error, :invalid_format} -&gt;
        case NaiveDateTime.from_iso8601(time_str) do
          {:ok, ndt} -&gt;
            {:ok, DateTime.from_naive!(ndt, &quot;Etc/UTC&quot;)}

          error -&gt;
            log_time_parse_error(time_str, error)
            error
        end

      error -&gt;
        log_time_parse_error(time_str, error)
        error
    end
  end

  def parse_time(_), do: {:error, :invalid_time_format}

  @doc &quot;&quot;&quot;
  Converts a DateTime to an ISO8601 string for storage in cache.
  &quot;&quot;&quot;
  @spec datetime_to_string(DateTime.t() | any()) :: String.t() | nil
  def datetime_to_string(%DateTime{} = dt), do: DateTime.to_iso8601(dt)
  def datetime_to_string(_), do: nil

  # ============================================================================
  # Private Functions
  # ============================================================================

  defp parse_kill_time(time) when is_binary(time) do
    case DateTime.from_iso8601(time) do
      {:ok, datetime, _} -&gt; {:ok, datetime}
      error -&gt; error
    end
  end

  defp parse_kill_time(_), do: {:error, :invalid_kill_time}

  defp parse_zkb_time(time) when is_binary(time) do
    case DateTime.from_iso8601(time) do
      {:ok, datetime, _} -&gt; {:ok, datetime}
      error -&gt; error
    end
  end

  defp parse_zkb_time(_), do: {:error, :invalid_zkb_time}

  defp log_time_parse_error(time_str, error) do
    Logger.warning(&quot;[Clock] Failed to parse time: #{time_str}, error: #{inspect(error)}&quot;)
  end

  defp older_than_cutoff?(km_dt, cutoff_dt), do: DateTime.compare(km_dt, cutoff_dt) == :lt
end</file><file path="lib/wanderer_kills/infrastructure/config.ex">defmodule WandererKills.Infrastructure.Config do
  @moduledoc &quot;&quot;&quot;
  Centralized configuration management for WandererKills.

  This module provides a unified interface for all application configuration,
  including runtime settings, constants, and environment-specific values.

  ## Configuration Groups

  - **Cache**: TTL settings and cache behavior
  - **Retry**: Retry policies and backoff strategies
  - **Batch**: Concurrency and batch processing settings
  - **Timeouts**: Request timeout configurations
  - **HTTP Status**: Status code handling rules
  - **Services**: External service URLs
  - **RedisQ**: Queue processing configuration
  - **Parser**: Killmail parsing settings
  - **Enricher**: Data enrichment configuration
  - **Killmail Store**: Storage and GC settings
  - **Telemetry**: Metrics and monitoring configuration
  - **App**: Application-level settings
  - **Constants**: Core application constants

  ## Usage

  ```elixir
  # Get specific configuration groups
  cache_config = Config.cache()
  retry_config = Config.retry()

  # Get individual values
  timeout = Config.timeouts().esi_request_ms
  max_retries = Config.retry().http_max_retries

  # Get constants
  timeout = Config.gen_server_call_timeout()
  max_id = Config.validation(:max_killmail_id)
  ```
  &quot;&quot;&quot;

  # ============================================================================
  # Core Constants
  # ============================================================================

  # Timeout Configuration (Use Config module for runtime-configurable timeouts)
  @gen_server_call_timeout 5_000

  # Retry Configuration (Use Config module for runtime-configurable retry settings)
  @default_base_delay 1_000
  @max_backoff_delay 60_000
  @backoff_factor 2

  # Validation Limits
  @max_killmail_id 999_999_999_999
  @max_system_id 32_000_000
  @max_character_id 999_999_999_999

  defstruct [
    # Cache settings
    cache: %{
      killmails_ttl: 3600,
      system_ttl: 1800,
      esi_ttl: 3600,
      esi_killmail_ttl: 86_400,
      recent_fetch_threshold: 5
    },
    # Retry settings
    retry: %{
      http_max_retries: 3,
      http_base_delay: 1000,
      http_max_delay: 30_000,
      redisq_max_retries: 5,
      redisq_base_delay: 500
    },
    # Batch processing settings
    batch: %{
      concurrency_esi: 10,
      concurrency_zkb: 5,
      concurrency_default: 5,
      batch_size: 100
    },
    # Request timeout settings
    timeouts: %{
      esi_request_ms: 30_000,
      zkb_request_ms: 15_000,
      http_request_ms: 10_000,
      default_request_ms: 10_000
    },
    # HTTP status codes
    http_status: %{
      success: 200..299,
      not_found: 404,
      rate_limited: 429,
      retryable: [408, 429, 500, 502, 503, 504],
      fatal: [400, 401, 403, 405]
    },
    # Service URLs
    services: %{
      esi_base_url: &quot;https://esi.evetech.net/latest&quot;,
      zkb_base_url: &quot;https://zkillboard.com/api&quot;,
      redisq_base_url: nil
    },
    # RedisQ specific settings
    redisq: %{
      fast_interval_ms: 1_000,
      idle_interval_ms: 5_000,
      initial_backoff_ms: 1_000,
      max_backoff_ms: 30_000,
      backoff_factor: 2,
      task_timeout_ms: 10_000
    },
    # Parser settings
    parser: %{
      cutoff_seconds: 3_600,
      summary_interval_ms: 60_000
    },
    # Enricher settings
    enricher: %{
      max_concurrency: 10,
      task_timeout_ms: 30_000,
      min_attackers_for_parallel: 3
    },
    # Killmail store settings
    killmail_store: %{
      gc_interval_ms: 60_000,
      max_events_per_system: 10_000
    },
    # Telemetry settings
    telemetry: %{
      enabled_metrics: [:cache, :api, :circuit, :event],
      sampling_rate: 1.0,
      retention_period: 604_800
    },
    # Application settings
    app: %{
      port: 4004,
      http_client: &quot;WandererKills.Http.Client&quot;,
      zkb_client: WandererKills.Killmails.ZkbClient
    }
  ]

  @type config :: %__MODULE__{}

  @doc &quot;Gets the complete configuration struct with runtime values&quot;
  @spec config() :: config()
  def config do
    %__MODULE__{
      cache: %{
        killmails_ttl: get_env(:cache_killmails_ttl, 3600),
        system_ttl: get_env(:cache_system_ttl, 1800),
        esi_ttl: get_env(:cache_esi_ttl, 3600),
        esi_killmail_ttl: get_env(:cache_esi_killmail_ttl, 86_400),
        recent_fetch_threshold: get_env(:cache_system_recent_fetch_threshold, 5)
      },
      retry: %{
        http_max_retries: get_env(:retry_http_max_retries, 3),
        http_base_delay: get_env(:retry_http_base_delay, 1000),
        http_max_delay: get_env(:retry_http_max_delay, 30_000),
        redisq_max_retries: get_env(:retry_redisq_max_retries, 5),
        redisq_base_delay: get_env(:retry_redisq_base_delay, 500)
      },
      batch: %{
        concurrency_esi: get_env(:esi_batch_concurrency, 10),
        concurrency_zkb: get_env(:zkb_batch_concurrency, 5),
        concurrency_default: get_env(:default_batch_concurrency, 5),
        batch_size: get_env(:concurrency_batch_size, 100)
      },
      timeouts: %{
        esi_request_ms: get_env(:esi_request_timeout_ms, 30_000),
        zkb_request_ms: get_env(:zkb_request_timeout_ms, 15_000),
        http_request_ms: get_env(:http_request_timeout_ms, 10_000),
        default_request_ms: get_env(:default_request_timeout_ms, 10_000)
      },
      http_status: %{
        success: get_env(:http_status_success, 200..299),
        not_found: get_env(:http_status_not_found, 404),
        rate_limited: get_env(:http_status_rate_limited, 429),
        retryable: get_env(:http_status_retryable, [408, 429, 500, 502, 503, 504]),
        fatal: get_env(:http_status_fatal, [400, 401, 403, 405])
      },
      services: %{
        esi_base_url: get_env(:esi_base_url, &quot;https://esi.evetech.net/latest&quot;),
        zkb_base_url: get_env(:zkb_base_url, &quot;https://zkillboard.com/api&quot;),
        redisq_base_url: get_env(:redisq_base_url, nil)
      },
      redisq: %{
        fast_interval_ms: get_env(:redisq_fast_interval_ms, 1_000),
        idle_interval_ms: get_env(:redisq_idle_interval_ms, 5_000),
        initial_backoff_ms: get_env(:redisq_initial_backoff_ms, 1_000),
        max_backoff_ms: get_env(:redisq_max_backoff_ms, 30_000),
        backoff_factor: get_env(:redisq_backoff_factor, 2),
        task_timeout_ms: get_env(:redisq_task_timeout_ms, 10_000)
      },
      parser: %{
        cutoff_seconds: get_env(:parser_cutoff_seconds, 3_600),
        summary_interval_ms: get_env(:parser_summary_interval_ms, 60_000)
      },
      enricher: %{
        max_concurrency: get_env(:enricher_max_concurrency, 10),
        task_timeout_ms: get_env(:enricher_task_timeout_ms, 30_000),
        min_attackers_for_parallel: get_env(:enricher_min_attackers_for_parallel, 3)
      },
      killmail_store: %{
        gc_interval_ms: get_env(:killmail_store_gc_interval_ms, 60_000),
        max_events_per_system: get_env(:killmail_store_max_events_per_system, 10_000)
      },
      telemetry: %{
        enabled_metrics: get_env(:telemetry_enabled_metrics, [:cache, :api, :circuit, :event]),
        sampling_rate: get_env(:telemetry_sampling_rate, 1.0),
        retention_period: get_env(:telemetry_retention_period, 604_800)
      },
      app: %{
        port: get_env(:port, 4004),
        http_client: get_env(:http_client, &quot;WandererKills.Http.Client&quot;),
        zkb_client: get_env(:zkb_client, WandererKills.Killmails.ZkbClient)
      }
    }
  end

  # Convenience accessors for each configuration group
  @doc &quot;Gets cache configuration&quot;
  @spec cache() :: map()
  def cache, do: config().cache

  @doc &quot;Gets retry configuration&quot;
  @spec retry() :: map()
  def retry, do: config().retry

  @doc &quot;Gets batch processing configuration&quot;
  @spec batch() :: map()
  def batch, do: config().batch

  @doc &quot;Gets timeout configuration&quot;
  @spec timeouts() :: map()
  def timeouts, do: config().timeouts

  @doc &quot;Gets HTTP status configuration&quot;
  @spec http_status() :: map()
  def http_status, do: config().http_status

  @doc &quot;Gets service URLs configuration&quot;
  @spec services() :: map()
  def services, do: config().services

  @doc &quot;Gets RedisQ configuration&quot;
  @spec redisq() :: map()
  def redisq do
    get_env(:redisq, %{
      task_timeout_ms: 10_000,
      fast_interval_ms: 1_000,
      idle_interval_ms: 5_000,
      initial_backoff_ms: 1_000,
      max_backoff_ms: 30_000,
      backoff_factor: 2
    })
  end

  @doc &quot;Gets parser configuration&quot;
  @spec parser() :: map()
  def parser, do: config().parser

  @doc &quot;Gets enricher configuration&quot;
  @spec enricher() :: map()
  def enricher, do: config().enricher

  @doc &quot;Gets killmail store configuration&quot;
  @spec killmail_store() :: map()
  def killmail_store, do: config().killmail_store

  @doc &quot;Gets telemetry configuration&quot;
  @spec telemetry() :: map()
  def telemetry, do: config().telemetry

  @doc &quot;Gets application configuration&quot;
  @spec app() :: map()
  def app, do: config().app

  # ============================================================================
  # Constants API
  # ============================================================================

  @doc &quot;&quot;&quot;
  Gets GenServer call timeout in milliseconds.

  This is a true constant used for GenServer.call timeouts.
  For HTTP request timeouts, use `Config.timeouts().default_request_ms`.
  &quot;&quot;&quot;
  @spec gen_server_call_timeout() :: integer()
  def gen_server_call_timeout, do: @gen_server_call_timeout

  @doc &quot;&quot;&quot;
  Gets retry base delay in milliseconds.

  This is an algorithmic constant for exponential backoff calculations.
  &quot;&quot;&quot;
  @spec retry_base_delay() :: integer()
  def retry_base_delay, do: @default_base_delay

  @doc &quot;&quot;&quot;
  Gets maximum retry delay in milliseconds.

  This is an algorithmic constant for exponential backoff calculations.
  &quot;&quot;&quot;
  @spec retry_max_delay() :: integer()
  def retry_max_delay, do: @max_backoff_delay

  @doc &quot;&quot;&quot;
  Gets retry backoff factor.

  This is an algorithmic constant for exponential backoff calculations.
  &quot;&quot;&quot;
  @spec retry_backoff_factor() :: integer()
  def retry_backoff_factor, do: @backoff_factor

  @doc &quot;&quot;&quot;
  Gets validation limits.
  &quot;&quot;&quot;
  @spec validation(atom()) :: integer()
  def validation(type) do
    case type do
      :max_killmail_id -&gt; @max_killmail_id
      :max_system_id -&gt; @max_system_id
      :max_character_id -&gt; @max_character_id
    end
  end

  # Non-deprecated utility functions
  @doc &quot;Checks if preloader should start&quot;
  @spec start_preloader?() :: boolean()
  def start_preloader?, do: get_env(:start_preloader, true)

  @doc &quot;Checks if RedisQ should start&quot;
  @spec start_redisq?() :: boolean()
  def start_redisq?, do: get_env(:start_redisq, true)

  # Private helper function
  defp get_env(key, default) do
    Application.get_env(:wanderer_kills, key, default)
  end
end</file><file path="lib/wanderer_kills/infrastructure/error.ex">defmodule WandererKills.Infrastructure.Error do
  @moduledoc &quot;&quot;&quot;
  Centralized error handling for WandererKills.

  This module provides a unified error structure and helper functions for all
  error handling across the application, replacing disparate error tuple patterns
  with a consistent approach.

  ## Error Structure

  All errors have a standardized format with:
  - `domain` - Which part of the system generated the error
  - `type` - Specific error type within the domain
  - `message` - Human-readable error message
  - `details` - Additional error context (optional)
  - `retryable` - Whether the operation can be retried

  ## Usage

  ```elixir
  # HTTP errors
  {:error, Error.http_error(:timeout, &quot;Request timed out&quot;, true)}

  # Cache errors
  {:error, Error.cache_error(:miss, &quot;Cache key not found&quot;)}

  # Killmail processing errors
  {:error, Error.killmail_error(:invalid_format, &quot;Missing required fields&quot;)}

  # Checking if error is retryable
  if Error.retryable?(error) do
    retry_operation()
  end

  # Creating standardized errors
  Error.not_found_error(&quot;Resource not found&quot;)
  ```
  &quot;&quot;&quot;

  defstruct [:domain, :type, :message, :details, :retryable]

  @type domain ::
          :http
          | :cache
          | :killmail
          | :system
          | :esi
          | :zkb
          | :parsing
          | :enrichment
          | :redis_q
          | :ship_types
          | :validation
          | :config
          | :time
          | :csv
  @type error_type :: atom()
  @type details :: map() | nil

  @type t :: %__MODULE__{
          domain: domain(),
          type: error_type(),
          message: String.t(),
          details: details(),
          retryable: boolean()
        }

  # ============================================================================
  # Constructor Functions by Domain
  # ============================================================================

  @doc &quot;Creates an HTTP-related error&quot;
  @spec http_error(error_type(), String.t(), boolean(), details()) :: t()
  def http_error(type, message, retryable \\ false, details \\ nil) do
    %__MODULE__{
      domain: :http,
      type: type,
      message: message,
      details: details,
      retryable: retryable
    }
  end

  @doc &quot;Creates a cache-related error&quot;
  @spec cache_error(error_type(), String.t(), details()) :: t()
  def cache_error(type, message, details \\ nil) do
    %__MODULE__{
      domain: :cache,
      type: type,
      message: message,
      details: details,
      retryable: false
    }
  end

  @doc &quot;Creates a killmail processing error&quot;
  @spec killmail_error(error_type(), String.t(), boolean(), details()) :: t()
  def killmail_error(type, message, retryable \\ false, details \\ nil) do
    %__MODULE__{
      domain: :killmail,
      type: type,
      message: message,
      details: details,
      retryable: retryable
    }
  end

  @doc &quot;Creates a system-related error&quot;
  @spec system_error(error_type(), String.t(), boolean(), details()) :: t()
  def system_error(type, message, retryable \\ false, details \\ nil) do
    %__MODULE__{
      domain: :system,
      type: type,
      message: message,
      details: details,
      retryable: retryable
    }
  end

  @doc &quot;Creates an ESI API error&quot;
  @spec esi_error(error_type(), String.t(), boolean(), details()) :: t()
  def esi_error(type, message, retryable \\ false, details \\ nil) do
    %__MODULE__{
      domain: :esi,
      type: type,
      message: message,
      details: details,
      retryable: retryable
    }
  end

  @doc &quot;Creates a zKillboard API error&quot;
  @spec zkb_error(error_type(), String.t(), boolean(), details()) :: t()
  def zkb_error(type, message, retryable \\ false, details \\ nil) do
    %__MODULE__{
      domain: :zkb,
      type: type,
      message: message,
      details: details,
      retryable: retryable
    }
  end

  @doc &quot;Creates a parsing error&quot;
  @spec parsing_error(error_type(), String.t(), details()) :: t()
  def parsing_error(type, message, details \\ nil) do
    %__MODULE__{
      domain: :parsing,
      type: type,
      message: message,
      details: details,
      retryable: false
    }
  end

  @doc &quot;Creates an enrichment error&quot;
  @spec enrichment_error(error_type(), String.t(), boolean(), details()) :: t()
  def enrichment_error(type, message, retryable \\ false, details \\ nil) do
    %__MODULE__{
      domain: :enrichment,
      type: type,
      message: message,
      details: details,
      retryable: retryable
    }
  end

  @doc &quot;Creates a RedisQ error&quot;
  @spec redisq_error(error_type(), String.t(), boolean(), details()) :: t()
  def redisq_error(type, message, retryable \\ true, details \\ nil) do
    %__MODULE__{
      domain: :redis_q,
      type: type,
      message: message,
      details: details,
      retryable: retryable
    }
  end

  @doc &quot;Creates a ship types error&quot;
  @spec ship_types_error(error_type(), String.t(), boolean(), details()) :: t()
  def ship_types_error(type, message, retryable \\ false, details \\ nil) do
    %__MODULE__{
      domain: :ship_types,
      type: type,
      message: message,
      details: details,
      retryable: retryable
    }
  end

  @doc &quot;Creates a validation error&quot;
  @spec validation_error(error_type(), String.t(), details()) :: t()
  def validation_error(type, message, details \\ nil) do
    %__MODULE__{
      domain: :validation,
      type: type,
      message: message,
      details: details,
      retryable: false
    }
  end

  @doc &quot;Creates a configuration error&quot;
  @spec config_error(error_type(), String.t(), details()) :: t()
  def config_error(type, message, details \\ nil) do
    %__MODULE__{
      domain: :config,
      type: type,
      message: message,
      details: details,
      retryable: false
    }
  end

  @doc &quot;Creates a time processing error&quot;
  @spec time_error(error_type(), String.t(), details()) :: t()
  def time_error(type, message, details \\ nil) do
    %__MODULE__{
      domain: :time,
      type: type,
      message: message,
      details: details,
      retryable: false
    }
  end

  @doc &quot;Creates a CSV processing error&quot;
  @spec csv_error(error_type(), String.t(), details()) :: t()
  def csv_error(type, message, details \\ nil) do
    %__MODULE__{
      domain: :csv,
      type: type,
      message: message,
      details: details,
      retryable: false
    }
  end

  # ============================================================================
  # Utility Functions
  # ============================================================================

  @doc &quot;Checks if an error is retryable&quot;
  @spec retryable?(t()) :: boolean()
  def retryable?(%__MODULE__{retryable: retryable}), do: retryable

  @doc &quot;Gets the error domain&quot;
  @spec domain(t()) :: domain()
  def domain(%__MODULE__{domain: domain}), do: domain

  @doc &quot;Gets the error type&quot;
  @spec type(t()) :: error_type()
  def type(%__MODULE__{type: type}), do: type

  @doc &quot;Gets the error message&quot;
  @spec message(t()) :: String.t()
  def message(%__MODULE__{message: message}), do: message

  @doc &quot;Gets the error details&quot;
  @spec details(t()) :: details()
  def details(%__MODULE__{details: details}), do: details

  @doc &quot;Converts an error to a string representation&quot;
  @spec to_string(t()) :: String.t()
  def to_string(%__MODULE__{domain: domain, type: type, message: message}) do
    &quot;[#{domain}:#{type}] #{message}&quot;
  end

  @doc &quot;Converts an error to a map for serialization&quot;
  @spec to_map(t()) :: map()
  def to_map(%__MODULE__{} = error) do
    %{
      domain: error.domain,
      type: error.type,
      message: error.message,
      details: error.details,
      retryable: error.retryable
    }
  end

  # ============================================================================
  # Common Error Patterns
  # ============================================================================

  @doc &quot;Standard timeout error&quot;
  @spec timeout_error(String.t(), details()) :: t()
  def timeout_error(message \\ &quot;Operation timed out&quot;, details \\ nil) do
    http_error(:timeout, message, true, details)
  end

  @doc &quot;Standard not found error&quot;
  @spec not_found_error(String.t(), details()) :: t()
  def not_found_error(message \\ &quot;Resource not found&quot;, details \\ nil) do
    system_error(:not_found, message, false, details)
  end

  @doc &quot;Standard invalid format error&quot;
  @spec invalid_format_error(String.t(), details()) :: t()
  def invalid_format_error(message \\ &quot;Invalid data format&quot;, details \\ nil) do
    validation_error(:invalid_format, message, details)
  end

  @doc &quot;Standard rate limit error&quot;
  @spec rate_limit_error(String.t(), details()) :: t()
  def rate_limit_error(message \\ &quot;Rate limit exceeded&quot;, details \\ nil) do
    http_error(:rate_limited, message, true, details)
  end

  @doc &quot;Standard connection error&quot;
  @spec connection_error(String.t(), details()) :: t()
  def connection_error(message \\ &quot;Connection failed&quot;, details \\ nil) do
    http_error(:connection_failed, message, true, details)
  end

  # ============================================================================
  # HTTP Exception Types
  # ============================================================================

  defmodule ConnectionError do
    @moduledoc &quot;&quot;&quot;
    Error raised when a connection fails.
    &quot;&quot;&quot;
    defexception [:message]
  end

  defmodule TimeoutError do
    @moduledoc &quot;&quot;&quot;
    Error raised when a request times out.
    &quot;&quot;&quot;
    defexception [:message]
  end

  defmodule RateLimitError do
    @moduledoc &quot;&quot;&quot;
    Error raised when rate limit is exceeded.
    &quot;&quot;&quot;
    defexception [:message]
  end
end</file><file path="lib/wanderer_kills/infrastructure/retry.ex">defmodule WandererKills.Infrastructure.Retry do
  @moduledoc &quot;&quot;&quot;
  Provides retry functionality with exponential backoff for any operation.

  This module consolidates retry logic from across the application into a single,
  reusable implementation. It handles:
  - Exponential backoff with configurable parameters
  - Retryable error detection
  - Logging of retry attempts
  - Custom error types for different failure scenarios

  Originally designed for HTTP requests but generalized to handle any retryable operation.
  &quot;&quot;&quot;

  require Logger
  alias WandererKills.Infrastructure.Config

  @type retry_opts :: [
          max_retries: non_neg_integer(),
          base_delay: non_neg_integer(),
          max_delay: non_neg_integer(),
          rescue_only: [module()],
          operation_name: String.t()
        ]

  @doc &quot;&quot;&quot;
  Retries a function with exponential backoff.

  ## Parameters
    - `fun` - A zero-arity function that either returns a value or raises one of the specified errors
    - `opts` - Retry options:
      - `:max_retries` - Maximum number of retry attempts (default: 3)
      - `:base_delay` - Initial delay in milliseconds (default: 1000)
      - `:max_delay` - Maximum delay in milliseconds (default: 30000)
      - `:rescue_only` - List of exception types to retry on (default: all common retryable errors)
      - `:operation_name` - Name for logging purposes (default: &quot;operation&quot;)

  ## Returns
    - `{:ok, result}` on successful execution
    - `{:error, :max_retries_exceeded}` when max retries are reached
  &quot;&quot;&quot;
  @spec retry_with_backoff((-&gt; term()), retry_opts()) :: {:ok, term()} | {:error, term()}
  def retry_with_backoff(fun, opts \\ []) do
    max_retries = Keyword.get(opts, :max_retries, Config.retry().http_max_retries)
    base_delay = Keyword.get(opts, :base_delay, Config.retry().http_base_delay)
    max_delay = Keyword.get(opts, :max_delay, Config.retry().http_max_delay)
    operation_name = Keyword.get(opts, :operation_name, &quot;operation&quot;)

    rescue_only =
      Keyword.get(opts, :rescue_only, [
        WandererKills.Infrastructure.Error.ConnectionError,
        WandererKills.Infrastructure.Error.TimeoutError,
        WandererKills.Infrastructure.Error.RateLimitError,
        # Add common retryable exceptions
        RuntimeError,
        ArgumentError
      ])

    # Create an Erlang backoff state: init(StartDelay, MaxDelay)
    backoff_state = :backoff.init(base_delay, max_delay)

    do_retry(fun, max_retries, backoff_state, rescue_only, operation_name)
  end

  @spec do_retry((-&gt; term()), non_neg_integer(), :backoff.backoff(), [module()], String.t()) ::
          {:ok, term()} | {:error, term()}
  defp do_retry(_fun, 0, _backoff_state, _rescue_only, operation_name) do
    Logger.error(&quot;#{operation_name} failed after exhausting all retry attempts&quot;)
    {:error, :max_retries_exceeded}
  end

  defp do_retry(fun, retries_left, backoff_state, rescue_only, operation_name) do
    result = fun.()
    {:ok, result}
  rescue
    error -&gt;
      if error.__struct__ in rescue_only do
        # Each time we fail, we call :backoff.fail/1 → {delay_ms, next_backoff}
        {delay_ms, next_backoff} = :backoff.fail(backoff_state)

        Logger.warning(
          &quot;#{operation_name} failed with retryable error: #{inspect(error)}. &quot; &lt;&gt;
            &quot;Retrying in #{delay_ms}ms (#{retries_left - 1} attempts left).&quot;
        )

        Process.sleep(delay_ms)
        do_retry(fun, retries_left - 1, next_backoff, rescue_only, operation_name)
      else
        # Not one of our listed retriable errors: bubble up immediately
        Logger.error(&quot;#{operation_name} failed with non-retryable error: #{inspect(error)}&quot;)
        reraise(error, __STACKTRACE__)
      end
  end

  @doc &quot;&quot;&quot;
  Determines if an error is retriable for HTTP operations.

  ## Parameters
    - `reason` - Error reason to check

  ## Returns
    - `true` - If error should be retried
    - `false` - If error should not be retried
  &quot;&quot;&quot;
  @spec retriable_http_error?(term()) :: boolean()
  def retriable_http_error?(:rate_limited), do: true
  def retriable_http_error?(%WandererKills.Infrastructure.Error.RateLimitError{}), do: true
  def retriable_http_error?(%WandererKills.Infrastructure.Error.TimeoutError{}), do: true
  def retriable_http_error?(%WandererKills.Infrastructure.Error.ConnectionError{}), do: true
  def retriable_http_error?(_), do: false

  @doc &quot;&quot;&quot;
  Alias for retriable_http_error?/1 for backward compatibility.
  &quot;&quot;&quot;
  @spec retriable_error?(term()) :: boolean()
  def retriable_error?(reason), do: retriable_http_error?(reason)

  @doc &quot;&quot;&quot;
  Convenience function for retrying HTTP operations with sensible defaults.

  ## Parameters
    - `fun` - Function to retry
    - `opts` - Options (same as retry_with_backoff/2)

  ## Returns
    - `{:ok, result}` on success
    - `{:error, reason}` on failure
  &quot;&quot;&quot;
  @spec retry_http_operation((-&gt; term()), retry_opts()) :: {:ok, term()} | {:error, term()}
  def retry_http_operation(fun, opts \\ []) do
    default_opts = [
      operation_name: &quot;HTTP request&quot;,
      rescue_only: [
        WandererKills.Infrastructure.Error.ConnectionError,
        WandererKills.Infrastructure.Error.TimeoutError,
        WandererKills.Infrastructure.Error.RateLimitError
      ]
    ]

    merged_opts = Keyword.merge(default_opts, opts)
    retry_with_backoff(fun, merged_opts)
  end
end</file><file path="lib/wanderer_kills/killmails/coordinator.ex">defmodule WandererKills.Killmails.Coordinator do
  @moduledoc &quot;&quot;&quot;
  Main parser coordinator that handles the parsing pipeline.

  This module provides functionality to:
  - Parse full killmails from ESI
  - Parse partial killmails from system listings
  - Enrich killmail data with additional information
  - Store parsed killmails in the cache
  - Handle time-based filtering of killmails

  ## Features

  - Full killmail parsing and enrichment
  - Partial killmail handling with ESI fallback
  - Automatic data merging and validation
  - Time-based filtering of old killmails
  - Error handling and logging
  - Cache integration

  ## Usage

  ```elixir
  # Parse a full killmail
  {:ok, enriched} = Coordinator.parse_full_and_store(full_killmail, partial_killmail, cutoff_time)

  # Parse a partial killmail
  {:ok, enriched} = Coordinator.parse_partial(partial_killmail, cutoff_time)

  # Handle skipped kills
  {:ok, :kill_skipped} = Coordinator.parse_partial(old_killmail, cutoff_time)
  ```

  ## Data Flow

  1. Full killmails:
     - Merge full and partial data
     - Build kill data structure
     - Enrich with additional information
     - Store in cache

  2. Partial killmails:
     - Fetch full data from ESI
     - Process as full killmail
     - Skip if too old
     - Handle errors appropriately

  ## Error Handling

  All functions return either:
  - `{:ok, killmail}` - On successful parsing
  - `{:ok, :kill_skipped}` - When killmail is too old
  - `:older` - When killmail is older than cutoff
  - `{:error, Error.t()}` - On failure with standardized error
  &quot;&quot;&quot;

  require Logger
  alias WandererKills.Infrastructure.Error
  alias WandererKills.Killmails.Parser
  alias WandererKills.Killmails.Store

  @type killmail :: map()
  @type raw_killmail :: map()

  @doc &quot;&quot;&quot;
  Parses a full killmail with enrichment and stores it.

  ## Parameters
  - `full` - The full killmail data from ESI
  - `partial` - The partial killmail data with zkb info
  - `cutoff` - DateTime cutoff for filtering old killmails

  ## Returns
  - `{:ok, enriched_killmail}` - On successful parsing and enrichment
  - `{:error, Error.t()}` - On failure with standardized error

  ## Examples

  ```elixir
  # Parse a full killmail
  full = %{&quot;killmail_id&quot; =&gt; 12345, &quot;victim&quot; =&gt; %{...}}
  partial = %{&quot;zkb&quot; =&gt; %{&quot;hash&quot; =&gt; &quot;abc123&quot;}}
  cutoff = Clock.now()

  {:ok, enriched} = parse_full_and_store(full, partial, cutoff)

  # Handle invalid format
  {:error, %Error{}} = parse_full_and_store(invalid_data, invalid_data, cutoff)
  ```
  &quot;&quot;&quot;
  @spec parse_full_and_store(killmail(), killmail(), DateTime.t()) ::
          {:ok, killmail()} | {:ok, :kill_older} | {:error, Error.t()}
  def parse_full_and_store(full, %{&quot;zkb&quot; =&gt; zkb}, cutoff) when is_map(full) do
    Logger.debug(&quot;Starting to parse and store killmail&quot;, %{
      killmail_id: full[&quot;killmail_id&quot;],
      operation: :parse_full_and_store,
      step: :start
    })

    process_killmail(full, zkb, cutoff)
  end

  def parse_full_and_store(_, _, _) do
    {:error, Error.killmail_error(:invalid_format, &quot;Invalid payload format for killmail parsing&quot;)}
  end

  @spec process_killmail(killmail(), map(), DateTime.t()) ::
          {:ok, killmail()} | {:ok, :kill_older} | {:error, Error.t()}
  defp process_killmail(full, zkb, cutoff) do
    # Merge zkb data into the full killmail
    merged = Map.put(full, &quot;zkb&quot;, zkb)

    case parse_killmail_with_cutoff(merged, cutoff, full[&quot;killmail_id&quot;]) do
      {:ok, :kill_older} -&gt;
        # Return immediately for older kills, don&apos;t try to enrich
        {:ok, :kill_older}

      {:ok, parsed} -&gt;
        # Continue with enrichment pipeline for valid killmails
        with {:ok, enriched} &lt;- enrich_and_log_killmail(parsed, full[&quot;killmail_id&quot;]),
             {:ok, system_id} &lt;- extract_system_id(enriched, full[&quot;killmail_id&quot;]) do
          store_killmail_async(system_id, enriched, full[&quot;killmail_id&quot;])
          {:ok, enriched}
        end

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  @spec parse_killmail_with_cutoff(killmail(), DateTime.t(), term()) ::
          {:ok, killmail()} | {:ok, :kill_older} | {:error, Error.t()}
  defp parse_killmail_with_cutoff(merged, cutoff, killmail_id) do
    case Parser.parse_full_killmail(merged, cutoff) do
      {:ok, :kill_older} -&gt;
        Logger.debug(&quot;Killmail is older than cutoff&quot;, %{
          killmail_id: killmail_id,
          operation: :process_killmail,
          status: :kill_older
        })

        {:ok, :kill_older}

      {:ok, parsed} -&gt;
        {:ok, parsed}

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to parse killmail&quot;, %{
          killmail_id: killmail_id,
          operation: :process_killmail,
          error: reason,
          status: :error
        })

        {:error, reason}
    end
  end

  @spec enrich_and_log_killmail(killmail(), term()) :: {:ok, killmail()} | {:error, Error.t()}
  defp enrich_and_log_killmail(parsed, killmail_id) do
    case enrich_killmail(parsed) do
      {:ok, enriched} -&gt;
        {:ok, enriched}

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to enrich killmail&quot;, %{
          killmail_id: killmail_id,
          operation: :process_killmail,
          error: reason,
          status: :error
        })

        {:error, reason}
    end
  end

  @spec extract_system_id(killmail(), term()) :: {:ok, integer()} | {:error, Error.t()}
  defp extract_system_id(enriched, killmail_id) do
    system_id = enriched[&quot;solar_system_id&quot;] || enriched[&quot;system_id&quot;]

    if system_id do
      {:ok, system_id}
    else
      Logger.error(&quot;Missing system_id in enriched killmail&quot;, %{
        killmail_id: killmail_id,
        operation: :process_killmail,
        status: :error
      })

      {:error,
       Error.killmail_error(
         :missing_system_id,
         &quot;System ID missing from enriched killmail&quot;,
         false,
         %{killmail_id: killmail_id}
       )}
    end
  end

  @spec store_killmail_async(integer(), killmail(), term()) :: :ok
  defp store_killmail_async(system_id, enriched, killmail_id) do
    Task.start(fn -&gt;
      try do
        killmail_id = enriched[&quot;killmail_id&quot;]
        :ok = Store.put(killmail_id, system_id, enriched)

        Logger.debug(&quot;Successfully enriched and stored killmail&quot;, %{
          killmail_id: killmail_id,
          system_id: system_id,
          operation: :process_killmail,
          status: :success
        })
      rescue
        # Only rescue specific known exception types to avoid masking bugs
        error in [ArgumentError] -&gt;
          Logger.error(&quot;Invalid arguments when storing killmail&quot;, %{
            killmail_id: killmail_id,
            system_id: system_id,
            operation: :process_killmail,
            error: Exception.message(error),
            status: :error
          })

        error in [BadMapError] -&gt;
          Logger.error(&quot;Invalid killmail data structure&quot;, %{
            killmail_id: killmail_id,
            system_id: system_id,
            operation: :process_killmail,
            error: Exception.message(error),
            status: :error
          })
      end
    end)

    :ok
  end

  @spec enrich_killmail(killmail()) :: {:ok, killmail()} | {:error, Error.t()}
  defp enrich_killmail(killmail) do
    WandererKills.Killmails.Enricher.enrich_killmail(killmail)
  end

  @doc &quot;&quot;&quot;
  Parses a partial killmail by fetching the full data from ESI.

  ## Parameters
  - `partial` - The partial killmail data with zkb info
  - `cutoff` - DateTime cutoff for filtering old killmails

  ## Returns
  - `{:ok, enriched_killmail}` - On successful parsing
  - `{:ok, :kill_skipped}` - When killmail is too old
  - `:older` - When killmail is older than cutoff
  - `{:error, Error.t()}` - On failure with standardized error

  ## Examples

  ```elixir
  # Parse a partial killmail
  partial = %{
    &quot;killmail_id&quot; =&gt; 12345,
    &quot;zkb&quot; =&gt; %{&quot;hash&quot; =&gt; &quot;abc123&quot;}
  }
  cutoff = Clock.now()

  {:ok, enriched} = parse_partial(partial, cutoff)

  # Handle old killmail
  {:ok, :kill_skipped} = parse_partial(old_killmail, cutoff)

  # Handle invalid format
  {:error, %Error{}} = parse_partial(invalid_data, cutoff)
  ```
  &quot;&quot;&quot;
  @spec parse_partial(raw_killmail(), DateTime.t()) ::
          {:ok, killmail()} | {:ok, :kill_skipped} | :older | {:error, Error.t()}
  def parse_partial(%{&quot;killID&quot; =&gt; id, &quot;zkb&quot; =&gt; %{&quot;hash&quot; =&gt; hash}} = partial, cutoff) do
    Logger.debug(&quot;Starting to parse partial killmail&quot;, %{
      killmail_id: id,
      operation: :parse_partial,
      step: :start
    })

    case WandererKills.ESI.Client.get_killmail_raw(id, hash) do
      {:ok, full} -&gt;
        Logger.debug(&quot;Successfully fetched full killmail from ESI&quot;, %{
          killmail_id: id,
          operation: :fetch_from_esi,
          status: :success
        })

        parse_full_and_store(full, partial, cutoff)

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to fetch full killmail&quot;, %{
          killmail_id: id,
          operation: :fetch_from_esi,
          error: reason,
          status: :error
        })

        {:error, reason}
    end
  end

  def parse_partial(_, _) do
    {:error,
     Error.killmail_error(
       :invalid_format,
       &quot;Invalid partial killmail format - missing required fields&quot;
     )}
  end

  @doc &quot;&quot;&quot;
  Processes a list of raw killmails from zKillboard.

  This function handles the complete processing pipeline:
  1. Parse raw killmails
  2. Filter by time constraints
  3. Enrich with additional data

  ## Parameters
  - `raw_killmails` - List of raw killmail data from ZKB
  - `system_id` - The system ID (used for logging)
  - `since_hours` - Only process killmails newer than this many hours

  ## Returns
  - `{:ok, [enriched_killmail]}` - On successful processing
  - `{:error, reason}` - On failure
  &quot;&quot;&quot;
  @spec process_killmails([map()], pos_integer(), pos_integer()) ::
          {:ok, [killmail()]} | {:error, term()}
  def process_killmails(raw_killmails, system_id, since_hours)
      when is_list(raw_killmails) and is_integer(system_id) and is_integer(since_hours) do
    Logger.debug(&quot;Processing killmails&quot;,
      system_id: system_id,
      raw_count: length(raw_killmails),
      since_hours: since_hours,
      operation: :process_killmails,
      step: :start
    )

    with {:ok, parsed_killmails} &lt;- parse_killmails(raw_killmails, since_hours),
         {:ok, enriched_killmails} &lt;- enrich_killmails(parsed_killmails, system_id) do
      Logger.debug(&quot;Successfully processed killmails&quot;,
        system_id: system_id,
        raw_count: length(raw_killmails),
        parsed_count: length(parsed_killmails),
        enriched_count: length(enriched_killmails),
        operation: :process_killmails,
        step: :success
      )

      {:ok, enriched_killmails}
    else
      {:error, reason} -&gt;
        Logger.error(&quot;Failed to process killmails&quot;,
          system_id: system_id,
          raw_count: length(raw_killmails),
          error: reason,
          operation: :process_killmails,
          step: :error
        )

        {:error, reason}
    end
  end

  def process_killmails(invalid_killmails, _system_id, _since_hours) do
    {:error,
     Error.validation_error(
       :invalid_type,
       &quot;Killmails must be a list, got: #{inspect(invalid_killmails)}&quot;
     )}
  end

  @doc &quot;&quot;&quot;
  Processes a single killmail.
  &quot;&quot;&quot;
  @spec process_single_killmail(map(), boolean()) :: {:ok, killmail()} | {:error, term()}
  def process_single_killmail(raw_killmail, enrich \\ true) do
    cutoff_time = DateTime.utc_now() |&gt; DateTime.add(-24 * 60 * 60, :second)

    case Parser.parse_partial_killmail(raw_killmail, cutoff_time) do
      {:ok, parsed} when enrich -&gt;
        case WandererKills.Killmails.Enricher.enrich_killmail(parsed) do
          {:ok, enriched} -&gt; {:ok, enriched}
          # Fall back to basic data
          {:error, _reason} -&gt; {:ok, parsed}
        end

      {:ok, parsed} -&gt;
        {:ok, parsed}

      {:error, reason} -&gt;
        {:error, reason}
    end
  rescue
    # Only rescue specific known exception types
    error in [ArgumentError] -&gt;
      Logger.error(&quot;Invalid arguments during killmail processing&quot;,
        error: Exception.message(error),
        operation: :process_single_killmail
      )

      {:error, Error.validation_error(:invalid_arguments, Exception.message(error))}

    error in [BadMapError] -&gt;
      Logger.error(&quot;Invalid killmail data structure&quot;,
        error: Exception.message(error),
        operation: :process_single_killmail
      )

      {:error, Error.killmail_error(:invalid_format, Exception.message(error))}
  end

  @doc &quot;&quot;&quot;
  Parses raw killmails with time filtering.
  &quot;&quot;&quot;
  @spec parse_killmails([map()], pos_integer()) :: {:ok, [killmail()]} | {:error, term()}
  def parse_killmails(raw_killmails, since_hours)
      when is_list(raw_killmails) and is_integer(since_hours) do
    # Calculate cutoff time
    cutoff_time = DateTime.utc_now() |&gt; DateTime.add(-since_hours * 60 * 60, :second)

    Logger.debug(&quot;Parsing killmails with time filter&quot;,
      raw_count: length(raw_killmails),
      since_hours: since_hours,
      cutoff_time: cutoff_time,
      operation: :parse_killmails,
      step: :start
    )

    parsed =
      raw_killmails
      |&gt; Enum.map(&amp;Parser.parse_partial_killmail(&amp;1, cutoff_time))
      |&gt; Enum.filter(fn
        {:ok, _} -&gt; true
        _ -&gt; false
      end)
      |&gt; Enum.flat_map(fn
        {:ok, killmail} when is_map(killmail) -&gt; [killmail]
        {:ok, killmails} when is_list(killmails) -&gt; killmails
      end)

    Logger.debug(&quot;Successfully parsed killmails&quot;,
      raw_count: length(raw_killmails),
      parsed_count: length(parsed),
      parser_type: &quot;partial_killmail&quot;,
      cutoff_time: cutoff_time,
      operation: :parse_killmails,
      step: :success
    )

    {:ok, parsed}
  rescue
    error -&gt;
      Logger.error(&quot;Exception during killmail parsing&quot;,
        raw_count: length(raw_killmails),
        error: inspect(error),
        operation: :parse_killmails,
        step: :exception
      )

      {:error, Error.parsing_error(:exception, &quot;Exception during killmail parsing&quot;)}
  end

  def parse_killmails(invalid_killmails, _since_hours) do
    {:error,
     Error.validation_error(
       :invalid_type,
       &quot;Killmails must be a list, got: #{inspect(invalid_killmails)}&quot;
     )}
  end

  @doc &quot;&quot;&quot;
  Enriches parsed killmails with additional information.
  &quot;&quot;&quot;
  @spec enrich_killmails([killmail()], pos_integer()) :: {:ok, [killmail()]} | {:error, term()}
  def enrich_killmails(parsed_killmails, system_id)
      when is_list(parsed_killmails) and is_integer(system_id) do
    Logger.debug(&quot;Enriching killmails&quot;,
      system_id: system_id,
      parsed_count: length(parsed_killmails),
      operation: :enrich_killmails,
      step: :start
    )

    enriched =
      parsed_killmails
      |&gt; Enum.map(fn killmail -&gt;
        case WandererKills.Killmails.Enricher.enrich_killmail(killmail) do
          {:ok, enriched} -&gt;
            enriched

          # Fall back to original if enrichment fails
          {:error, reason} -&gt;
            Logger.debug(&quot;Enrichment failed for killmail, using basic data&quot;,
              killmail_id: Map.get(killmail, &quot;killmail_id&quot;),
              system_id: system_id,
              error: reason,
              operation: :enrich_killmails,
              step: :fallback
            )

            killmail
        end
      end)

    Logger.debug(&quot;Successfully enriched killmails&quot;,
      system_id: system_id,
      parsed_count: length(parsed_killmails),
      enriched_count: length(enriched),
      operation: :enrich_killmails,
      step: :success
    )

    {:ok, enriched}
  rescue
    error -&gt;
      Logger.error(&quot;Exception during killmail enrichment&quot;,
        system_id: system_id,
        parsed_count: length(parsed_killmails),
        error: inspect(error),
        operation: :enrich_killmails,
        step: :exception
      )

      {:error, Error.enrichment_error(:exception, &quot;Exception during killmail enrichment&quot;)}
  end

  def enrich_killmails(invalid_killmails, _system_id) do
    {:error,
     Error.validation_error(
       :invalid_type,
       &quot;Killmails must be a list, got: #{inspect(invalid_killmails)}&quot;
     )}
  end
end</file><file path="lib/wanderer_kills/killmails/enricher.ex">defmodule WandererKills.Killmails.Enricher do
  @moduledoc &quot;&quot;&quot;
  Enriches killmails with additional information.

  This module handles the enrichment of killmail data with additional
  information such as character, corporation, alliance, and ship details.
  It supports both sequential and parallel processing of attackers
  depending on the number of attackers in the killmail.
  &quot;&quot;&quot;

  require Logger
  alias WandererKills.ESI.Client, as: EsiClient
  alias WandererKills.Infrastructure.Config
  alias WandererKills.ShipTypes.Info, as: ShipTypeInfo

  @doc &quot;&quot;&quot;
  Enriches a killmail with additional information.

  ## Parameters
  - `killmail` - The killmail map to enrich

  ## Returns
  - `{:ok, enriched_killmail}` - On successful enrichment
  - `{:error, reason}` - On failure

  ## Examples

  ```elixir
  {:ok, enriched} = Enricher.enrich_killmail(raw_killmail)
  ```
  &quot;&quot;&quot;
  @spec enrich_killmail(map()) :: {:ok, map()} | {:error, term()}
  def enrich_killmail(killmail) do
    with {:ok, killmail} &lt;- enrich_victim(killmail),
         {:ok, killmail} &lt;- enrich_attackers(killmail),
         {:ok, killmail} &lt;- enrich_ship(killmail),
         {:ok, killmail} &lt;- flatten_enriched_data(killmail) do
      {:ok, killmail}
    else
      error -&gt;
        Logger.error(&quot;Failed to enrich killmail: #{inspect(error)}&quot;)
        error
    end
  end

  defp enrich_victim(killmail) do
    victim = Map.get(killmail, &quot;victim&quot;, %{})

    with {:ok, character} &lt;- get_character_info(Map.get(victim, &quot;character_id&quot;)),
         {:ok, corporation} &lt;- get_corporation_info(Map.get(victim, &quot;corporation_id&quot;)) do
      # Alliance is optional - handle separately
      alliance = get_alliance_info_safe(Map.get(victim, &quot;alliance_id&quot;))

      victim =
        victim
        |&gt; Map.put(&quot;character&quot;, character)
        |&gt; Map.put(&quot;corporation&quot;, corporation)
        |&gt; Map.put(&quot;alliance&quot;, alliance)

      killmail = Map.put(killmail, &quot;victim&quot;, victim)
      {:ok, killmail}
    end
  end

  defp enrich_attackers(killmail) do
    attackers = Map.get(killmail, &quot;attackers&quot;, [])

    enricher_config = %{
      min_attackers_for_parallel: Config.enricher().min_attackers_for_parallel,
      max_concurrency: Config.enricher().max_concurrency,
      task_timeout_ms: Config.enricher().task_timeout_ms
    }

    enriched_attackers =
      if length(attackers) &gt;= enricher_config.min_attackers_for_parallel do
        process_attackers_parallel(attackers, enricher_config)
      else
        process_attackers_sequential(attackers)
      end

    {:ok, Map.put(killmail, &quot;attackers&quot;, enriched_attackers)}
  end

  @spec process_attackers_parallel([map()], map()) :: [map()]
  defp process_attackers_parallel(attackers, enricher_config) when is_list(attackers) do
    Task.Supervisor.async_stream_nolink(
      WandererKills.TaskSupervisor,
      attackers,
      fn attacker -&gt;
        case enrich_attacker(attacker) do
          {:ok, enriched} -&gt; enriched
          {:error, _} -&gt; nil
        end
      end,
      max_concurrency: enricher_config.max_concurrency,
      timeout: enricher_config.task_timeout_ms
    )
    |&gt; Stream.map(fn
      {:ok, result} -&gt; result
      {:exit, _} -&gt; nil
    end)
    |&gt; Stream.filter(&amp; &amp;1)
    |&gt; Enum.to_list()
  end

  @spec process_attackers_sequential([map()]) :: [map()]
  defp process_attackers_sequential(attackers) when is_list(attackers) do
    Enum.map(attackers, fn attacker -&gt;
      case enrich_attacker(attacker) do
        {:ok, enriched} -&gt; enriched
        {:error, _} -&gt; nil
      end
    end)
    |&gt; Enum.filter(&amp; &amp;1)
  end

  @spec enrich_attacker(map()) :: {:ok, map()} | {:error, term()}
  defp enrich_attacker(attacker) do
    with {:ok, character} &lt;- get_character_info(Map.get(attacker, &quot;character_id&quot;)),
         {:ok, corporation} &lt;- get_corporation_info(Map.get(attacker, &quot;corporation_id&quot;)) do
      # Alliance is optional - handle separately
      alliance = get_alliance_info_safe(Map.get(attacker, &quot;alliance_id&quot;))

      attacker =
        attacker
        |&gt; Map.put(&quot;character&quot;, character)
        |&gt; Map.put(&quot;corporation&quot;, corporation)
        |&gt; Map.put(&quot;alliance&quot;, alliance)

      {:ok, attacker}
    else
      error -&gt;
        Logger.warning(&quot;Failed to enrich attacker: #{inspect(error)}&quot;)
        {:error, error}
    end
  end

  defp enrich_ship(killmail) do
    victim = Map.get(killmail, &quot;victim&quot;, %{})
    ship_type_id = Map.get(victim, &quot;ship_type_id&quot;)

    ship =
      case ShipTypeInfo.get_ship_type(ship_type_id) do
        {:ok, ship_data} -&gt; ship_data
        _ -&gt; nil
      end

    victim = Map.put(victim, &quot;ship&quot;, ship)
    killmail = Map.put(killmail, &quot;victim&quot;, victim)
    {:ok, killmail}
  end

  defp get_character_info(id) when is_integer(id), do: EsiClient.get_character(id)
  defp get_character_info(_), do: {:ok, nil}

  defp get_corporation_info(id) when is_integer(id), do: EsiClient.get_corporation(id)
  defp get_corporation_info(_), do: {:ok, nil}

  defp get_alliance_info(id) when is_integer(id) and id &gt; 0, do: EsiClient.get_alliance(id)
  defp get_alliance_info(_), do: {:ok, nil}

  defp get_alliance_info_safe(id) when is_integer(id) do
    case get_alliance_info(id) do
      {:ok, alliance} -&gt; alliance
      _ -&gt; nil
    end
  end

  defp get_alliance_info_safe(_), do: nil

  defp flatten_enriched_data(killmail) do
    try do
      flattened =
        killmail
        |&gt; flatten_victim_data()
        |&gt; flatten_attackers_data()
        |&gt; add_attacker_count()

      {:ok, flattened}
    rescue
      error -&gt;
        Logger.warning(&quot;Failed to flatten enriched data&quot;, error: inspect(error))
        # Return original killmail if flattening fails
        {:ok, killmail}
    end
  end

  defp flatten_victim_data(killmail) do
    victim = Map.get(killmail, &quot;victim&quot;, %{})

    # Extract names from nested enriched data and add to victim object
    flattened_victim =
      victim
      |&gt; add_character_name(get_in(victim, [&quot;character&quot;, &quot;name&quot;]))
      |&gt; add_corporation_info()
      |&gt; add_alliance_info()
      |&gt; add_ship_name()

    Map.put(killmail, &quot;victim&quot;, flattened_victim)
  end

  defp flatten_attackers_data(killmail) do
    attackers = Map.get(killmail, &quot;attackers&quot;, [])

    # Flatten enriched data for each attacker
    flattened_attackers =
      Enum.map(attackers, fn attacker -&gt;
        attacker
        |&gt; add_character_name(get_in(attacker, [&quot;character&quot;, &quot;name&quot;]))
        |&gt; add_corporation_info()
        |&gt; add_alliance_info()
        |&gt; add_ship_name_for_attacker()
      end)

    Map.put(killmail, &quot;attackers&quot;, flattened_attackers)
  end

  defp add_character_name(entity, character_name) do
    entity
    |&gt; Map.put(&quot;character_name&quot;, character_name)
    # Alternative field name
    |&gt; Map.put(&quot;name&quot;, character_name)
  end

  defp add_corporation_info(entity) do
    corp_name = get_in(entity, [&quot;corporation&quot;, &quot;name&quot;])
    corp_ticker = get_in(entity, [&quot;corporation&quot;, &quot;ticker&quot;])

    entity
    |&gt; Map.put(&quot;corporation_name&quot;, corp_name)
    # Alternative field name
    |&gt; Map.put(&quot;corp_name&quot;, corp_name)
    |&gt; Map.put(&quot;corporation_ticker&quot;, corp_ticker)
    # Alternative field name
    |&gt; Map.put(&quot;corp_ticker&quot;, corp_ticker)
  end

  defp add_alliance_info(entity) do
    alliance_name = get_in(entity, [&quot;alliance&quot;, &quot;name&quot;])
    alliance_ticker = get_in(entity, [&quot;alliance&quot;, &quot;ticker&quot;])

    entity
    |&gt; Map.put(&quot;alliance_name&quot;, alliance_name)
    |&gt; Map.put(&quot;alliance_ticker&quot;, alliance_ticker)
  end

  defp add_ship_name(entity) do
    ship_name = get_in(entity, [&quot;ship&quot;, &quot;name&quot;])

    entity
    |&gt; Map.put(&quot;ship_name&quot;, ship_name)
    # Alternative field name
    |&gt; Map.put(&quot;ship_type_name&quot;, ship_name)
  end

  defp add_ship_name_for_attacker(attacker) do
    # For attackers, get ship name from ship type ID
    ship_type_id = Map.get(attacker, &quot;ship_type_id&quot;)

    ship_name =
      case ShipTypeInfo.get_ship_type(ship_type_id) do
        {:ok, ship_data} -&gt; Map.get(ship_data, &quot;name&quot;)
        _ -&gt; nil
      end

    attacker
    |&gt; Map.put(&quot;ship_name&quot;, ship_name)
    # Alternative field name
    |&gt; Map.put(&quot;ship_type_name&quot;, ship_name)
  end

  defp add_attacker_count(killmail) do
    attackers = Map.get(killmail, &quot;attackers&quot;, [])
    attacker_count = length(attackers)

    Map.put(killmail, &quot;attacker_count&quot;, attacker_count)
  end
end</file><file path="lib/wanderer_kills/killmails/parser.ex">defmodule WandererKills.Killmails.Parser do
  @moduledoc &quot;&quot;&quot;
  Core killmail parsing functionality with a focused API.

  This module provides the essential killmail parsing operations while keeping
  internal implementation details private. It follows a consistent naming
  convention and minimizes the public API surface.

  ## Public API

  - `parse_full_killmail/2` - Parse a complete killmail with zkb data
  - `parse_partial_killmail/2` - Parse a partial killmail, fetching full data
  - `merge_killmail_data/2` - Merge ESI and zKB data
  - `validate_killmail_time/1` - Validate killmail timestamp

  ## Usage

  ```elixir
  # Parse a complete killmail
  {:ok, parsed} = KillmailParser.parse_full_killmail(killmail, cutoff_time)

  # Parse a partial killmail
  {:ok, parsed} = KillmailParser.parse_partial_killmail(partial, cutoff_time)

  # Merge ESI and zKB data
  {:ok, merged} = KillmailParser.merge_killmail_data(esi_data, zkb_data)
  ```

  ## Error Handling

  All functions return standardized results:
  - `{:ok, result}` - On success
  - `{:ok, :kill_older}` - When killmail is older than cutoff
  - `{:error, reason}` - On failure
  &quot;&quot;&quot;

  require Logger

  alias WandererKills.Cache.Helper
  alias WandererKills.Infrastructure.Error
  alias WandererKills.Killmails.Enricher
  alias WandererKills.Observability.Monitoring

  @type killmail :: map()
  @type raw_killmail :: map()
  @type merged_killmail :: map()
  @type parse_result :: {:ok, killmail()} | {:ok, :kill_older} | {:error, term()}

  @doc &quot;&quot;&quot;
  Parses a complete killmail with zkb data.

  This is the main entry point for parsing killmails when you have both
  the full ESI data and zKB metadata.

  ## Parameters
  - `killmail` - The merged killmail data (ESI + zKB)
  - `cutoff_time` - DateTime cutoff for filtering old killmails

  ## Returns
  - `{:ok, parsed_killmail}` - On successful parsing
  - `{:ok, :kill_older}` - When killmail is older than cutoff
  - `{:error, reason}` - On failure
  &quot;&quot;&quot;
  @spec parse_full_killmail(killmail(), DateTime.t()) :: parse_result()
  def parse_full_killmail(killmail, cutoff_time) when is_map(killmail) do
    killmail_id = get_killmail_id(killmail)

    Logger.debug(&quot;Parsing full killmail&quot;,
      killmail_id: killmail_id,
      has_solar_system_id: Map.has_key?(killmail, &quot;solar_system_id&quot;),
      has_victim: Map.has_key?(killmail, &quot;victim&quot;),
      has_attackers: Map.has_key?(killmail, &quot;attackers&quot;),
      has_zkb: Map.has_key?(killmail, &quot;zkb&quot;),
      killmail_keys: Map.keys(killmail) |&gt; Enum.sort()
    )

    with {:ok, validated} &lt;- validate_killmail_structure(killmail),
         {:ok, time_checked} &lt;- check_killmail_time(validated, cutoff_time),
         {:ok, built} &lt;- build_killmail_data(time_checked),
         {:ok, enriched} &lt;- enrich_killmail_data(built) do
      Monitoring.increment_stored()
      {:ok, enriched}
    else
      {:error, %WandererKills.Infrastructure.Error{type: :kill_too_old}} -&gt;
        Monitoring.increment_skipped()
        {:ok, :kill_older}

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to parse killmail&quot;,
          killmail_id: get_killmail_id(killmail),
          error: reason,
          step: determine_failure_step(reason),
          killmail_sample: inspect(killmail, limit: 3, printable_limit: 100)
        )

        {:error, reason}
    end
  end

  @doc &quot;&quot;&quot;
  Parses a partial killmail by fetching full data from ESI.

  ## Parameters
  - `partial` - The partial killmail data with zKB metadata
  - `cutoff_time` - DateTime cutoff for filtering old killmails

  ## Returns
  - `{:ok, parsed_killmail}` - On successful parsing
  - `{:ok, :kill_older}` - When killmail is older than cutoff
  - `{:error, reason}` - On failure
  &quot;&quot;&quot;
  @spec parse_partial_killmail(raw_killmail(), DateTime.t()) :: parse_result()
  def parse_partial_killmail(%{&quot;killID&quot; =&gt; id, &quot;zkb&quot; =&gt; zkb} = partial, cutoff_time) do
    Logger.debug(&quot;Parsing partial killmail&quot;, killmail_id: id)

    with {:ok, full_data} &lt;- fetch_full_killmail(id, zkb),
         {:ok, merged} &lt;- merge_killmail_data(full_data, partial) do
      parse_full_killmail(merged, cutoff_time)
    else
      {:error, reason} -&gt;
        Logger.error(&quot;Failed to parse partial killmail&quot;, killmail_id: id, error: reason)
        {:error, reason}
    end
  end

  def parse_partial_killmail(_, _),
    do:
      {:error,
       Error.killmail_error(
         :invalid_partial_format,
         &quot;Partial killmail must have killID and zkb fields&quot;
       )}

  @doc &quot;&quot;&quot;
  Merges ESI killmail data with zKB metadata.

  ## Parameters
  - `esi_data` - Full killmail data from ESI
  - `zkb_data` - Partial data with zKB metadata

  ## Returns
  - `{:ok, merged_killmail}` - On successful merge
  - `{:error, reason}` - On failure
  &quot;&quot;&quot;
  @spec merge_killmail_data(killmail(), raw_killmail()) ::
          {:ok, merged_killmail()} | {:error, term()}
  def merge_killmail_data(%{&quot;killmail_id&quot; =&gt; id} = esi_data, %{&quot;zkb&quot; =&gt; zkb})
      when is_integer(id) and is_map(zkb) do
    kill_time = get_kill_time_field(esi_data)

    if kill_time do
      merged =
        esi_data
        |&gt; Map.put(&quot;zkb&quot;, zkb)
        |&gt; Map.put(&quot;kill_time&quot;, kill_time)

      {:ok, merged}
    else
      {:error, Error.killmail_error(:missing_kill_time, &quot;Kill time not found in ESI data&quot;)}
    end
  end

  def merge_killmail_data(_, _),
    do:
      {:error,
       Error.killmail_error(:invalid_merge_data, &quot;Invalid data format for merge operation&quot;)}

  @doc &quot;&quot;&quot;
  Validates killmail timestamp and parses it.

  ## Parameters
  - `killmail` - Killmail data containing time information

  ## Returns
  - `{:ok, datetime}` - On successful parsing
  - `{:error, reason}` - On failure
  &quot;&quot;&quot;
  @spec validate_killmail_time(killmail()) :: {:ok, DateTime.t()} | {:error, term()}
  def validate_killmail_time(%{&quot;killmail_time&quot; =&gt; time}) when is_binary(time) do
    case DateTime.from_iso8601(time) do
      {:ok, dt, _} -&gt;
        {:ok, dt}

      {:error, reason} -&gt;
        {:error,
         Error.killmail_error(:invalid_time_format, &quot;Failed to parse ISO8601 timestamp&quot;, false, %{
           underlying_error: reason
         })}
    end
  end

  def validate_killmail_time(%{&quot;kill_time&quot; =&gt; time}) when is_binary(time) do
    validate_killmail_time(%{&quot;killmail_time&quot; =&gt; time})
  end

  def validate_killmail_time(_),
    do: {:error, Error.killmail_error(:missing_kill_time, &quot;Killmail missing valid time field&quot;)}

  # Private functions for internal implementation

  @spec get_killmail_id(killmail()) :: integer() | nil
  defp get_killmail_id(%{&quot;killmail_id&quot; =&gt; id}) when is_integer(id), do: id
  defp get_killmail_id(%{&quot;killID&quot; =&gt; id}) when is_integer(id), do: id
  defp get_killmail_id(_), do: nil

  @spec validate_killmail_structure(killmail()) :: {:ok, killmail()} | {:error, term()}
  defp validate_killmail_structure(%{&quot;killmail_id&quot; =&gt; id} = killmail) when is_integer(id) do
    required_fields = [&quot;solar_system_id&quot;, &quot;victim&quot;, &quot;attackers&quot;]

    missing_fields =
      required_fields
      |&gt; Enum.reject(&amp;Map.has_key?(killmail, &amp;1))

    if Enum.empty?(missing_fields) do
      {:ok, killmail}
    else
      Logger.error(&quot;[Parser] Killmail structure validation failed&quot;,
        killmail_id: id,
        required_fields: required_fields,
        missing_fields: missing_fields,
        available_keys: Map.keys(killmail),
        killmail_sample: killmail |&gt; inspect(limit: 5, printable_limit: 200)
      )

      {:error,
       Error.killmail_error(
         :missing_required_fields,
         &quot;Killmail missing required ESI fields&quot;,
         false,
         %{
           missing_fields: missing_fields,
           required_fields: required_fields
         }
       )}
    end
  end

  defp validate_killmail_structure(killmail) when is_map(killmail) do
    Logger.error(&quot;[Parser] Killmail missing killmail_id field&quot;,
      available_keys: Map.keys(killmail),
      killmail_sample: killmail |&gt; inspect(limit: 5, printable_limit: 200)
    )

    {:error, Error.killmail_error(:missing_killmail_id, &quot;Killmail missing killmail_id field&quot;)}
  end

  @spec determine_failure_step(term()) :: String.t()
  defp determine_failure_step(%Error{type: :missing_required_fields}), do: &quot;structure_validation&quot;
  defp determine_failure_step(%Error{type: :missing_killmail_id}), do: &quot;structure_validation&quot;
  defp determine_failure_step(%Error{type: :invalid_time_format}), do: &quot;time_validation&quot;
  defp determine_failure_step(%Error{type: :missing_kill_time}), do: &quot;time_validation&quot;
  defp determine_failure_step(%Error{type: :kill_too_old}), do: &quot;time_check&quot;
  defp determine_failure_step(%Error{type: :build_failed}), do: &quot;data_building&quot;
  defp determine_failure_step(_), do: &quot;unknown&quot;

  @spec check_killmail_time(killmail(), DateTime.t()) :: {:ok, killmail()} | {:error, Error.t()}
  defp check_killmail_time(killmail, cutoff_time) do
    case validate_killmail_time(killmail) do
      {:ok, kill_time} -&gt;
        if DateTime.compare(kill_time, cutoff_time) == :lt do
          Logger.debug(&quot;Killmail is older than cutoff&quot;,
            killmail_id: get_killmail_id(killmail),
            kill_time: DateTime.to_iso8601(kill_time),
            cutoff: DateTime.to_iso8601(cutoff_time)
          )

          {:error,
           Error.killmail_error(:kill_too_old, &quot;Killmail is older than cutoff time&quot;, false, %{
             kill_time: DateTime.to_iso8601(kill_time),
             cutoff: DateTime.to_iso8601(cutoff_time)
           })}
        else
          {:ok, Map.put(killmail, &quot;parsed_kill_time&quot;, kill_time)}
        end

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  @spec build_killmail_data(killmail()) :: {:ok, killmail()} | {:error, term()}
  defp build_killmail_data(killmail) do
    # Extract and structure the core killmail data
    structured = %{
      &quot;killmail_id&quot; =&gt; killmail[&quot;killmail_id&quot;],
      &quot;kill_time&quot; =&gt; killmail[&quot;parsed_kill_time&quot;],
      &quot;solar_system_id&quot; =&gt; killmail[&quot;solar_system_id&quot;],
      &quot;victim&quot; =&gt; normalize_victim_data(killmail[&quot;victim&quot;]),
      &quot;attackers&quot; =&gt; normalize_attackers_data(killmail[&quot;attackers&quot;]),
      &quot;zkb&quot; =&gt; killmail[&quot;zkb&quot;] || %{},
      &quot;total_value&quot; =&gt; get_in(killmail, [&quot;zkb&quot;, &quot;totalValue&quot;]) || 0,
      &quot;npc&quot; =&gt; get_in(killmail, [&quot;zkb&quot;, &quot;npc&quot;]) || false
    }

    {:ok, structured}
  rescue
    error -&gt;
      Logger.error(&quot;Failed to build killmail data&quot;, error: inspect(error))

      {:error,
       Error.killmail_error(:build_failed, &quot;Failed to build killmail data structure&quot;, false, %{
         exception: inspect(error)
       })}
  end

  @spec enrich_killmail_data(killmail()) :: {:ok, killmail()} | {:error, term()}
  defp enrich_killmail_data(killmail) do
    case Enricher.enrich_killmail(killmail) do
      {:ok, enriched} -&gt;
        # Store in cache after successful enrichment
        Helper.killmail_put(enriched[&quot;killmail_id&quot;], enriched)
        {:ok, enriched}

      {:error, reason} -&gt;
        Logger.warning(&quot;Failed to enrich killmail, using basic data&quot;,
          killmail_id: killmail[&quot;killmail_id&quot;],
          error: reason
        )

        # Store basic data even if enrichment fails
        Helper.killmail_put(killmail[&quot;killmail_id&quot;], killmail)
        {:ok, killmail}
    end
  end

  @spec fetch_full_killmail(integer(), map()) :: {:ok, killmail()} | {:error, term()}
  defp fetch_full_killmail(killmail_id, zkb) do
    hash = zkb[&quot;hash&quot;]

    # Try to get from cache first, then fetch from ESI if needed
    case Helper.killmail_get(killmail_id) do
      {:ok, full_data} -&gt;
        {:ok, full_data}

      {:error, %WandererKills.Infrastructure.Error{type: :not_found}} -&gt;
        # Fetch full killmail data from ESI
        case WandererKills.ESI.Client.get_killmail_raw(killmail_id, hash) do
          {:ok, esi_data} when is_map(esi_data) -&gt;
            # Cache the result
            Helper.killmail_put(killmail_id, esi_data)
            {:ok, esi_data}

          {:error, reason} -&gt;
            Logger.error(&quot;Failed to fetch full killmail from ESI&quot;,
              killmail_id: killmail_id,
              hash: hash,
              error: reason
            )

            {:error, reason}
        end

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  @spec get_kill_time_field(killmail()) :: String.t() | nil
  defp get_kill_time_field(killmail) do
    killmail[&quot;kill_time&quot;] || killmail[&quot;killmail_time&quot;]
  end

  @spec normalize_victim_data(map()) :: map()
  defp normalize_victim_data(victim) when is_map(victim) do
    %{
      &quot;character_id&quot; =&gt; victim[&quot;character_id&quot;],
      &quot;corporation_id&quot; =&gt; victim[&quot;corporation_id&quot;],
      &quot;alliance_id&quot; =&gt; victim[&quot;alliance_id&quot;],
      &quot;ship_type_id&quot; =&gt; victim[&quot;ship_type_id&quot;],
      &quot;damage_taken&quot; =&gt; victim[&quot;damage_taken&quot;],
      &quot;items&quot; =&gt; victim[&quot;items&quot;] || []
    }
  end

  @spec normalize_attackers_data([map()]) :: [map()]
  defp normalize_attackers_data(attackers) when is_list(attackers) do
    Enum.map(attackers, fn attacker -&gt;
      %{
        &quot;character_id&quot; =&gt; attacker[&quot;character_id&quot;],
        &quot;corporation_id&quot; =&gt; attacker[&quot;corporation_id&quot;],
        &quot;alliance_id&quot; =&gt; attacker[&quot;alliance_id&quot;],
        &quot;ship_type_id&quot; =&gt; attacker[&quot;ship_type_id&quot;],
        &quot;weapon_type_id&quot; =&gt; attacker[&quot;weapon_type_id&quot;],
        &quot;damage_done&quot; =&gt; attacker[&quot;damage_done&quot;],
        &quot;final_blow&quot; =&gt; attacker[&quot;final_blow&quot;] || false,
        &quot;security_status&quot; =&gt; attacker[&quot;security_status&quot;]
      }
    end)
  end
end</file><file path="lib/wanderer_kills/killmails/store.ex">defmodule WandererKills.Killmails.Store do
  @moduledoc &quot;&quot;&quot;
  Simplified ETS-backed killmail storage with pattern-matching query support.

  This module provides a clean API over ETS tables for storing and querying
  killmails. ETS is used here specifically because we need efficient pattern
  matching queries like &quot;give me all kills for system X&quot;.

  The module exposes only the core operations needed for killmail storage
  without the complexity of a GenServer.
  &quot;&quot;&quot;

  require Logger

  # ETS tables for killmail storage
  @killmail_table :killmails
  @system_killmails_table :system_killmails
  @system_fetch_timestamps_table :system_fetch_timestamps

  @type kill_id :: integer()
  @type system_id :: integer()
  @type kill_data :: map()

  @doc &quot;&quot;&quot;
  Initializes all required ETS tables at application start.

  This should be called from Application.start/2 before starting the supervision tree.
  &quot;&quot;&quot;
  @spec init_tables!() :: :ok
  def init_tables! do
    # Main killmail storage table
    :ets.new(@killmail_table, [:set, :named_table, :public, {:read_concurrency, true}])

    # System-specific killmail lists
    :ets.new(@system_killmails_table, [:set, :named_table, :public, {:read_concurrency, true}])

    # System fetch timestamps
    :ets.new(@system_fetch_timestamps_table, [
      :set,
      :named_table,
      :public,
      {:read_concurrency, true}
    ])

    Logger.info(
      &quot;Initialized KillStore ETS tables: #{inspect([@killmail_table, @system_killmails_table, @system_fetch_timestamps_table])}&quot;
    )

    :ok
  end

  # ============================================================================
  # Core Killmail Storage API
  # ============================================================================

  @doc &quot;&quot;&quot;
  Stores a killmail in the store.
  &quot;&quot;&quot;
  @spec put(kill_id(), system_id(), kill_data()) :: :ok
  def put(kill_id, system_id, kill_data)
      when is_integer(kill_id) and is_integer(system_id) and is_map(kill_data) do
    :ets.insert(@killmail_table, {kill_id, kill_data})

    # Associate with system
    case :ets.lookup(@system_killmails_table, system_id) do
      [] -&gt;
        :ets.insert(@system_killmails_table, {system_id, [kill_id]})

      [{^system_id, existing_ids}] -&gt;
        # Ensure we don&apos;t add duplicates
        if kill_id not in existing_ids do
          :ets.insert(@system_killmails_table, {system_id, [kill_id | existing_ids]})
        end
    end

    :ok
  end

  @doc &quot;&quot;&quot;
  Retrieves a killmail by ID.
  &quot;&quot;&quot;
  @spec get(kill_id()) :: {:ok, kill_data()} | :error
  def get(kill_id) when is_integer(kill_id) do
    case :ets.lookup(@killmail_table, kill_id) do
      [{^kill_id, data}] -&gt; {:ok, data}
      [] -&gt; :error
    end
  end

  @doc &quot;&quot;&quot;
  Lists all killmails for a specific system.
  &quot;&quot;&quot;
  @spec list_by_system(system_id()) :: [kill_data()]
  def list_by_system(system_id) when is_integer(system_id) do
    case :ets.lookup(@system_killmails_table, system_id) do
      [{^system_id, killmail_ids}] -&gt;
        Enum.flat_map(killmail_ids, &amp;get_killmail_data/1)

      [] -&gt;
        []
    end
  end

  # ============================================================================
  # Private Helper Functions
  # ============================================================================

  @spec get_killmail_data(kill_id()) :: [kill_data()]
  defp get_killmail_data(killmail_id) do
    case :ets.lookup(@killmail_table, killmail_id) do
      [{^killmail_id, killmail_data}] -&gt; [killmail_data]
      [] -&gt; []
    end
  end

  @doc &quot;&quot;&quot;
  Deletes a killmail from the store.
  &quot;&quot;&quot;
  @spec delete(kill_id()) :: :ok
  def delete(kill_id) when is_integer(kill_id) do
    :ets.delete(@killmail_table, kill_id)

    # Remove from system associations
    # We need to search through all systems to find and remove this killmail_id
    :ets.foldl(
      fn {system_id, killmail_ids}, _acc -&gt;
        if kill_id in killmail_ids do
          updated_ids = List.delete(killmail_ids, kill_id)
          :ets.insert(@system_killmails_table, {system_id, updated_ids})
        end

        :ok
      end,
      :ok,
      @system_killmails_table
    )

    :ok
  end

  # ============================================================================
  # System Fetch Timestamp Management
  # ============================================================================

  @doc &quot;&quot;&quot;
  Sets the fetch timestamp for a system.
  &quot;&quot;&quot;
  @spec fetch_timestamp(system_id(), DateTime.t()) :: :ok
  def fetch_timestamp(system_id, timestamp) when is_integer(system_id) do
    :ets.insert(@system_fetch_timestamps_table, {system_id, timestamp})
    :ok
  end

  @doc &quot;&quot;&quot;
  Gets the fetch timestamp for a system.
  &quot;&quot;&quot;
  @spec fetch_timestamp(system_id()) :: {:ok, DateTime.t()} | :error
  def fetch_timestamp(system_id) when is_integer(system_id) do
    case :ets.lookup(@system_fetch_timestamps_table, system_id) do
      [{^system_id, timestamp}] -&gt; {:ok, timestamp}
      [] -&gt; :error
    end
  end

  # ============================================================================
  # Testing Support
  # ============================================================================

  @doc &quot;&quot;&quot;
  Clears all data from all tables (for testing).
  &quot;&quot;&quot;
  @spec cleanup_tables() :: :ok
  def cleanup_tables do
    :ets.delete_all_objects(@killmail_table)
    :ets.delete_all_objects(@system_killmails_table)
    :ets.delete_all_objects(@system_fetch_timestamps_table)
    :ok
  end

  @doc &quot;&quot;&quot;
  Clears all data from all tables (alias for cleanup_tables).
  &quot;&quot;&quot;
  @spec clear() :: :ok
  def clear, do: cleanup_tables()
end</file><file path="lib/wanderer_kills/killmails/zkb_client.ex">defmodule WandererKills.Killmails.ZkbClientBehaviour do
  @moduledoc &quot;&quot;&quot;
  Behaviour for ZKB (zKillboard) client implementations.
  &quot;&quot;&quot;

  @doc &quot;&quot;&quot;
  Fetches a killmail from zKillboard.
  &quot;&quot;&quot;
  @callback fetch_killmail(integer()) :: {:ok, map()} | {:error, term()}

  @doc &quot;&quot;&quot;
  Fetches killmails for a system from zKillboard.
  &quot;&quot;&quot;
  @callback fetch_system_killmails(integer()) :: {:ok, [map()]} | {:error, term()}

  @doc &quot;&quot;&quot;
  Gets the kill count for a system.
  &quot;&quot;&quot;
  @callback get_system_kill_count(integer()) :: {:ok, integer()} | {:error, term()}
end

defmodule WandererKills.Killmails.ZkbClient do
  @moduledoc &quot;&quot;&quot;
  Unified ZKB API client for zKillboard with telemetry and processing.

  This module consolidates ZKB API interactions with telemetry, logging,
  and processing functionality. It replaces the previous split architecture
  with a single unified approach.
  &quot;&quot;&quot;

  @behaviour WandererKills.Killmails.ZkbClientBehaviour

  require Logger
  alias WandererKills.Infrastructure.{Config, Error}
  alias WandererKills.Http.{Client, ClientProvider}
  alias WandererKills.Observability.Telemetry

  @base_url Application.compile_env(:wanderer_kills, :zkb_base_url)

  @type killmail_id :: pos_integer()
  @type system_id :: pos_integer()
  @type killmail :: map()

  @doc &quot;&quot;&quot;
  Fetches a killmail from zKillboard with telemetry.
  Returns {:ok, killmail} or {:error, reason}.
  &quot;&quot;&quot;
  @spec fetch_killmail(killmail_id()) :: {:ok, killmail()} | {:error, term()}
  def fetch_killmail(killmail_id) when is_integer(killmail_id) and killmail_id &gt; 0 do
    Logger.debug(&quot;Fetching killmail from ZKB&quot;,
      killmail_id: killmail_id,
      operation: :fetch_killmail,
      step: :start
    )

    Telemetry.fetch_system_start(killmail_id, 1, :zkb)

    url = &quot;#{base_url()}/killID/#{killmail_id}/&quot;

    request_opts =
      ClientProvider.build_request_opts(
        params: [no_items: true],
        headers: ClientProvider.eve_api_headers(),
        timeout: Config.timeouts().zkb_request_ms
      )

    request_opts = Keyword.put(request_opts, :operation, :fetch_killmail)

    case Client.request_with_telemetry(url, :zkb, request_opts) do
      {:ok, response} -&gt;
        case Client.parse_json_response(response) do
          # ZKB API returns array with single killmail
          {:ok, [killmail]} -&gt;
            Telemetry.fetch_system_complete(killmail_id, :success)
            {:ok, killmail}

          {:ok, []} -&gt;
            Telemetry.fetch_system_error(killmail_id, :not_found, :zkb)
            {:error, Error.zkb_error(:not_found, &quot;Killmail not found in zKillboard&quot;, false)}

          # Take first if multiple
          {:ok, killmails} when is_list(killmails) -&gt;
            Telemetry.fetch_system_complete(killmail_id, :success)
            {:ok, List.first(killmails)}

          {:error, reason} -&gt;
            Telemetry.fetch_system_error(killmail_id, reason, :zkb)
            {:error, reason}
        end

      {:error, reason} -&gt;
        Telemetry.fetch_system_error(killmail_id, reason, :zkb)
        {:error, reason}
    end
  end

  def fetch_killmail(invalid_id) do
    {:error,
     Error.validation_error(:invalid_format, &quot;Invalid killmail ID format: #{inspect(invalid_id)}&quot;)}
  end

  @doc &quot;&quot;&quot;
  Fetches killmails for a system from zKillboard with telemetry.
  Returns {:ok, [killmail]} or {:error, reason}.
  &quot;&quot;&quot;
  @spec fetch_system_killmails(system_id()) :: {:ok, [killmail()]} | {:error, term()}
  def fetch_system_killmails(system_id) when is_integer(system_id) and system_id &gt; 0 do
    Logger.debug(&quot;Fetching system killmails from ZKB&quot;,
      system_id: system_id,
      operation: :fetch_system_killmails,
      step: :start
    )

    Telemetry.fetch_system_start(system_id, 0, :zkb)

    url = &quot;#{base_url()}/systemID/#{system_id}/&quot;

    Logger.info(&quot;[ZKB] Fetching system killmails&quot;,
      system_id: system_id,
      data_source: &quot;zkillboard.com/api&quot;,
      request_type: &quot;historical_data&quot;
    )

    request_opts =
      ClientProvider.build_request_opts(
        params: [no_items: true],
        headers: ClientProvider.eve_api_headers(),
        timeout: 60_000
      )

    request_opts = Keyword.put(request_opts, :operation, :fetch_system_killmails)

    case Client.request_with_telemetry(url, :zkb, request_opts) do
      {:ok, response} -&gt;
        case Client.parse_json_response(response) do
          {:ok, killmails} when is_list(killmails) -&gt;
            Telemetry.fetch_system_success(system_id, length(killmails), :zkb)

            Logger.debug(&quot;Successfully fetched system killmails from ZKB&quot;,
              system_id: system_id,
              killmail_count: length(killmails),
              operation: :fetch_system_killmails,
              step: :success
            )

            # Validate and log the format of received killmails
            validate_zkb_format(killmails, system_id)

            # Convert ZKB reference format to partial killmail format for parser
            converted_killmails = convert_zkb_to_partial_format(killmails)

            Logger.info(
              &quot;[ZKB] Converted #{length(killmails)} reference killmails to partial format&quot;
            )

            {:ok, converted_killmails}

          {:error, reason} -&gt;
            Telemetry.fetch_system_error(system_id, reason, :zkb)

            Logger.error(&quot;Failed to fetch system killmails from ZKB&quot;,
              system_id: system_id,
              operation: :fetch_system_killmails,
              error: reason,
              step: :error
            )

            {:error, reason}

          other -&gt;
            # Handle unexpected successful responses
            error_reason =
              Error.zkb_error(:unexpected_response, &quot;Unexpected response format from ZKB&quot;, false)

            Telemetry.fetch_system_error(system_id, error_reason, :zkb)

            Logger.error(&quot;Failed to fetch system killmails from ZKB&quot;,
              system_id: system_id,
              operation: :fetch_system_killmails,
              error: error_reason,
              unexpected_response: other,
              step: :error
            )

            {:error, error_reason}
        end

      {:error, reason} -&gt;
        Telemetry.fetch_system_error(system_id, reason, :zkb)

        Logger.error(&quot;Failed to fetch system killmails from ZKB&quot;,
          system_id: system_id,
          operation: :fetch_system_killmails,
          error: reason,
          step: :error
        )

        {:error, reason}
    end
  end

  def fetch_system_killmails(invalid_id) do
    {:error,
     Error.validation_error(:invalid_format, &quot;Invalid system ID format: #{inspect(invalid_id)}&quot;)}
  end

  @doc &quot;&quot;&quot;
  Fetches killmails for a system from zKillboard with telemetry (compatibility function).
  The limit and since_hours parameters are currently ignored but kept for API compatibility.
  &quot;&quot;&quot;
  @spec fetch_system_killmails(system_id(), pos_integer(), pos_integer()) ::
          {:ok, [killmail()]} | {:error, term()}
  def fetch_system_killmails(system_id, _limit, _since_hours)
      when is_integer(system_id) and system_id &gt; 0 do
    # For now, delegate to the main function - in the future we could use limit/since_hours
    fetch_system_killmails(system_id)
  end

  def fetch_system_killmails(invalid_id, _limit, _since_hours) do
    {:error,
     Error.validation_error(:invalid_format, &quot;Invalid system ID format: #{inspect(invalid_id)}&quot;)}
  end

  @doc &quot;&quot;&quot;
  Gets killmails for a corporation from zKillboard.
  &quot;&quot;&quot;
  def get_corporation_killmails(corporation_id) do
    fetch_entity_killmails(&quot;corporationID&quot;, corporation_id)
  end

  @doc &quot;&quot;&quot;
  Gets killmails for an alliance from zKillboard.
  &quot;&quot;&quot;
  def get_alliance_killmails(alliance_id) do
    fetch_entity_killmails(&quot;allianceID&quot;, alliance_id)
  end

  @doc &quot;&quot;&quot;
  Gets killmails for a character from zKillboard.
  &quot;&quot;&quot;
  def get_character_killmails(character_id) do
    fetch_entity_killmails(&quot;characterID&quot;, character_id)
  end

  # Shared function for fetching killmails by entity type
  defp fetch_entity_killmails(entity_type, entity_id) do
    url = &quot;#{base_url()}/#{entity_type}/#{entity_id}/&quot;

    request_opts =
      ClientProvider.build_request_opts(
        params: [no_items: true],
        headers: ClientProvider.eve_api_headers(),
        timeout: Config.timeouts().zkb_request_ms
      )

    request_opts = Keyword.put(request_opts, :operation, :&quot;fetch_#{entity_type}_killmails&quot;)

    case Client.request_with_telemetry(url, :zkb, request_opts) do
      {:ok, response} -&gt; Client.parse_json_response(response)
      {:error, reason} -&gt; {:error, reason}
    end
  end

  @doc &quot;&quot;&quot;
  Fetches killmails for a system from ESI.
  Returns {:ok, [killmail]} or {:error, reason}.
  &quot;&quot;&quot;
  def fetch_system_killmails_esi(system_id) do
    url = &quot;#{base_url()}/systemID/#{system_id}/&quot;

    request_opts =
      ClientProvider.build_request_opts(
        headers: ClientProvider.eve_api_headers(),
        timeout: Config.timeouts().zkb_request_ms
      )

    request_opts = Keyword.put(request_opts, :operation, :fetch_system_killmails_esi)

    case Client.request_with_telemetry(url, :zkb, request_opts) do
      {:ok, response} -&gt; Client.parse_json_response(response)
      {:error, reason} -&gt; {:error, reason}
    end
  end

  @doc &quot;&quot;&quot;
  Enriches a killmail with additional information.
  Returns {:ok, enriched_killmail} or {:error, reason}.
  &quot;&quot;&quot;
  def enrich_killmail(killmail) do
    with {:ok, victim} &lt;- get_victim_info(killmail),
         {:ok, attackers} &lt;- get_attackers_info(killmail),
         {:ok, items} &lt;- get_items_info(killmail) do
      enriched =
        Map.merge(killmail, %{
          &quot;victim&quot; =&gt; victim,
          &quot;attackers&quot; =&gt; attackers,
          &quot;items&quot; =&gt; items
        })

      {:ok, enriched}
    end
  end

  @doc &quot;&quot;&quot;
  Gets the kill count for a system from zKillboard with telemetry.
  Returns {:ok, count} or {:error, reason}.
  &quot;&quot;&quot;
  @spec get_system_kill_count(system_id()) :: {:ok, integer()} | {:error, term()}
  def get_system_kill_count(system_id) when is_integer(system_id) and system_id &gt; 0 do
    Logger.debug(&quot;Fetching system kill count from ZKB&quot;,
      system_id: system_id,
      operation: :get_system_kill_count,
      step: :start
    )

    url = &quot;#{base_url()}/systemID/#{system_id}/&quot;

    request_opts =
      ClientProvider.build_request_opts(
        headers: ClientProvider.eve_api_headers(),
        timeout: Config.timeouts().zkb_request_ms
      )

    request_opts = Keyword.put(request_opts, :operation, :get_system_kill_count)

    case Client.request_with_telemetry(url, :zkb, request_opts) do
      {:ok, response} -&gt;
        case Client.parse_json_response(response) do
          {:ok, data} when is_list(data) -&gt;
            count = length(data)

            Logger.debug(&quot;Successfully fetched system kill count from ZKB&quot;,
              system_id: system_id,
              kill_count: count,
              operation: :get_system_kill_count,
              step: :success
            )

            {:ok, count}

          {:ok, _} -&gt;
            error_reason =
              Error.zkb_error(
                :unexpected_response,
                &quot;Expected list data for kill count but got different format&quot;,
                false
              )

            Logger.error(&quot;Failed to fetch system kill count from ZKB&quot;,
              system_id: system_id,
              operation: :get_system_kill_count,
              error: error_reason,
              step: :error
            )

            {:error, error_reason}

          {:error, reason} -&gt;
            Logger.error(&quot;Failed to fetch system kill count from ZKB&quot;,
              system_id: system_id,
              operation: :get_system_kill_count,
              error: reason,
              step: :error
            )

            {:error, reason}
        end

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to fetch system kill count from ZKB&quot;,
          system_id: system_id,
          operation: :get_system_kill_count,
          error: reason,
          step: :error
        )

        {:error, reason}
    end
  end

  def get_system_kill_count(invalid_id) do
    {:error,
     Error.validation_error(:invalid_format, &quot;Invalid system ID format: #{inspect(invalid_id)}&quot;)}
  end

  @doc &quot;&quot;&quot;
  Fetches active systems from zKillboard with caching.
  &quot;&quot;&quot;
  @spec fetch_active_systems(keyword()) :: {:ok, [system_id()]} | {:error, term()}
  def fetch_active_systems(opts \\ []) do
    force = Keyword.get(opts, :force, false)

    if force do
      do_fetch_active_systems()
    else
      case fetch_from_cache() do
        {:ok, systems} -&gt; {:ok, systems}
        {:error, _reason} -&gt; do_fetch_active_systems()
      end
    end
  end

  defp fetch_from_cache do
    alias WandererKills.Cache.Helper

    case Helper.system_get_active_systems() do
      {:ok, systems} when is_list(systems) -&gt;
        {:ok, systems}

      {:error, reason} -&gt;
        Logger.warning(&quot;Cache error for active systems, falling back to fresh fetch&quot;,
          operation: :fetch_active_systems,
          step: :cache_error,
          error: reason
        )

        do_fetch_active_systems()
    end
  end

  defp do_fetch_active_systems do
    url = &quot;#{base_url()}/systems/&quot;

    request_opts =
      ClientProvider.build_request_opts(
        headers: ClientProvider.eve_api_headers(),
        timeout: Config.timeouts().zkb_request_ms
      )

    request_opts = Keyword.put(request_opts, :operation, :fetch_active_systems)

    case Client.request_with_telemetry(url, :zkb, request_opts) do
      {:ok, response} -&gt;
        case Client.parse_json_response(response) do
          {:ok, systems} when is_list(systems) -&gt;
            {:ok, systems}

          {:ok, _} -&gt;
            {:error,
             Error.zkb_error(
               :unexpected_response,
               &quot;Expected list of systems but got different format&quot;,
               false
             )}

          {:error, reason} -&gt;
            {:error, reason}
        end

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  # Note: Query parameter building now handled by WandererKills.Http.Client

  # Helper functions for enriching killmails
  defp get_victim_info(killmail) do
    victim = Map.get(killmail, &quot;victim&quot;, %{})
    {:ok, victim}
  end

  defp get_attackers_info(killmail) do
    attackers = Map.get(killmail, &quot;attackers&quot;, [])
    {:ok, attackers}
  end

  defp get_items_info(killmail) do
    items = Map.get(killmail, &quot;items&quot;, [])
    {:ok, items}
  end

  @doc &quot;&quot;&quot;
  Gets the base URL for zKillboard API calls.
  &quot;&quot;&quot;
  def base_url do
    @base_url
  end

  # Note: Response parsing now handled by WandererKills.Http.Client

  # Converts ZKB reference format to partial killmail format expected by parser.
  # ZKB format: %{&quot;killmail_id&quot; =&gt; id, &quot;zkb&quot; =&gt; metadata}
  # Partial format: %{&quot;killID&quot; =&gt; id, &quot;zkb&quot; =&gt; metadata}
  defp convert_zkb_to_partial_format(zkb_killmails) when is_list(zkb_killmails) do
    Enum.map(zkb_killmails, &amp;convert_single_zkb_killmail/1)
  end

  defp convert_single_zkb_killmail(%{&quot;killmail_id&quot; =&gt; id, &quot;zkb&quot; =&gt; zkb_data}) do
    %{
      &quot;killID&quot; =&gt; id,
      &quot;zkb&quot; =&gt; zkb_data
    }
  end

  defp convert_single_zkb_killmail(killmail) do
    Logger.warning(&quot;[ZKB] Unexpected killmail format, passing through unchanged&quot;,
      killmail_keys: Map.keys(killmail),
      killmail_sample: inspect(killmail, limit: 3)
    )

    killmail
  end

  @doc &quot;&quot;&quot;
  Validates and logs the format of killmails received from zKillboard API.
  This helps us understand the data structure and compare it with RedisQ formats.
  &quot;&quot;&quot;
  def validate_zkb_format(killmails, system_id) when is_list(killmails) do
    if length(killmails) &gt; 0 do
      sample_killmail = List.first(killmails)
      format_analysis = analyze_killmail_structure(sample_killmail)

      Logger.info(&quot;[ZKB] Format Analysis&quot;,
        system_id: system_id,
        data_source: &quot;zkillboard.com/api&quot;,
        killmail_count: length(killmails),
        sample_structure: format_analysis,
        data_type: &quot;historical_killmails&quot;
      )

      # Log detailed structure for first few killmails
      killmails
      |&gt; Enum.take(3)
      |&gt; Enum.with_index()
      |&gt; Enum.each(fn {killmail, index} -&gt;
        structure = analyze_killmail_structure(killmail)

        # Log the raw killmail structure for debugging
        Logger.info(&quot;[ZKB] Killmail RAW data&quot;,
          sample_index: index,
          killmail_id: Map.get(killmail, &quot;killmail_id&quot;) || Map.get(killmail, &quot;killID&quot;),
          raw_keys: Map.keys(killmail),
          raw_structure: killmail |&gt; inspect(limit: :infinity),
          byte_size: byte_size(inspect(killmail))
        )

        Logger.debug(&quot;[ZKB] Killmail structure detail&quot;,
          sample_index: index,
          killmail_id: Map.get(killmail, &quot;killmail_id&quot;) || Map.get(killmail, &quot;killID&quot;),
          structure: structure,
          has_full_data: has_full_killmail_data?(killmail),
          needs_esi_fetch: needs_esi_fetch?(killmail)
        )
      end)

      # Track format statistics
      track_zkb_format_usage(format_analysis)
    else
      Logger.info(&quot;[ZKB] No killmails received&quot;,
        system_id: system_id,
        data_source: &quot;zkillboard.com/api&quot;
      )
    end
  end

  # Analyze the structure of a killmail to understand its format
  defp analyze_killmail_structure(killmail) when is_map(killmail) do
    %{
      has_killmail_id: Map.has_key?(killmail, &quot;killmail_id&quot;),
      has_killID: Map.has_key?(killmail, &quot;killID&quot;),
      has_victim: Map.has_key?(killmail, &quot;victim&quot;),
      has_attackers: Map.has_key?(killmail, &quot;attackers&quot;),
      has_solar_system_id: Map.has_key?(killmail, &quot;solar_system_id&quot;),
      has_zkb: Map.has_key?(killmail, &quot;zkb&quot;),
      has_hash: Map.has_key?(killmail, &quot;hash&quot;),
      main_keys: Map.keys(killmail) |&gt; Enum.sort(),
      estimated_format: estimate_format_type(killmail)
    }
  end

  # Determine if killmail has full ESI-style data
  defp has_full_killmail_data?(killmail) do
    required_fields = [&quot;victim&quot;, &quot;attackers&quot;, &quot;solar_system_id&quot;]
    Enum.all?(required_fields, &amp;Map.has_key?(killmail, &amp;1))
  end

  # ZKB API confirmed to always return reference format requiring ESI fetch
  defp needs_esi_fetch?(killmail) do
    # ZKB API always returns reference format (killmail_id + zkb metadata only)
    Map.has_key?(killmail, &quot;killmail_id&quot;) &amp;&amp; Map.has_key?(killmail, &quot;zkb&quot;)
  end

  # Estimate the format type - ZKB API is consistently reference format
  defp estimate_format_type(killmail) do
    cond do
      # Should not occur with ZKB API
      has_full_killmail_data?(killmail) -&gt;
        :full_esi_format

      Map.has_key?(killmail, &quot;killmail_id&quot;) &amp;&amp; Map.has_key?(killmail, &quot;zkb&quot;) -&gt;
        :zkb_reference_format

      true -&gt;
        :unknown_format
    end
  end

  # Track ZKB format usage for comparison with RedisQ
  defp track_zkb_format_usage(format_analysis) do
    format_type = format_analysis.estimated_format

    # Emit telemetry event
    :telemetry.execute(
      [:wanderer_kills, :zkb, :format],
      %{count: 1},
      %{
        format: format_type,
        data_source: &quot;zkillboard_api&quot;,
        timestamp: DateTime.utc_now(),
        module: __MODULE__,
        analysis: format_analysis
      }
    )

    # Update persistent counters for periodic summaries
    current_stats = :persistent_term.get({__MODULE__, :zkb_format_stats}, %{})
    updated_stats = Map.update(current_stats, format_type, 1, &amp;(&amp;1 + 1))
    :persistent_term.put({__MODULE__, :zkb_format_stats}, updated_stats)

    new_count = :persistent_term.get({__MODULE__, :zkb_format_counter}, 0) + 1
    :persistent_term.put({__MODULE__, :zkb_format_counter}, new_count)

    # Log summary every 50 killmails
    if rem(new_count, 50) == 0 do
      log_zkb_format_summary(updated_stats, new_count)
    end
  end

  # Log comprehensive ZKB format summary
  defp log_zkb_format_summary(stats, total_count) do
    Logger.info(&quot;[ZKB] Format Summary&quot;,
      data_source: &quot;zkillboard.com/api (historical)&quot;,
      total_killmails_analyzed: total_count,
      format_distribution: stats,
      purpose: &quot;Format validation for preloader vs RedisQ comparison&quot;
    )

    Enum.each(stats, fn {format, count} -&gt;
      percentage = Float.round(count / total_count * 100, 1)
      recommendation = get_zkb_format_recommendation(format, percentage)

      Logger.info(&quot;[ZKB] Format details&quot;,
        format: format,
        count: count,
        percentage: &quot;#{percentage}%&quot;,
        description: describe_zkb_format(format),
        recommendation: recommendation
      )
    end)
  end

  # Describe ZKB format types
  defp describe_zkb_format(:full_esi_format),
    do: &quot;Complete killmail with victim/attackers (unexpected for ZKB API)&quot;

  defp describe_zkb_format(:zkb_reference_format),
    do: &quot;zKillboard reference format (killmail_id + zkb metadata) - confirmed production format&quot;

  defp describe_zkb_format(:unknown_format), do: &quot;Unknown/unexpected format&quot;

  # Provide recommendations for ZKB formats
  defp get_zkb_format_recommendation(:full_esi_format, _),
    do: &quot;UNEXPECTED: ZKB API should only return reference format&quot;

  defp get_zkb_format_recommendation(:zkb_reference_format, _),
    do: &quot;EXPECTED: Standard ZKB reference format - uses partial parser + ESI fetch&quot;

  defp get_zkb_format_recommendation(:unknown_format, _), do: &quot;ERROR: Review data structure&quot;
end</file><file path="lib/wanderer_kills/observability/health_checks.ex">defmodule WandererKills.Observability.HealthChecks do
  @moduledoc &quot;&quot;&quot;
  Consolidated health checks context for WandererKills observability.

  This module provides a unified interface for all health check functionality,
  including behaviour definitions and implementations for application and cache health.

  ## Unified Health Check Interface

  The module provides a simplified interface for common health check operations:

  ```elixir
  # Get comprehensive application health
  health_status = HealthChecks.check_health()

  # Get application metrics
  metrics = HealthChecks.get_metrics()

  # Get health for specific components
  cache_health = HealthChecks.check_health(components: [:cache])
  ```

  ## Health Check Behaviour

  All health checks should return a consistent status structure:

  ```elixir
  %{
    healthy: boolean(),
    status: String.t(),
    details: map(),
    timestamp: String.t()
  }
  ```

  ## Usage

  ```elixir
  # Check overall application health
  {:ok, health} = HealthChecks.check_application_health()

  # Check cache system health
  {:ok, cache_health} = HealthChecks.check_cache_health()

  # Get application metrics
  {:ok, metrics} = HealthChecks.get_application_metrics()
  ```
  &quot;&quot;&quot;

  require Logger
  alias WandererKills.Infrastructure.Clock

  # ============================================================================
  # Health Check Behaviour Definition
  # ============================================================================

  @type health_status :: %{
          healthy: boolean(),
          status: String.t(),
          details: map(),
          timestamp: String.t()
        }

  @type metrics :: %{
          component: String.t(),
          timestamp: String.t(),
          metrics: map()
        }

  @type health_opts :: keyword()
  @type health_component :: :application | :cache

  @doc &quot;&quot;&quot;
  Performs a health check for the component.

  ## Parameters
  - `opts` - Optional configuration for the health check

  ## Returns
  A health status map containing:
  - `:healthy` - Boolean indicating if the component is healthy
  - `:status` - String status (&quot;ok&quot;, &quot;error&quot;, &quot;degraded&quot;, etc.)
  - `:details` - Map with additional details about the health check
  - `:timestamp` - ISO8601 timestamp of when the check was performed
  &quot;&quot;&quot;
  @callback check_health(health_opts()) :: health_status()

  @doc &quot;&quot;&quot;
  Retrieves metrics for the component.

  ## Parameters
  - `opts` - Optional configuration for metrics collection

  ## Returns
  A metrics map containing:
  - `:component` - String identifying the component
  - `:timestamp` - ISO8601 timestamp of when metrics were collected
  - `:metrics` - Map containing component-specific metrics
  &quot;&quot;&quot;
  @callback get_metrics(health_opts()) :: metrics()

  @doc &quot;&quot;&quot;
  Optional callback for component-specific configuration.

  Components can implement this to provide default configuration
  that can be overridden by passed options.

  ## Returns
  Default configuration as a keyword list
  &quot;&quot;&quot;
  @callback default_config() :: keyword()

  @optional_callbacks [default_config: 0]

  # ============================================================================
  # Unified Health Check Interface
  # ============================================================================

  @doc &quot;&quot;&quot;
  Performs a comprehensive health check of the application.

  ## Options
  - `:components` - List of specific components to check (default: [:application])
  - `:timeout` - Timeout for health checks in milliseconds (default: 10_000)

  ## Returns
  A health status map with comprehensive application health information.

  ## Examples

  ```elixir
  # Full application health
  health = HealthChecks.check_health()

  # Only cache health
  cache_health = HealthChecks.check_health(components: [:cache])

  # Multiple components
  health = HealthChecks.check_health(components: [:application, :cache])
  ```
  &quot;&quot;&quot;
  @spec check_health(health_opts()) :: map()
  def check_health(opts \\ []) do
    components = Keyword.get(opts, :components, [:application])
    timeout = Keyword.get(opts, :timeout, 10_000)

    case components do
      [:application] -&gt;
        case check_application_health(timeout: timeout) do
          {:ok, health} -&gt;
            health

          {:error, _reason} -&gt;
            %{
              healthy: false,
              status: &quot;error&quot;,
              details: %{component: &quot;application&quot;},
              timestamp: Clock.now_iso8601()
            }
        end

      [:cache] -&gt;
        case check_cache_health(timeout: timeout) do
          {:ok, health} -&gt;
            health

          {:error, _reason} -&gt;
            %{
              healthy: false,
              status: &quot;error&quot;,
              details: %{component: &quot;cache&quot;},
              timestamp: Clock.now_iso8601()
            }
        end

      multiple_components when is_list(multiple_components) -&gt;
        aggregate_component_health(multiple_components, timeout)

      single_component -&gt;
        check_single_component(single_component, timeout)
    end
  end

  @doc &quot;&quot;&quot;
  Gets application metrics including component-specific metrics.

  ## Options
  - `:components` - List of specific components to get metrics for (default: [:application])
  - `:timeout` - Timeout for metrics collection in milliseconds (default: 10_000)

  ## Returns
  A metrics map with detailed performance and operational metrics.

  ## Examples

  ```elixir
  # Full application metrics
  metrics = HealthChecks.get_metrics()

  # Only cache metrics
  cache_metrics = HealthChecks.get_metrics(components: [:cache])
  ```
  &quot;&quot;&quot;
  @spec get_metrics(health_opts()) :: map()
  def get_metrics(opts \\ []) do
    components = Keyword.get(opts, :components, [:application])
    timeout = Keyword.get(opts, :timeout, 10_000)

    case components do
      [:application] -&gt;
        case get_application_metrics(timeout: timeout) do
          {:ok, metrics} -&gt;
            metrics

          {:error, _reason} -&gt;
            %{
              component: &quot;application&quot;,
              timestamp: Clock.now_iso8601(),
              metrics: %{error: &quot;Failed to collect metrics&quot;}
            }
        end

      [:cache] -&gt;
        case get_cache_metrics(timeout: timeout) do
          {:ok, metrics} -&gt;
            metrics

          {:error, _reason} -&gt;
            %{
              component: &quot;cache&quot;,
              timestamp: Clock.now_iso8601(),
              metrics: %{error: &quot;Failed to collect metrics&quot;}
            }
        end

      multiple_components when is_list(multiple_components) -&gt;
        aggregate_component_metrics(multiple_components, timeout)

      single_component -&gt;
        get_single_component_metrics(single_component, timeout)
    end
  end

  @doc &quot;&quot;&quot;
  Backwards compatibility: Get basic application version.

  Use `check_health/1` for full health information.
  &quot;&quot;&quot;
  @spec version() :: String.t()
  def version do
    case Application.spec(:wanderer_kills, :vsn) do
      nil -&gt; &quot;unknown&quot;
      version -&gt; to_string(version)
    end
  end

  # ============================================================================
  # Public API
  # ============================================================================

  @doc &quot;&quot;&quot;
  Checks overall application health by aggregating all component health checks.

  ## Options
  - `:health_modules` - List of health check modules to run (default: all registered)
  - `:timeout_ms` - Timeout for health checks (default: 10_000)
  - `:include_system_metrics` - Include system metrics (default: true)

  ## Returns
  - `{:ok, health_status}` - Application health status
  - `{:error, reason}` - If health check fails
  &quot;&quot;&quot;
  @spec check_application_health(health_opts()) :: {:ok, health_status()} | {:error, term()}
  def check_application_health(opts \\ []) do
    health_status = __MODULE__.ApplicationHealth.check_health(opts)
    {:ok, health_status}
  rescue
    error -&gt;
      Logger.error(&quot;Application health check failed: #{inspect(error)}&quot;)
      {:error, error}
  end

  @doc &quot;&quot;&quot;
  Checks cache system health for all registered caches.

  ## Options
  - `:cache_names` - List of cache names to check (default: all configured caches)
  - `:include_stats` - Include cache statistics (default: true)
  - `:timeout_ms` - Timeout for cache checks (default: 5_000)

  ## Returns
  - `{:ok, health_status}` - Cache system health status
  - `{:error, reason}` - If health check fails
  &quot;&quot;&quot;
  @spec check_cache_health(health_opts()) :: {:ok, health_status()} | {:error, term()}
  def check_cache_health(opts \\ []) do
    health_status = __MODULE__.CacheHealth.check_health(opts)
    {:ok, health_status}
  rescue
    error -&gt;
      Logger.error(&quot;Cache health check failed: #{inspect(error)}&quot;)
      {:error, error}
  end

  @doc &quot;&quot;&quot;
  Gets comprehensive application metrics including all components.

  ## Options
  - `:health_modules` - List of health check modules to collect metrics from
  - `:include_system_metrics` - Include system metrics (default: true)

  ## Returns
  - `{:ok, metrics}` - Application metrics
  - `{:error, reason}` - If metrics collection fails
  &quot;&quot;&quot;
  @spec get_application_metrics(health_opts()) :: {:ok, metrics()} | {:error, term()}
  def get_application_metrics(opts \\ []) do
    metrics = __MODULE__.ApplicationHealth.get_metrics(opts)
    {:ok, metrics}
  rescue
    error -&gt;
      Logger.error(&quot;Application metrics collection failed: #{inspect(error)}&quot;)
      {:error, error}
  end

  @doc &quot;&quot;&quot;
  Gets cache system metrics for all registered caches.

  ## Options
  - `:cache_names` - List of cache names to collect metrics from
  - `:include_stats` - Include detailed cache statistics (default: true)

  ## Returns
  - `{:ok, metrics}` - Cache system metrics
  - `{:error, reason}` - If metrics collection fails
  &quot;&quot;&quot;
  @spec get_cache_metrics(health_opts()) :: {:ok, metrics()} | {:error, term()}
  def get_cache_metrics(opts \\ []) do
    metrics = __MODULE__.CacheHealth.get_metrics(opts)
    {:ok, metrics}
  rescue
    error -&gt;
      Logger.error(&quot;Cache metrics collection failed: #{inspect(error)}&quot;)
      {:error, error}
  end

  # ============================================================================
  # Application Health Implementation
  # ============================================================================

  defmodule ApplicationHealth do
    @moduledoc &quot;&quot;&quot;
    Application-level health check that aggregates all component health checks.

    This module provides a unified view of application health by collecting
    and aggregating health status from all registered health check modules.
    &quot;&quot;&quot;

    @behaviour WandererKills.Observability.HealthChecks

    @impl true
    def check_health(opts \\ []) do
      config = Keyword.merge(default_config(), opts)
      health_modules = Keyword.get(config, :health_modules)

      component_checks = Enum.map(health_modules, &amp;run_component_health_check/1)
      all_healthy = Enum.all?(component_checks, &amp; &amp;1.healthy)

      %{
        healthy: all_healthy,
        status: determine_overall_status(component_checks),
        details: %{
          component: &quot;application&quot;,
          version: get_application_version(),
          uptime_seconds: get_uptime_seconds(),
          components: component_checks,
          total_components: length(health_modules),
          healthy_components: Enum.count(component_checks, &amp; &amp;1.healthy)
        },
        timestamp: Clock.now_iso8601()
      }
    end

    @impl true
    def get_metrics(opts \\ []) do
      config = Keyword.merge(default_config(), opts)
      health_modules = Keyword.get(config, :health_modules)

      component_metrics = Enum.map(health_modules, &amp;run_component_metrics/1)

      %{
        component: &quot;application&quot;,
        timestamp: Clock.now_iso8601(),
        metrics: %{
          version: get_application_version(),
          uptime_seconds: get_uptime_seconds(),
          total_components: length(health_modules),
          components: component_metrics,
          system: get_system_metrics()
        }
      }
    end

    @impl true
    def default_config do
      [
        health_modules: [
          WandererKills.Observability.HealthChecks.CacheHealth
        ],
        timeout_ms: 10_000,
        include_system_metrics: true
      ]
    end

    # Private helper functions

    @spec run_component_health_check(module()) :: map()
    defp run_component_health_check(health_module) do
      health_module.check_health()
    rescue
      error -&gt;
        Logger.error(&quot;Health check failed for #{inspect(health_module)}: #{inspect(error)}&quot;)

        %{
          healthy: false,
          status: &quot;error&quot;,
          details: %{
            component: inspect(health_module),
            error: &quot;Health check failed&quot;,
            reason: inspect(error)
          },
          timestamp: Clock.now_iso8601()
        }
    end

    @spec run_component_metrics(module()) :: map()
    defp run_component_metrics(health_module) do
      health_module.get_metrics()
    rescue
      error -&gt;
        Logger.error(&quot;Metrics collection failed for #{inspect(health_module)}: #{inspect(error)}&quot;)

        %{
          component: inspect(health_module),
          timestamp: Clock.now_iso8601(),
          metrics: %{
            error: &quot;Metrics collection failed&quot;,
            reason: inspect(error)
          }
        }
    end

    @spec determine_overall_status([map()]) :: String.t()
    defp determine_overall_status(component_checks) do
      healthy_count = Enum.count(component_checks, &amp; &amp;1.healthy)
      total_count = length(component_checks)

      cond do
        healthy_count == total_count -&gt; &quot;ok&quot;
        healthy_count == 0 -&gt; &quot;critical&quot;
        healthy_count &lt; total_count / 2 -&gt; &quot;degraded&quot;
        true -&gt; &quot;warning&quot;
      end
    end

    @spec get_application_version() :: String.t()
    defp get_application_version do
      case Application.spec(:wanderer_kills, :vsn) do
        nil -&gt; &quot;unknown&quot;
        version -&gt; to_string(version)
      end
    end

    @spec get_uptime_seconds() :: non_neg_integer()
    defp get_uptime_seconds do
      :erlang.statistics(:wall_clock)
      |&gt; elem(0)
      |&gt; div(1000)
    end

    @spec get_system_metrics() :: map()
    defp get_system_metrics do
      %{
        memory_usage: :erlang.memory(),
        process_count: :erlang.system_info(:process_count),
        port_count: :erlang.system_info(:port_count),
        ets_tables: length(:ets.all()),
        schedulers: :erlang.system_info(:schedulers),
        run_queue: :erlang.statistics(:run_queue)
      }
    rescue
      error -&gt;
        Logger.warning(&quot;Failed to collect system metrics: #{inspect(error)}&quot;)
        %{error: &quot;System metrics collection failed&quot;}
    end
  end

  # ============================================================================
  # Cache Health Implementation
  # ============================================================================

  defmodule CacheHealth do
    @moduledoc &quot;&quot;&quot;
    Health check implementation for cache systems.

    This module provides comprehensive health checking for all cache
    instances in the application, including size, connectivity, and
    performance metrics.
    &quot;&quot;&quot;

    @behaviour WandererKills.Observability.HealthChecks

    @impl true
    def check_health(opts \\ []) do
      config = Keyword.merge(default_config(), opts)
      cache_names = Keyword.get(config, :cache_names)

      cache_checks = Enum.map(cache_names, &amp;check_cache_health/1)
      all_healthy = Enum.all?(cache_checks, &amp; &amp;1.healthy)

      %{
        healthy: all_healthy,
        status: if(all_healthy, do: &quot;ok&quot;, else: &quot;error&quot;),
        details: %{
          component: &quot;cache_system&quot;,
          caches: cache_checks,
          total_caches: length(cache_names),
          healthy_caches: Enum.count(cache_checks, &amp; &amp;1.healthy)
        },
        timestamp: Clock.now_iso8601()
      }
    end

    @impl true
    def get_metrics(opts \\ []) do
      config = Keyword.merge(default_config(), opts)
      cache_names = Keyword.get(config, :cache_names)

      cache_metrics = Enum.map(cache_names, &amp;get_cache_metrics/1)

      %{
        component: &quot;cache_system&quot;,
        timestamp: Clock.now_iso8601(),
        metrics: %{
          total_caches: length(cache_names),
          caches: cache_metrics,
          aggregate: calculate_aggregate_metrics(cache_metrics)
        }
      }
    end

    @impl true
    def default_config do
      cache_names = [
        # Single unified cache instance
        :wanderer_cache
      ]

      [
        cache_names: cache_names,
        include_stats: true,
        timeout_ms: 5_000
      ]
    end

    # Private helper functions

    @spec check_cache_health(atom()) :: %{healthy: boolean(), name: atom(), status: String.t()}
    defp check_cache_health(cache_name) do
      case Cachex.size(cache_name) do
        {:ok, size} -&gt;
          %{
            healthy: true,
            name: cache_name,
            status: &quot;ok&quot;,
            size: size
          }

        {:error, reason} -&gt;
          %{
            healthy: false,
            name: cache_name,
            status: &quot;error&quot;,
            error: inspect(reason)
          }
      end
    rescue
      error -&gt;
        Logger.warning(&quot;Cache health check failed for #{cache_name}: #{inspect(error)}&quot;)

        %{
          healthy: false,
          name: cache_name,
          status: &quot;unavailable&quot;,
          error: inspect(error)
        }
    end

    @spec get_cache_metrics(atom()) :: map()
    defp get_cache_metrics(cache_name) do
      base_metrics = %{name: cache_name}

      try do
        case Cachex.stats(cache_name) do
          {:ok, stats} -&gt;
            Map.merge(base_metrics, %{
              size: Map.get(stats, :size, 0),
              hit_rate: Map.get(stats, :hit_rate, 0.0),
              miss_rate: Map.get(stats, :miss_rate, 0.0),
              eviction_count: Map.get(stats, :eviction_count, 0),
              expiration_count: Map.get(stats, :expiration_count, 0),
              update_count: Map.get(stats, :update_count, 0)
            })

          {:error, reason} -&gt;
            Map.merge(base_metrics, %{
              error: &quot;Unable to retrieve stats&quot;,
              reason: inspect(reason)
            })
        end
      rescue
        error -&gt;
          Map.merge(base_metrics, %{
            error: &quot;Stats collection failed&quot;,
            reason: inspect(error)
          })
      end
    end

    @spec calculate_aggregate_metrics([map()]) :: map()
    defp calculate_aggregate_metrics(cache_metrics) do
      valid_metrics = Enum.reject(cache_metrics, &amp;Map.has_key?(&amp;1, :error))

      if Enum.empty?(valid_metrics) do
        %{error: &quot;No valid cache metrics available&quot;}
      else
        %{
          total_size: Enum.sum(Enum.map(valid_metrics, &amp;Map.get(&amp;1, :size, 0))),
          average_hit_rate: calculate_average_hit_rate(valid_metrics),
          total_evictions: Enum.sum(Enum.map(valid_metrics, &amp;Map.get(&amp;1, :eviction_count, 0))),
          total_expirations: Enum.sum(Enum.map(valid_metrics, &amp;Map.get(&amp;1, :expiration_count, 0)))
        }
      end
    end

    @spec calculate_average_hit_rate([map()]) :: float()
    defp calculate_average_hit_rate(valid_metrics) do
      hit_rates = Enum.map(valid_metrics, &amp;Map.get(&amp;1, :hit_rate, 0.0))

      case hit_rates do
        [] -&gt; 0.0
        rates -&gt; Enum.sum(rates) / length(rates)
      end
    end
  end

  # ============================================================================
  # Private Helper Functions for Unified Interface
  # ============================================================================

  @spec aggregate_component_health([health_component()], pos_integer()) :: map()
  defp aggregate_component_health(components, timeout) do
    component_results =
      Enum.map(components, fn component -&gt;
        {component, check_single_component(component, timeout)}
      end)

    all_healthy = Enum.all?(component_results, fn {_comp, result} -&gt; result.healthy end)

    %{
      healthy: all_healthy,
      status: determine_aggregate_status(component_results),
      details: %{
        component: &quot;aggregate&quot;,
        components: Map.new(component_results),
        total_components: length(components),
        healthy_components:
          Enum.count(component_results, fn {_comp, result} -&gt; result.healthy end)
      },
      timestamp: Clock.now_iso8601()
    }
  end

  @spec aggregate_component_metrics([health_component()], pos_integer()) :: map()
  defp aggregate_component_metrics(components, timeout) do
    component_metrics =
      Enum.map(components, fn component -&gt;
        {component, get_single_component_metrics(component, timeout)}
      end)

    %{
      component: &quot;aggregate&quot;,
      timestamp: Clock.now_iso8601(),
      metrics: %{
        components: Map.new(component_metrics),
        total_components: length(components)
      }
    }
  end

  @spec check_single_component(health_component(), pos_integer()) :: map()
  defp check_single_component(component, timeout) do
    case component do
      :application -&gt;
        case check_application_health(timeout: timeout) do
          {:ok, health} -&gt;
            health

          {:error, _reason} -&gt;
            %{
              healthy: false,
              status: &quot;error&quot;,
              details: %{component: &quot;application&quot;},
              timestamp: Clock.now_iso8601()
            }
        end

      :cache -&gt;
        case check_cache_health(timeout: timeout) do
          {:ok, health} -&gt;
            health

          {:error, _reason} -&gt;
            %{
              healthy: false,
              status: &quot;error&quot;,
              details: %{component: &quot;cache&quot;},
              timestamp: Clock.now_iso8601()
            }
        end

      unknown -&gt;
        Logger.warning(&quot;Unknown health component: #{inspect(unknown)}&quot;)

        %{
          healthy: false,
          status: &quot;error&quot;,
          details: %{
            component: inspect(unknown),
            error: &quot;Unknown component&quot;
          },
          timestamp: Clock.now_iso8601()
        }
    end
  end

  @spec get_single_component_metrics(health_component(), pos_integer()) :: map()
  defp get_single_component_metrics(component, timeout) do
    case component do
      :application -&gt;
        case get_application_metrics(timeout: timeout) do
          {:ok, metrics} -&gt;
            metrics

          {:error, _reason} -&gt;
            %{
              component: &quot;application&quot;,
              timestamp: Clock.now_iso8601(),
              metrics: %{error: &quot;Failed to collect metrics&quot;}
            }
        end

      :cache -&gt;
        case get_cache_metrics(timeout: timeout) do
          {:ok, metrics} -&gt;
            metrics

          {:error, _reason} -&gt;
            %{
              component: &quot;cache&quot;,
              timestamp: Clock.now_iso8601(),
              metrics: %{error: &quot;Failed to collect metrics&quot;}
            }
        end

      unknown -&gt;
        Logger.warning(&quot;Unknown metrics component: #{inspect(unknown)}&quot;)

        %{
          component: inspect(unknown),
          timestamp: Clock.now_iso8601(),
          metrics: %{error: &quot;Unknown component&quot;}
        }
    end
  end

  @spec determine_aggregate_status([{health_component(), map()}]) :: String.t()
  defp determine_aggregate_status(component_results) do
    healthy_count = Enum.count(component_results, fn {_comp, result} -&gt; result.healthy end)
    total_count = length(component_results)

    cond do
      healthy_count == total_count -&gt; &quot;ok&quot;
      healthy_count == 0 -&gt; &quot;critical&quot;
      healthy_count &lt; total_count / 2 -&gt; &quot;degraded&quot;
      true -&gt; &quot;warning&quot;
    end
  end
end</file><file path="lib/wanderer_kills/observability/monitoring.ex">defmodule WandererKills.Observability.Monitoring do
  @moduledoc &quot;&quot;&quot;
  Unified monitoring and observability for the WandererKills application.

  This module consolidates health monitoring, metrics collection, telemetry measurements,
  and instrumentation functionality into a single observability interface.

  ## Features

  - Cache health monitoring and metrics collection
  - Application health status and uptime tracking
  - Telemetry measurements and periodic data gathering
  - Unified error handling and logging
  - Periodic health checks with configurable intervals
  - System metrics collection (memory, CPU, processes)

  ## Usage

  ```elixir
  # Start the monitoring GenServer
  {:ok, pid} = Monitoring.start_link([])

  # Check overall health
  {:ok, health} = Monitoring.check_health()

  # Get metrics
  {:ok, metrics} = Monitoring.get_metrics()

  # Get stats for a specific cache
  {:ok, stats} = Monitoring.get_cache_stats(:killmails_cache)

  # Telemetry measurements (called by TelemetryPoller)
  Monitoring.measure_http_requests()
  Monitoring.measure_cache_operations()
  Monitoring.measure_fetch_operations()
  ```

  ## Cache Names

  The following cache names are monitored:
  - `:wanderer_cache` - Unified cache with namespaced keys (killmails, systems, ESI data)
  &quot;&quot;&quot;

  use GenServer
  require Logger
  alias WandererKills.Infrastructure.Clock

  @cache_names [:wanderer_cache]
  @health_check_interval :timer.minutes(5)
  @summary_interval :timer.minutes(5)

  # Client API

  def start_link(opts \\ []) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  @doc &quot;&quot;&quot;
  Performs a comprehensive health check of the application.

  Returns a map with health status for each cache and overall application status,
  including version, uptime, and timestamp information.

  ## Returns
  - `{:ok, health_map}` - Complete health status
  - `{:error, reason}` - If health check fails entirely

  ## Example

  ```elixir
  {:ok, health} = check_health()
  # %{
  #   healthy: true,
  #   timestamp: &quot;2024-01-01T12:00:00Z&quot;,
  #   version: &quot;1.0.0&quot;,
  #   uptime_seconds: 3600,
  #   caches: [
  #     %{name: :wanderer_cache, healthy: true, status: &quot;ok&quot;}
  #   ]
  # }
  ```
  &quot;&quot;&quot;
  @spec check_health() :: {:ok, map()} | {:error, term()}
  def check_health do
    GenServer.call(__MODULE__, :check_health)
  end

  @doc &quot;&quot;&quot;
  Gets comprehensive metrics for all monitored caches and application stats.

  Returns cache statistics and application metrics that can be used for
  monitoring, alerting, and performance analysis.

  ## Returns
  - `{:ok, metrics_map}` - Metrics for all caches and app stats
  - `{:error, reason}` - If metrics collection fails

  ## Example

  ```elixir
  {:ok, metrics} = get_metrics()
  # %{
  #   timestamp: &quot;2024-01-01T12:00:00Z&quot;,
  #   uptime_seconds: 3600,
  #   caches: [
  #     %{name: :wanderer_cache, size: 1000, hit_rate: 0.85, miss_rate: 0.15}
  #   ]
  # }
  ```
  &quot;&quot;&quot;
  @spec get_metrics() :: {:ok, map()} | {:error, term()}
  def get_metrics do
    GenServer.call(__MODULE__, :get_metrics)
  end

  @doc &quot;&quot;&quot;
  Get telemetry data for all monitored caches.

  This is an alias for `get_metrics/0` as telemetry and metrics
  are essentially the same data in this context.

  ## Returns
  - `{:ok, telemetry_map}` - Telemetry data for all caches
  - `{:error, reason}` - If telemetry collection fails
  &quot;&quot;&quot;
  @spec get_telemetry() :: {:ok, map()} | {:error, term()}
  def get_telemetry do
    get_metrics()
  end

  @doc &quot;&quot;&quot;
  Get statistics for a specific cache.

  ## Parameters
  - `cache_name` - The name of the cache to get stats for

  ## Returns
  - `{:ok, stats}` - Cache statistics map
  - `{:error, reason}` - If stats collection fails

  ## Example

  ```elixir
  {:ok, stats} = get_cache_stats(:wanderer_cache)
  # %{hit_rate: 0.85, size: 1000, evictions: 10, ...}
  ```
  &quot;&quot;&quot;
  @spec get_cache_stats(atom()) :: {:ok, map()} | {:error, term()}
  def get_cache_stats(cache_name) do
    GenServer.call(__MODULE__, {:get_cache_stats, cache_name})
  end

  # Parser statistics functions

  @doc &quot;&quot;&quot;
  Increments the count of successfully stored killmails.
  &quot;&quot;&quot;
  @spec increment_stored() :: :ok
  def increment_stored do
    :telemetry.execute([:wanderer_kills, :parser, :stored], %{count: 1}, %{})
    GenServer.cast(__MODULE__, {:increment, :stored})
  end

  @doc &quot;&quot;&quot;
  Increments the count of skipped killmails (too old).
  &quot;&quot;&quot;
  @spec increment_skipped() :: :ok
  def increment_skipped do
    :telemetry.execute([:wanderer_kills, :parser, :skipped], %{count: 1}, %{})
    GenServer.cast(__MODULE__, {:increment, :skipped})
  end

  @doc &quot;&quot;&quot;
  Increments the count of failed killmail parsing attempts.
  &quot;&quot;&quot;
  @spec increment_failed() :: :ok
  def increment_failed do
    :telemetry.execute([:wanderer_kills, :parser, :failed], %{count: 1}, %{})
    GenServer.cast(__MODULE__, {:increment, :failed})
  end

  @doc &quot;&quot;&quot;
  Gets the current parsing statistics.
  &quot;&quot;&quot;
  @spec get_parser_stats() :: {:ok, map()} | {:error, term()}
  def get_parser_stats do
    GenServer.call(__MODULE__, :get_parser_stats)
  end

  @doc &quot;&quot;&quot;
  Resets all parser statistics counters to zero.
  &quot;&quot;&quot;
  @spec reset_parser_stats() :: :ok
  def reset_parser_stats do
    GenServer.call(__MODULE__, :reset_parser_stats)
  end

  # Telemetry measurement functions (called by TelemetryPoller)

  @doc &quot;&quot;&quot;
  Measures HTTP request metrics for telemetry.

  This function is called by TelemetryPoller to emit HTTP request metrics.
  &quot;&quot;&quot;
  @spec measure_http_requests() :: :ok
  def measure_http_requests do
    :telemetry.execute(
      [:wanderer_kills, :system, :http_requests],
      %{count: :erlang.statistics(:reductions) |&gt; elem(0)},
      %{}
    )
  end

  @doc &quot;&quot;&quot;
  Measures cache operation metrics for telemetry.

  This function is called by TelemetryPoller to emit cache operation metrics.
  &quot;&quot;&quot;
  @spec measure_cache_operations() :: :ok
  def measure_cache_operations do
    cache_metrics =
      Enum.map(@cache_names, fn cache_name -&gt;
        case Cachex.size(cache_name) do
          {:ok, size} -&gt; size
          _ -&gt; 0
        end
      end)
      |&gt; Enum.sum()

    :telemetry.execute(
      [:wanderer_kills, :system, :cache_operations],
      %{total_cache_size: cache_metrics},
      %{}
    )
  end

  @doc &quot;&quot;&quot;
  Measures fetch operation metrics for telemetry.

  This function is called by TelemetryPoller to emit fetch operation metrics.
  &quot;&quot;&quot;
  @spec measure_fetch_operations() :: :ok
  def measure_fetch_operations do
    process_count = :erlang.system_info(:process_count)

    :telemetry.execute(
      [:wanderer_kills, :system, :fetch_operations],
      %{process_count: process_count},
      %{}
    )
  end

  @doc &quot;&quot;&quot;
  Measures system resource metrics for telemetry.

  This function emits comprehensive system metrics including memory and CPU usage.
  &quot;&quot;&quot;
  @spec measure_system_resources() :: :ok
  def measure_system_resources do
    memory_info = :erlang.memory()

    :telemetry.execute(
      [:wanderer_kills, :system, :memory],
      %{
        total_memory: memory_info[:total],
        process_memory: memory_info[:processes],
        atom_memory: memory_info[:atom],
        binary_memory: memory_info[:binary]
      },
      %{}
    )

    # Process and scheduler metrics
    :telemetry.execute(
      [:wanderer_kills, :system, :cpu],
      %{
        process_count: :erlang.system_info(:process_count),
        port_count: :erlang.system_info(:port_count),
        schedulers: :erlang.system_info(:schedulers),
        run_queue: :erlang.statistics(:run_queue)
      },
      %{}
    )
  end

  # Server Callbacks

  @impl true
  def init(opts) do
    Logger.info(&quot;[Monitoring] Starting unified monitoring with periodic health checks&quot;)

    # Start periodic health checks if not disabled in opts
    if !Keyword.get(opts, :disable_periodic_checks, false) do
      schedule_health_check()
    end

    # Schedule parser stats summary
    schedule_parser_summary()

    state = %{
      parser_stats: %{
        stored: 0,
        skipped: 0,
        failed: 0,
        total_processed: 0,
        last_reset: DateTime.utc_now()
      }
    }

    {:ok, state}
  end

  @impl true
  def handle_call(:check_health, _from, state) do
    health = build_comprehensive_health_status()
    {:reply, {:ok, health}, state}
  end

  @impl true
  def handle_call(:get_metrics, _from, state) do
    metrics = build_comprehensive_metrics()
    {:reply, {:ok, metrics}, state}
  end

  @impl true
  def handle_call({:get_cache_stats, cache_name}, _from, state) do
    stats = get_cache_stats_internal(cache_name)
    {:reply, stats, state}
  end

  @impl true
  def handle_call(:get_parser_stats, _from, state) do
    {:reply, {:ok, state.parser_stats}, state}
  end

  @impl true
  def handle_call(:reset_parser_stats, _from, state) do
    new_parser_stats = %{
      stored: 0,
      skipped: 0,
      failed: 0,
      total_processed: 0,
      last_reset: DateTime.utc_now()
    }

    new_state = %{state | parser_stats: new_parser_stats}
    {:reply, :ok, new_state}
  end

  @impl true
  def handle_cast({:increment, key}, state) when key in [:stored, :skipped, :failed] do
    current_stats = state.parser_stats

    new_stats =
      current_stats
      |&gt; Map.update!(key, &amp;(&amp;1 + 1))
      |&gt; Map.update!(:total_processed, &amp;(&amp;1 + 1))

    new_state = %{state | parser_stats: new_stats}
    {:noreply, new_state}
  end

  @impl true
  def handle_info(:check_health, state) do
    Logger.debug(&quot;[Monitoring] Running periodic health check&quot;)
    _health = build_comprehensive_health_status()
    schedule_health_check()
    {:noreply, state}
  end

  @impl true
  def handle_info(:log_parser_summary, state) do
    stats = state.parser_stats

    Logger.info(
      &quot;[Parser] Killmail processing summary - Stored: #{stats.stored}, Skipped: #{stats.skipped}, Failed: #{stats.failed}&quot;
    )

    # Emit telemetry for the summary
    :telemetry.execute(
      [:wanderer_kills, :parser, :summary],
      %{stored: stats.stored, skipped: stats.skipped, failed: stats.failed},
      %{}
    )

    # Reset counters after summary
    new_parser_stats = %{
      stored: 0,
      skipped: 0,
      failed: 0,
      total_processed: 0,
      last_reset: DateTime.utc_now()
    }

    schedule_parser_summary()
    new_state = %{state | parser_stats: new_parser_stats}
    {:noreply, new_state}
  end

  # Private helper functions

  defp schedule_health_check do
    Process.send_after(self(), :check_health, @health_check_interval)
  end

  defp schedule_parser_summary do
    Process.send_after(self(), :log_parser_summary, @summary_interval)
  end

  @spec build_comprehensive_health_status() :: map()
  defp build_comprehensive_health_status do
    cache_checks = Enum.map(@cache_names, &amp;build_cache_health_check/1)
    all_healthy = Enum.all?(cache_checks, &amp; &amp;1.healthy)

    %{
      healthy: all_healthy,
      timestamp: Clock.now_iso8601(),
      version: get_app_version(),
      uptime_seconds: get_uptime_seconds(),
      caches: cache_checks,
      system: get_system_info()
    }
  end

  @spec build_comprehensive_metrics() :: map()
  defp build_comprehensive_metrics do
    cache_metrics = Enum.map(@cache_names, &amp;build_cache_metrics/1)

    %{
      timestamp: Clock.now_iso8601(),
      uptime_seconds: get_uptime_seconds(),
      caches: cache_metrics,
      system: get_system_info(),
      aggregate: %{
        total_cache_size: Enum.sum(Enum.map(cache_metrics, &amp;Map.get(&amp;1, :size, 0))),
        average_hit_rate: calculate_average_hit_rate(cache_metrics)
      }
    }
  end

  @spec build_cache_health_check(atom()) :: map()
  defp build_cache_health_check(cache_name) do
    case Cachex.size(cache_name) do
      {:ok, _size} -&gt;
        %{name: cache_name, healthy: true, status: &quot;ok&quot;}

      {:error, reason} -&gt;
        Logger.error(
          &quot;[Monitoring] Cache health check failed for #{cache_name}: #{inspect(reason)}&quot;
        )

        %{name: cache_name, healthy: false, status: &quot;error&quot;, reason: inspect(reason)}
    end
  rescue
    error -&gt;
      Logger.error(
        &quot;[Monitoring] Cache health check exception for #{cache_name}: #{inspect(error)}&quot;
      )

      %{name: cache_name, healthy: false, status: &quot;unavailable&quot;}
  end

  @spec build_cache_metrics(atom()) :: map()
  defp build_cache_metrics(cache_name) do
    case Cachex.stats(cache_name) do
      {:ok, stats} -&gt;
        %{
          name: cache_name,
          size: Map.get(stats, :size, 0),
          hit_rate: Map.get(stats, :hit_rate, 0.0),
          miss_rate: Map.get(stats, :miss_rate, 0.0),
          evictions: Map.get(stats, :evictions, 0),
          operations: Map.get(stats, :operations, 0),
          memory: Map.get(stats, :memory, 0)
        }

      {:error, reason} -&gt;
        Logger.error(
          &quot;[Monitoring] Cache metrics collection failed for #{cache_name}: #{inspect(reason)}&quot;
        )

        %{name: cache_name, error: &quot;Unable to retrieve stats&quot;, reason: inspect(reason)}
    end
  end

  @spec get_cache_stats_internal(atom()) :: {:ok, map()} | {:error, term()}
  defp get_cache_stats_internal(cache_name) do
    case Cachex.stats(cache_name) do
      {:ok, stats} -&gt; {:ok, stats}
      {:error, reason} -&gt; {:error, reason}
    end
  end

  @spec get_system_info() :: map()
  defp get_system_info do
    memory_info = :erlang.memory()

    %{
      memory: %{
        total: memory_info[:total],
        processes: memory_info[:processes],
        atom: memory_info[:atom],
        binary: memory_info[:binary]
      },
      processes: %{
        count: :erlang.system_info(:process_count),
        limit: :erlang.system_info(:process_limit)
      },
      ports: %{
        count: :erlang.system_info(:port_count),
        limit: :erlang.system_info(:port_limit)
      },
      schedulers: :erlang.system_info(:schedulers),
      run_queue: :erlang.statistics(:run_queue),
      ets_tables: length(:ets.all())
    }
  rescue
    error -&gt;
      Logger.warning(&quot;Failed to collect system info: #{inspect(error)}&quot;)
      %{error: &quot;System info collection failed&quot;}
  end

  @spec calculate_average_hit_rate([map()]) :: float()
  defp calculate_average_hit_rate(cache_metrics) do
    valid_metrics = Enum.reject(cache_metrics, &amp;Map.has_key?(&amp;1, :error))

    case valid_metrics do
      [] -&gt;
        0.0

      metrics -&gt;
        hit_rates = Enum.map(metrics, &amp;Map.get(&amp;1, :hit_rate, 0.0))
        Enum.sum(hit_rates) / length(hit_rates)
    end
  end

  defp get_app_version do
    Application.spec(:wanderer_kills, :vsn)
    |&gt; to_string()
  rescue
    _ -&gt; &quot;unknown&quot;
  end

  defp get_uptime_seconds do
    :erlang.statistics(:wall_clock)
    |&gt; elem(0)
    |&gt; div(1000)
  end
end</file><file path="lib/wanderer_kills/observability/telemetry.ex">defmodule WandererKills.Observability.Telemetry do
  @moduledoc &quot;&quot;&quot;
  Handles telemetry events for the WandererKills application.

  This module provides functionality to:
  - Execute telemetry events with helper functions
  - Attach and detach telemetry event handlers
  - Process telemetry events through handlers
  - Centralized logging of telemetry events

  ## Events

  Cache events:
  - `[:wanderer_kills, :cache, :hit]` - When a cache lookup succeeds
  - `[:wanderer_kills, :cache, :miss]` - When a cache lookup fails
  - `[:wanderer_kills, :cache, :error]` - When a cache operation fails

  HTTP events:
  - `[:wanderer_kills, :http, :request, :start]` - When an HTTP request starts
  - `[:wanderer_kills, :http, :request, :stop]` - When an HTTP request completes

  Fetch events:
  - `[:wanderer_kills, :fetch, :killmail, :success]` - When a killmail is successfully fetched
  - `[:wanderer_kills, :fetch, :killmail, :error]` - When a killmail fetch fails
  - `[:wanderer_kills, :fetch, :system, :complete]` - When a system fetch completes
  - `[:wanderer_kills, :fetch, :system, :error]` - When a system fetch fails

  Parser events:
  - `[:wanderer_kills, :parser, :stored]` - When killmails are stored
  - `[:wanderer_kills, :parser, :skipped]` - When killmails are skipped
  - `[:wanderer_kills, :parser, :summary]` - Parser summary statistics

  System events:
  - `[:wanderer_kills, :system, :memory]` - Memory usage metrics
  - `[:wanderer_kills, :system, :cpu]` - CPU usage metrics

  ## Usage

  ```elixir
  # Execute telemetry events using helper functions:
  Telemetry.http_request_start(&quot;GET&quot;, &quot;https://api.example.com&quot;)
  Telemetry.fetch_system_complete(12345, :success)
  Telemetry.cache_hit(&quot;my_key&quot;)

  # Attach/detach handlers during application lifecycle
  Telemetry.attach_handlers()
  Telemetry.detach_handlers()
  ```

  ## Note

  Periodic measurements and metrics collection are handled by
  `WandererKills.Observability.Monitoring` module.
  &quot;&quot;&quot;

  require Logger

  # -------------------------------------------------
  # Helper functions for telemetry execution
  # -------------------------------------------------

  @doc &quot;&quot;&quot;
  Executes HTTP request start telemetry.
  &quot;&quot;&quot;
  @spec http_request_start(String.t(), String.t()) :: :ok
  def http_request_start(method, url) do
    :telemetry.execute(
      [:wanderer_kills, :http, :request, :start],
      %{system_time: System.system_time(:native)},
      %{method: method, url: url}
    )
  end

  @doc &quot;&quot;&quot;
  Executes HTTP request stop telemetry.
  &quot;&quot;&quot;
  @spec http_request_stop(String.t(), String.t(), integer(), integer()) :: :ok
  def http_request_stop(method, url, duration, status_code) do
    :telemetry.execute(
      [:wanderer_kills, :http, :request, :stop],
      %{duration: duration},
      %{method: method, url: url, status_code: status_code}
    )
  end

  @doc &quot;&quot;&quot;
  Executes HTTP request error telemetry.
  &quot;&quot;&quot;
  @spec http_request_error(String.t(), String.t(), integer(), term()) :: :ok
  def http_request_error(method, url, duration, error) do
    :telemetry.execute(
      [:wanderer_kills, :http, :request, :stop],
      %{duration: duration},
      %{method: method, url: url, error: error}
    )
  end

  @doc &quot;&quot;&quot;
  Executes fetch system start telemetry.
  &quot;&quot;&quot;
  @spec fetch_system_start(integer(), integer(), atom()) :: :ok
  def fetch_system_start(system_id, limit, source) do
    :telemetry.execute(
      [:wanderer_kills, :fetch, :system, :start],
      %{system_id: system_id, limit: limit},
      %{source: source}
    )
  end

  @doc &quot;&quot;&quot;
  Executes fetch system complete telemetry.
  &quot;&quot;&quot;
  @spec fetch_system_complete(integer(), atom()) :: :ok
  def fetch_system_complete(system_id, result) do
    :telemetry.execute(
      [:wanderer_kills, :fetch, :system, :complete],
      %{system_id: system_id},
      %{result: result}
    )
  end

  @doc &quot;&quot;&quot;
  Executes fetch system success telemetry.
  &quot;&quot;&quot;
  @spec fetch_system_success(integer(), integer(), atom()) :: :ok
  def fetch_system_success(system_id, killmail_count, source) do
    :telemetry.execute(
      [:wanderer_kills, :fetch, :system, :success],
      %{system_id: system_id, killmail_count: killmail_count},
      %{source: source}
    )
  end

  @doc &quot;&quot;&quot;
  Executes fetch system error telemetry.
  &quot;&quot;&quot;
  @spec fetch_system_error(integer(), term(), atom()) :: :ok
  def fetch_system_error(system_id, error, source) do
    :telemetry.execute(
      [:wanderer_kills, :fetch, :system, :error],
      %{system_id: system_id},
      %{error: error, source: source}
    )
  end

  @doc &quot;&quot;&quot;
  Executes cache hit telemetry.
  &quot;&quot;&quot;
  @spec cache_hit(String.t()) :: :ok
  def cache_hit(key) do
    :telemetry.execute(
      [:wanderer_kills, :cache, :hit],
      %{},
      %{key: key}
    )
  end

  @doc &quot;&quot;&quot;
  Executes cache miss telemetry.
  &quot;&quot;&quot;
  @spec cache_miss(String.t()) :: :ok
  def cache_miss(key) do
    :telemetry.execute(
      [:wanderer_kills, :cache, :miss],
      %{},
      %{key: key}
    )
  end

  @doc &quot;&quot;&quot;
  Executes cache error telemetry.
  &quot;&quot;&quot;
  @spec cache_error(String.t(), term()) :: :ok
  def cache_error(key, reason) do
    :telemetry.execute(
      [:wanderer_kills, :cache, :error],
      %{},
      %{key: key, reason: reason}
    )
  end

  @doc &quot;&quot;&quot;
  Executes parser telemetry.
  &quot;&quot;&quot;
  @spec parser_stored(integer()) :: :ok
  def parser_stored(count \\ 1) do
    :telemetry.execute(
      [:wanderer_kills, :parser, :stored],
      %{count: count},
      %{}
    )
  end

  @doc &quot;&quot;&quot;
  Executes parser skipped telemetry.
  &quot;&quot;&quot;
  @spec parser_skipped(integer()) :: :ok
  def parser_skipped(count \\ 1) do
    :telemetry.execute(
      [:wanderer_kills, :parser, :skipped],
      %{count: count},
      %{}
    )
  end

  @doc &quot;&quot;&quot;
  Executes parser summary telemetry.
  &quot;&quot;&quot;
  @spec parser_summary(integer(), integer()) :: :ok
  def parser_summary(stored, skipped) do
    :telemetry.execute(
      [:wanderer_kills, :parser, :summary],
      %{stored: stored, skipped: skipped},
      %{}
    )
  end

  # -------------------------------------------------
  # Handler attachment/detachment functions
  # -------------------------------------------------

  @doc &quot;&quot;&quot;
  Attaches telemetry event handlers for all application events.

  This should be called during application startup to ensure
  all telemetry events are properly logged and processed.
  &quot;&quot;&quot;
  @spec attach_handlers() :: :ok
  def attach_handlers do
    # Cache hit/miss handlers
    :telemetry.attach_many(
      &quot;wanderer-kills-cache-handler&quot;,
      [
        [:wanderer_kills, :cache, :hit],
        [:wanderer_kills, :cache, :miss],
        [:wanderer_kills, :cache, :error]
      ],
      &amp;WandererKills.Observability.Telemetry.handle_cache_event/4,
      nil
    )

    # HTTP request handlers
    :telemetry.attach_many(
      &quot;wanderer-kills-http-handler&quot;,
      [
        [:wanderer_kills, :http, :request, :start],
        [:wanderer_kills, :http, :request, :stop]
      ],
      &amp;WandererKills.Observability.Telemetry.handle_http_event/4,
      nil
    )

    # Fetch handlers
    :telemetry.attach_many(
      &quot;wanderer-kills-fetch-handler&quot;,
      [
        [:wanderer_kills, :fetch, :killmail, :success],
        [:wanderer_kills, :fetch, :killmail, :error],
        [:wanderer_kills, :fetch, :system, :start],
        [:wanderer_kills, :fetch, :system, :complete],
        [:wanderer_kills, :fetch, :system, :success],
        [:wanderer_kills, :fetch, :system, :error]
      ],
      &amp;WandererKills.Observability.Telemetry.handle_fetch_event/4,
      nil
    )

    # Parser handlers
    :telemetry.attach_many(
      &quot;wanderer-kills-parser-handler&quot;,
      [
        [:wanderer_kills, :parser, :stored],
        [:wanderer_kills, :parser, :skipped],
        [:wanderer_kills, :parser, :summary]
      ],
      &amp;WandererKills.Observability.Telemetry.handle_parser_event/4,
      nil
    )

    # System metrics handlers
    :telemetry.attach_many(
      &quot;wanderer-kills-system-handler&quot;,
      [
        [:wanderer_kills, :system, :memory],
        [:wanderer_kills, :system, :cpu]
      ],
      &amp;WandererKills.Observability.Telemetry.handle_system_event/4,
      nil
    )

    :ok
  end

  @doc &quot;&quot;&quot;
  Detaches all telemetry event handlers.

  This should be called during application shutdown to clean up
  telemetry handlers properly.
  &quot;&quot;&quot;
  @spec detach_handlers() :: :ok
  def detach_handlers do
    :telemetry.detach(&quot;wanderer-kills-cache-handler&quot;)
    :telemetry.detach(&quot;wanderer-kills-http-handler&quot;)
    :telemetry.detach(&quot;wanderer-kills-fetch-handler&quot;)
    :telemetry.detach(&quot;wanderer-kills-parser-handler&quot;)
    :telemetry.detach(&quot;wanderer-kills-system-handler&quot;)
    :ok
  end

  # -------------------------------------------------
  # Event handlers
  # -------------------------------------------------

  @doc &quot;&quot;&quot;
  Handles cache-related telemetry events.
  &quot;&quot;&quot;
  def handle_cache_event([:wanderer_kills, :cache, event], _measurements, metadata, _config) do
    case event do
      :hit -&gt;
        Logger.debug(&quot;[Cache] Hit for key: #{inspect(metadata.key)}&quot;)

      :miss -&gt;
        Logger.debug(&quot;[Cache] Miss for key: #{inspect(metadata.key)}&quot;)

      :error -&gt;
        Logger.error(
          &quot;[Cache] Error for key: #{inspect(metadata.key)}, reason: #{inspect(metadata.reason)}&quot;
        )
    end
  end

  @doc &quot;&quot;&quot;
  Handles HTTP request telemetry events.
  &quot;&quot;&quot;
  def handle_http_event(
        [:wanderer_kills, :http, :request, event],
        _measurements,
        metadata,
        _config
      ) do
    case event do
      :start -&gt;
        Logger.debug(&quot;[HTTP] Starting request: #{metadata.method} #{metadata.url}&quot;)

      :stop -&gt;
        case metadata do
          %{status_code: status} -&gt;
            Logger.debug(
              &quot;[HTTP] Completed request: #{metadata.method} #{metadata.url} (#{status})&quot;
            )

          %{error: reason} -&gt;
            Logger.error(
              &quot;[HTTP] Failed request: #{metadata.method} #{metadata.url} (#{inspect(reason)})&quot;
            )
        end
    end
  end

  @doc &quot;&quot;&quot;
  Handles fetch operation telemetry events.
  &quot;&quot;&quot;
  def handle_fetch_event([:wanderer_kills, :fetch, type, event], measurements, metadata, _config) do
    case {type, event} do
      {:killmail, :success} -&gt;
        Logger.debug(&quot;[Fetch] Successfully fetched killmail: #{measurements.killmail_id}&quot;)

      {:killmail, :error} -&gt;
        Logger.error(
          &quot;[Fetch] Failed to fetch killmail: #{measurements.killmail_id}, reason: #{inspect(metadata.error)}&quot;
        )

      {:system, :start} -&gt;
        Logger.debug(
          &quot;[Fetch] Starting system fetch: #{measurements.system_id} (limit: #{measurements.limit})&quot;
        )

      {:system, :complete} -&gt;
        Logger.debug(
          &quot;[Fetch] Completed system fetch: #{measurements.system_id} (#{metadata.result})&quot;
        )

      {:system, :success} -&gt;
        Logger.debug(
          &quot;[Fetch] Successful system fetch: #{measurements.system_id} (#{measurements.killmail_count} killmails)&quot;
        )

      {:system, :error} -&gt;
        Logger.error(
          &quot;[Fetch] Failed system fetch: #{measurements.system_id}, reason: #{inspect(metadata.error)}&quot;
        )
    end
  end

  @doc &quot;&quot;&quot;
  Handles parser telemetry events.
  &quot;&quot;&quot;
  def handle_parser_event([:wanderer_kills, :parser, event], measurements, _metadata, _config) do
    case event do
      :stored -&gt;
        Logger.debug(&quot;[Parser] Stored #{measurements.count} killmails&quot;)

      :skipped -&gt;
        Logger.debug(&quot;[Parser] Skipped #{measurements.count} killmails&quot;)

      :summary -&gt;
        Logger.info(
          &quot;[Parser] Summary - Stored: #{measurements.stored}, Skipped: #{measurements.skipped}&quot;
        )
    end
  end

  @doc &quot;&quot;&quot;
  Handles system resource telemetry events.
  &quot;&quot;&quot;
  def handle_system_event([:wanderer_kills, :system, event], measurements, _metadata, _config) do
    case event do
      :memory -&gt;
        Logger.debug(
          &quot;[System] Memory usage - Total: #{measurements.total_memory}MB, Process: #{measurements.process_memory}MB&quot;
        )

      :cpu -&gt;
        # Safely handle the case where total_cpu might not be present
        case Map.get(measurements, :total_cpu) do
          nil -&gt;
            # If total_cpu is not available, log the available metrics
            Logger.debug(
              &quot;[System] System metrics - Processes: #{measurements.process_count}, Ports: #{measurements.port_count}, Schedulers: #{measurements.schedulers}, Run Queue: #{measurements.run_queue}&quot;
            )

          total_cpu -&gt;
            # Log with total_cpu and process_cpu if available
            process_cpu = Map.get(measurements, :process_cpu, &quot;N/A&quot;)
            Logger.debug(&quot;[System] CPU usage - Total: #{total_cpu}%, Process: #{process_cpu}%&quot;)
        end
    end
  end
end</file><file path="lib/wanderer_kills/ship_types/csv.ex">defmodule WandererKills.ShipTypes.CSV do
  @moduledoc &quot;&quot;&quot;
  Unified CSV parsing utilities for WandererKills.

  This module consolidates all CSV parsing functionality from across the codebase,
  providing consistent parsing, validation, and error handling for all CSV data
  sources in the application.

  ## Features

  - Standardized CSV file reading with error handling
  - Generic row parsing with header mapping
  - Type-safe number parsing with defaults
  - Schema validation for structured data
  - EVE Online specific parsing for ship types and groups

  ## Usage

  ```elixir
  # Basic CSV reading
  {:ok, records} = CSV.read_file(path, &amp;parse_my_record/1)

  # With validation
  {:ok, records} = CSV.read_file_with_validation(path, &amp;parse_my_record/1, &amp;validate_record/1)

  # Parse specific EVE data
  ship_type = CSV.parse_type_row(csv_row_map)
  ship_group = CSV.parse_group_row(csv_row_map)
  ```
  &quot;&quot;&quot;

  require Logger
  alias NimbleCSV.RFC4180, as: CSVParser
  alias WandererKills.Infrastructure.BatchProcessor
  alias WandererKills.Infrastructure.Error

  @type parse_result :: {:ok, term()} | {:error, Error.t()}
  @type parser_function :: (map() -&gt; term() | nil)
  @type validator_function :: (term() -&gt; boolean())

  @type ship_type :: %{
          type_id: integer(),
          name: String.t(),
          group_id: integer(),
          mass: float(),
          volume: float(),
          capacity: float(),
          portion_size: integer(),
          race_id: integer(),
          base_price: float(),
          published: boolean(),
          market_group_id: integer(),
          icon_id: integer(),
          sound_id: integer(),
          graphic_id: integer()
        }

  @type ship_group :: %{
          group_id: integer(),
          category_id: integer(),
          name: String.t(),
          icon_id: integer(),
          use_base_price: boolean(),
          anchored: boolean(),
          anchorable: boolean(),
          fittable_non_singleton: boolean(),
          published: boolean()
        }

  # ============================================================================
  # File Reading API
  # ============================================================================

  @doc &quot;&quot;&quot;
  Reads a CSV file and converts each row to a record using the provided parser function.

  ## Parameters
  - `file_path` - Path to the CSV file
  - `parser` - Function that converts a row map to a record (returns nil to skip)
  - `opts` - Optional parameters:
    - `:skip_invalid` - Skip rows that return nil from parser (default: true)
    - `:max_errors` - Maximum parse errors before giving up (default: 10)

  ## Returns
  - `{:ok, records}` - List of successfully parsed records
  - `{:error, reason}` - Parse error or file error
  &quot;&quot;&quot;
  @spec read_file(String.t(), parser_function(), keyword()) ::
          {:ok, [term()]} | {:error, Error.t()}
  def read_file(file_path, parser, opts \\ []) do
    skip_invalid = Keyword.get(opts, :skip_invalid, true)
    max_errors = Keyword.get(opts, :max_errors, 10)

    case File.read(file_path) do
      {:ok, content} -&gt;
        parse_csv_content(content, parser, skip_invalid, max_errors)

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to read CSV file #{file_path}: #{inspect(reason)}&quot;)

        {:error,
         Error.csv_error(:file_read_error, &quot;Failed to read CSV file: #{file_path}&quot;, %{
           file_path: file_path,
           reason: reason
         })}
    end
  end

  @doc &quot;&quot;&quot;
  Reads a CSV file with validation of parsed records.

  ## Parameters
  - `file_path` - Path to the CSV file
  - `parser` - Function that converts a row map to a record
  - `validator` - Function that validates a parsed record (returns boolean)
  - `opts` - Same as read_file/3

  ## Returns
  - `{:ok, records}` - List of successfully parsed and validated records
  - `{:error, reason}` - Parse/validation error or file error
  &quot;&quot;&quot;
  @spec read_file_with_validation(String.t(), parser_function(), validator_function(), keyword()) ::
          {:ok, [term()]} | {:error, Error.t()}
  def read_file_with_validation(file_path, parser, validator, opts \\ []) do
    case read_file(file_path, parser, opts) do
      {:ok, records} -&gt;
        valid_records = Enum.filter(records, validator)
        invalid_count = length(records) - length(valid_records)

        if invalid_count &gt; 0 do
          Logger.warning(&quot;Filtered out #{invalid_count} invalid records from #{file_path}&quot;)
        end

        {:ok, valid_records}

      error -&gt;
        error
    end
  end

  # ============================================================================
  # Row Parsing API
  # ============================================================================

  @doc &quot;&quot;&quot;
  Parses a CSV data row into a map using the provided headers.

  ## Parameters
  - `row` - List of values from a CSV row
  - `headers` - List of header names

  ## Returns
  Map with headers as keys and row values as values
  &quot;&quot;&quot;
  @spec parse_row(list(String.t()), list(String.t())) :: map()
  def parse_row(row, headers) do
    headers
    |&gt; Enum.zip(row)
    |&gt; Map.new()
  end

  # ============================================================================
  # Number Parsing API
  # ============================================================================

  @doc &quot;&quot;&quot;
  Parses a string value to integer with error handling.
  &quot;&quot;&quot;
  @spec parse_integer(String.t() | nil) :: {:ok, integer()} | {:error, Error.t()}
  def parse_integer(value) when is_binary(value) and value != &quot;&quot; do
    case Integer.parse(value) do
      {int, &quot;&quot;} -&gt;
        {:ok, int}

      _ -&gt;
        {:error,
         Error.csv_error(:invalid_integer, &quot;Failed to parse string as integer&quot;, %{
           value: value
         })}
    end
  end

  def parse_integer(value) when value in [nil, &quot;&quot;] do
    {:error,
     Error.csv_error(:missing_value, &quot;Cannot parse empty/nil value as integer&quot;, %{
       value: inspect(value)
     })}
  end

  def parse_integer(value) do
    {:error,
     Error.csv_error(:invalid_type, &quot;Cannot parse non-string value as integer&quot;, %{
       value: inspect(value)
     })}
  end

  @doc &quot;&quot;&quot;
  Parses a string value to float with error handling.
  &quot;&quot;&quot;
  @spec parse_float(String.t() | nil) :: {:ok, float()} | {:error, Error.t()}
  def parse_float(value) when is_binary(value) and value != &quot;&quot; do
    case Float.parse(value) do
      {float, _} -&gt;
        {:ok, float}

      :error -&gt;
        {:error,
         Error.csv_error(:invalid_float, &quot;Failed to parse string as float&quot;, %{value: value})}
    end
  end

  def parse_float(value) when value in [nil, &quot;&quot;] do
    {:error,
     Error.csv_error(:missing_value, &quot;Cannot parse empty/nil value as float&quot;, %{
       value: inspect(value)
     })}
  end

  def parse_float(value) do
    {:error,
     Error.csv_error(:invalid_type, &quot;Cannot parse non-string value as float&quot;, %{
       value: inspect(value)
     })}
  end

  @doc &quot;&quot;&quot;
  Parses a number with a default value on error.
  &quot;&quot;&quot;
  @spec parse_number_with_default(String.t() | nil, :integer | :float, number()) :: number()
  def parse_number_with_default(value, type, default) do
    result =
      case type do
        :integer -&gt; parse_integer(value)
        :float -&gt; parse_float(value)
      end

    case result do
      {:ok, number} -&gt; number
      {:error, _} -&gt; default
    end
  end

  @doc &quot;&quot;&quot;
  Parses a boolean value from common CSV representations.
  &quot;&quot;&quot;
  @spec parse_boolean(String.t() | nil, boolean()) :: boolean()
  def parse_boolean(value, default \\ false)

  def parse_boolean(value, _default) when value in [&quot;1&quot;, &quot;true&quot;, &quot;TRUE&quot;, &quot;True&quot;, &quot;yes&quot;, &quot;YES&quot;],
    do: true

  def parse_boolean(value, _default) when value in [&quot;0&quot;, &quot;false&quot;, &quot;FALSE&quot;, &quot;False&quot;, &quot;no&quot;, &quot;NO&quot;],
    do: false

  def parse_boolean(_value, default), do: default

  # ============================================================================
  # EVE Online Specific Parsers
  # ============================================================================

  @doc &quot;&quot;&quot;
  Parses a CSV row map into a ship type record.

  Expected columns: typeID, typeName, groupID, mass, volume, capacity,
  portionSize, raceID, basePrice, published, marketGroupID, iconID, soundID, graphicID
  &quot;&quot;&quot;
  @spec parse_type_row(map()) :: ship_type() | nil
  def parse_type_row(row) when is_map(row) do
    %{
      type_id: parse_number_with_default(row[&quot;typeID&quot;], :integer, 0),
      name: Map.get(row, &quot;typeName&quot;, &quot;&quot;),
      group_id: parse_number_with_default(row[&quot;groupID&quot;], :integer, 0),
      mass: parse_number_with_default(row[&quot;mass&quot;], :float, 0.0),
      volume: parse_number_with_default(row[&quot;volume&quot;], :float, 0.0),
      capacity: parse_number_with_default(row[&quot;capacity&quot;], :float, 0.0),
      portion_size: parse_number_with_default(row[&quot;portionSize&quot;], :integer, 1),
      race_id: parse_number_with_default(row[&quot;raceID&quot;], :integer, 0),
      base_price: parse_number_with_default(row[&quot;basePrice&quot;], :float, 0.0),
      published: parse_boolean(row[&quot;published&quot;], false),
      market_group_id: parse_number_with_default(row[&quot;marketGroupID&quot;], :integer, 0),
      icon_id: parse_number_with_default(row[&quot;iconID&quot;], :integer, 0),
      sound_id: parse_number_with_default(row[&quot;soundID&quot;], :integer, 0),
      graphic_id: parse_number_with_default(row[&quot;graphicID&quot;], :integer, 0)
    }
  rescue
    error -&gt;
      Logger.warning(&quot;Failed to parse ship type row: #{inspect(error)}, row: #{inspect(row)}&quot;)
      nil
  end

  def parse_type_row(_), do: nil

  @doc &quot;&quot;&quot;
  Parses a CSV row map into a ship group record.

  Expected columns: groupID, categoryID, groupName, iconID, useBasePrice,
  anchored, anchorable, fittableNonSingleton, published
  &quot;&quot;&quot;
  @spec parse_group_row(map()) :: ship_group() | nil
  def parse_group_row(row) when is_map(row) do
    %{
      group_id: parse_number_with_default(row[&quot;groupID&quot;], :integer, 0),
      category_id: parse_number_with_default(row[&quot;categoryID&quot;], :integer, 0),
      name: Map.get(row, &quot;groupName&quot;, &quot;&quot;),
      icon_id: parse_number_with_default(row[&quot;iconID&quot;], :integer, 0),
      use_base_price: parse_boolean(row[&quot;useBasePrice&quot;], false),
      anchored: parse_boolean(row[&quot;anchored&quot;], false),
      anchorable: parse_boolean(row[&quot;anchorable&quot;], false),
      fittable_non_singleton: parse_boolean(row[&quot;fittableNonSingleton&quot;], false),
      published: parse_boolean(row[&quot;published&quot;], false)
    }
  rescue
    error -&gt;
      Logger.warning(&quot;Failed to parse ship group row: #{inspect(error)}, row: #{inspect(row)}&quot;)
      nil
  end

  def parse_group_row(_), do: nil

  # ============================================================================
  # Ship Type Update Pipeline
  # ============================================================================

  @eve_db_dump_url &quot;https://www.fuzzwork.co.uk/dump/latest&quot;
  @required_files [&quot;invGroups.csv&quot;, &quot;invTypes.csv&quot;]

  @doc &quot;&quot;&quot;
  Complete ship type update pipeline: download -&gt; parse -&gt; process.

  ## Parameters
  - `opts` - Options passed to the download step

  ## Returns
  - `:ok` - Complete pipeline succeeded
  - `{:error, reason}` - Pipeline failed at some step
  &quot;&quot;&quot;
  @spec update_ship_types(keyword()) :: :ok | {:error, Error.t()}
  def update_ship_types(opts \\ []) do
    Logger.debug(&quot;Starting ship type update from CSV&quot;)

    with {:ok, raw_data} &lt;- download_csv_files(opts),
         {:ok, _parsed_data} &lt;- parse_ship_type_csvs(raw_data) do
      Logger.debug(&quot;Ship type update from CSV completed successfully&quot;)
      :ok
    else
      {:error, reason} -&gt;
        Logger.error(&quot;Ship type update from CSV failed: #{inspect(reason)}&quot;)
        {:error, reason}
    end
  end

  @doc &quot;&quot;&quot;
  Downloads CSV files for ship type data.

  ## Parameters
  - `opts` - Download options including `:force_download`

  ## Returns
  - `{:ok, file_paths}` - List of downloaded file paths
  - `{:error, reason}` - Download failed
  &quot;&quot;&quot;
  @spec download_csv_files(keyword()) :: {:ok, [String.t()]} | {:error, Error.t()}
  def download_csv_files(opts \\ []) do
    Logger.debug(&quot;Downloading CSV files for ship type data&quot;)

    data_dir = get_data_directory()
    File.mkdir_p!(data_dir)

    force_download = Keyword.get(opts, :force_download, false)

    missing_files =
      if force_download do
        @required_files
      else
        get_missing_files(data_dir)
      end

    if Enum.empty?(missing_files) do
      Logger.debug(&quot;All required CSV files are present&quot;)
      {:ok, get_file_paths(data_dir)}
    else
      Logger.info(&quot;Downloading #{length(missing_files)} CSV files: #{inspect(missing_files)}&quot;)

      case download_files(missing_files, data_dir) do
        :ok -&gt; {:ok, get_file_paths(data_dir)}
        {:error, reason} -&gt; {:error, reason}
      end
    end
  end

  @doc &quot;&quot;&quot;
  Parses CSV files into ship type data.

  ## Parameters
  - `file_paths` - List of CSV file paths to parse

  ## Returns
  - `{:ok, ship_types}` - Parsed ship type data
  - `{:error, reason}` - Parsing failed
  &quot;&quot;&quot;
  @spec parse_ship_type_csvs([String.t()]) :: {:ok, [map()]} | {:error, Error.t()}
  def parse_ship_type_csvs(file_paths) when is_list(file_paths) do
    Logger.debug(&quot;Parsing ship type data from CSV files&quot;)

    case find_csv_files(file_paths) do
      {:ok, {types_path, groups_path}} -&gt;
        process_csv_data(types_path, groups_path)

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  # ============================================================================
  # Private Functions
  # ============================================================================

  @spec parse_csv_content(String.t(), parser_function(), boolean(), pos_integer()) ::
          {:ok, [term()]} | {:error, Error.t()}
  defp parse_csv_content(content, parser, skip_invalid, max_errors) do
    rows =
      content
      |&gt; CSVParser.parse_string(skip_headers: false)
      |&gt; Enum.to_list()

    case rows do
      [] -&gt;
        {:ok, []}

      [headers | data_rows] -&gt;
        process_rows(data_rows, headers, parser, skip_invalid, max_errors)
    end
  rescue
    error -&gt;
      Logger.error(&quot;CSV parsing failed: #{inspect(error)}&quot;)

      {:error,
       Error.csv_error(:parse_error, &quot;Failed to parse CSV content&quot;, %{
         error: inspect(error)
       })}
  end

  @spec process_rows([list()], list(), parser_function(), boolean(), pos_integer()) ::
          {:ok, [term()]} | {:error, Error.t()}
  defp process_rows(data_rows, headers, parser, skip_invalid, max_errors) do
    {records, errors} =
      data_rows
      |&gt; Enum.with_index(1)
      |&gt; Enum.reduce({[], []}, fn {row, line_num}, {acc_records, acc_errors} -&gt;
        if length(acc_errors) &gt;= max_errors do
          {acc_records, acc_errors}
        else
          try do
            row_map = parse_row(row, headers)
            result = parser.(row_map)

            cond do
              result == nil and skip_invalid -&gt;
                {acc_records, acc_errors}

              result == nil -&gt;
                error = &quot;Parser returned nil for row #{line_num}&quot;
                {acc_records, [error | acc_errors]}

              true -&gt;
                {[result | acc_records], acc_errors}
            end
          rescue
            error -&gt;
              error_msg = &quot;Parse error on line #{line_num}: #{inspect(error)}&quot;
              {acc_records, [error_msg | acc_errors]}
          end
        end
      end)

    if length(errors) &gt;= max_errors do
      Logger.error(&quot;Too many CSV parse errors (#{length(errors)} &gt;= #{max_errors})&quot;)

      {:error,
       Error.csv_error(:too_many_errors, &quot;Exceeded maximum parse errors&quot;, %{
         error_count: length(errors),
         max_errors: max_errors,
         errors: Enum.reverse(errors)
       })}
    else
      if length(errors) &gt; 0 do
        Logger.warning(&quot;CSV parsing completed with #{length(errors)} errors&quot;)
      end

      {:ok, Enum.reverse(records)}
    end
  end

  # Ship type CSV download/processing helpers
  defp get_data_directory do
    Path.join([:code.priv_dir(:wanderer_kills), &quot;data&quot;])
  end

  defp get_missing_files(data_dir) do
    @required_files
    |&gt; Enum.reject(fn file -&gt;
      File.exists?(Path.join(data_dir, file))
    end)
  end

  defp get_file_paths(data_dir) do
    @required_files
    |&gt; Enum.map(&amp;Path.join(data_dir, &amp;1))
  end

  defp download_files(file_names, data_dir) do
    download_fn = fn file_name -&gt; download_single_file(file_name, data_dir) end

    case BatchProcessor.process_parallel(file_names, download_fn,
           timeout: :timer.minutes(5),
           description: &quot;CSV file downloads&quot;
         ) do
      {:ok, _results} -&gt;
        Logger.info(&quot;Successfully downloaded all CSV files&quot;)
        :ok

      {:partial, _results, failures} -&gt;
        Logger.error(&quot;Some CSV downloads failed: #{inspect(failures)}&quot;)

        {:error,
         Error.ship_types_error(:download_failed, &quot;Some CSV file downloads failed&quot;, true, %{
           failures: failures
         })}

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to download CSV files: #{inspect(reason)}&quot;)

        {:error,
         Error.ship_types_error(:download_failed, &quot;Failed to download CSV files&quot;, true, %{
           underlying_error: reason
         })}
    end
  end

  defp download_single_file(file_name, data_dir) do
    url = &quot;#{@eve_db_dump_url}/#{file_name}&quot;
    download_path = Path.join(data_dir, file_name)

    Logger.info(&quot;Downloading CSV file&quot;, file: file_name, url: url, path: download_path)

    case WandererKills.Http.ClientProvider.get_client().get(url, []) do
      {:ok, %{body: body}} -&gt;
        case File.write(download_path, body) do
          :ok -&gt;
            Logger.info(&quot;Successfully downloaded #{file_name}&quot;)
            :ok

          {:error, reason} -&gt;
            Logger.error(&quot;Failed to write file #{file_name}: #{inspect(reason)}&quot;)
            {:error, reason}
        end

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to download file #{file_name}: #{inspect(reason)}&quot;)
        {:error, reason}
    end
  end

  defp find_csv_files(file_paths) do
    types_path = Enum.find(file_paths, &amp;String.ends_with?(&amp;1, &quot;invTypes.csv&quot;))
    groups_path = Enum.find(file_paths, &amp;String.ends_with?(&amp;1, &quot;invGroups.csv&quot;))

    case {types_path, groups_path} do
      {nil, _} -&gt;
        {:error,
         Error.ship_types_error(
           :missing_types_file,
           &quot;invTypes.csv file not found in provided paths&quot;
         )}

      {_, nil} -&gt;
        {:error,
         Error.ship_types_error(
           :missing_groups_file,
           &quot;invGroups.csv file not found in provided paths&quot;
         )}

      {types, groups} -&gt;
        {:ok, {types, groups}}
    end
  end

  defp process_csv_data(types_path, groups_path) do
    Logger.debug(&quot;Processing CSV data from files&quot;, types: types_path, groups: groups_path)

    with {:ok, types_data} &lt;- parse_csv_file_simple(types_path),
         {:ok, groups_data} &lt;- parse_csv_file_simple(groups_path) do
      ship_types = build_ship_types(types_data, groups_data)
      Logger.debug(&quot;Successfully processed #{length(ship_types)} ship types from CSV&quot;)
      {:ok, ship_types}
    else
      {:error, reason} -&gt;
        Logger.error(&quot;Failed to process CSV data: #{inspect(reason)}&quot;)

        {:error,
         Error.ship_types_error(:csv_processing_failed, &quot;Failed to process CSV data&quot;, false, %{
           underlying_error: reason
         })}
    end
  end

  defp parse_csv_file_simple(file_path) do
    try do
      case File.read(file_path) do
        {:ok, content} -&gt;
          # Parse CSV content using simple string splitting
          rows =
            content
            |&gt; String.split(&quot;\n&quot;)
            |&gt; Enum.reject(&amp;(&amp;1 == &quot;&quot;))
            |&gt; Enum.map(&amp;String.split(&amp;1, &quot;,&quot;))

          case rows do
            [headers | data_rows] -&gt;
              parsed_data =
                data_rows
                |&gt; Enum.map(fn row -&gt;
                  headers
                  |&gt; Enum.zip(row)
                  |&gt; Map.new()
                end)

              {:ok, parsed_data}

            [] -&gt;
              {:ok, []}
          end

        {:error, reason} -&gt;
          {:error,
           Error.csv_error(:file_read_error, &quot;Failed to read CSV file&quot;, %{
             file: file_path,
             reason: reason
           })}
      end
    rescue
      error -&gt;
        {:error,
         Error.csv_error(:parse_error, &quot;Failed to parse CSV file&quot;, %{
           file: file_path,
           error: inspect(error)
         })}
    end
  end

  defp build_ship_types(types, groups) do
    # Get ship group IDs from configuration
    ship_group_ids = [6, 7, 9, 11, 16, 17, 23]

    groups_map = build_groups_map(groups)

    types
    |&gt; filter_ship_types(ship_group_ids)
    |&gt; map_to_ship_types(groups_map)
    |&gt; Enum.reject(&amp;is_nil(&amp;1.type_id))
  end

  @spec build_groups_map([map()]) :: map()
  defp build_groups_map(groups) do
    groups
    |&gt; Enum.reduce(%{}, fn row, acc -&gt;
      build_single_group_entry(row, acc)
    end)
  end

  @spec build_single_group_entry(map(), map()) :: map()
  defp build_single_group_entry(row, acc) do
    group_id_str = Map.get(row, &quot;groupID&quot;)
    group_name = Map.get(row, &quot;groupName&quot;)

    if valid_group_entry?(group_id_str, group_name) do
      case Integer.parse(group_id_str) do
        {group_id, &quot;&quot;} -&gt; Map.put(acc, group_id, group_name)
        _ -&gt; acc
      end
    else
      acc
    end
  end

  @spec valid_group_entry?(term(), term()) :: boolean()
  defp valid_group_entry?(group_id_str, group_name) do
    is_binary(group_id_str) and is_binary(group_name)
  end

  @spec filter_ship_types([map()], [integer()]) :: [map()]
  defp filter_ship_types(types, ship_group_ids) do
    Enum.filter(types, fn row -&gt;
      ship_type?(row, ship_group_ids)
    end)
  end

  @spec ship_type?(map(), [integer()]) :: boolean()
  defp ship_type?(row, ship_group_ids) do
    group_id_str = Map.get(row, &quot;groupID&quot;)

    if is_binary(group_id_str) do
      case Integer.parse(group_id_str) do
        {group_id, &quot;&quot;} -&gt; group_id in ship_group_ids
        _ -&gt; false
      end
    else
      false
    end
  end

  @spec map_to_ship_types([map()], map()) :: [map()]
  defp map_to_ship_types(filtered_types, groups_map) do
    Enum.map(filtered_types, fn row -&gt;
      create_ship_type_entry(row, groups_map)
    end)
  end

  @spec create_ship_type_entry(map(), map()) :: map()
  defp create_ship_type_entry(row, groups_map) do
    type_id = parse_integer_simple(Map.get(row, &quot;typeID&quot;))
    group_id = parse_integer_simple(Map.get(row, &quot;groupID&quot;))

    %{
      type_id: type_id,
      name: Map.get(row, &quot;typeName&quot;, &quot;Unknown&quot;),
      group_id: group_id,
      group_name: Map.get(groups_map, group_id, &quot;Unknown Group&quot;)
    }
  end

  defp parse_integer_simple(nil), do: nil

  defp parse_integer_simple(str) when is_binary(str) do
    case Integer.parse(str) do
      {int, &quot;&quot;} -&gt; int
      _ -&gt; nil
    end
  end

  defp parse_integer_simple(int) when is_integer(int), do: int
end</file><file path="lib/wanderer_kills/ship_types/info.ex">defmodule WandererKills.ShipTypes.Info do
  @moduledoc &quot;&quot;&quot;
  Ship type information handler for the ship types domain.

  This module provides ship type data access by leveraging
  the existing ESI caching infrastructure and CSV data sources.
  &quot;&quot;&quot;

  require Logger
  alias WandererKills.Cache.Helper
  alias WandererKills.Infrastructure.Error

  @doc &quot;&quot;&quot;
  Gets ship type information from the ESI cache.

  This function first tries to get the data from cache, and if not found,
  falls back to ESI API as needed.
  &quot;&quot;&quot;
  @spec get_ship_type(integer()) :: {:ok, map()} | {:error, term()}
  def get_ship_type(type_id) when is_integer(type_id) and type_id &gt; 0 do
    Helper.ship_type_get(type_id)
  end

  def get_ship_type(_type_id) do
    {:error, Error.ship_types_error(:invalid_type_id, &quot;Type ID must be a positive integer&quot;)}
  end

  @doc &quot;&quot;&quot;
  Warms the cache with CSV data if needed.

  This is called during application startup to populate the cache
  with local CSV data before relying on ESI API calls.
  &quot;&quot;&quot;
  @spec warm_cache() :: :ok | {:error, term()}
  def warm_cache do
    Logger.debug(&quot;Warming ship type cache with CSV data&quot;)

    # Use the updater which handles downloading missing CSV files
    case WandererKills.ShipTypes.Updater.update_with_csv() do
      :ok -&gt;
        Logger.debug(&quot;Successfully warmed cache with CSV data&quot;)
        :ok

      result -&gt;
        Logger.warning(&quot;Failed to warm cache with CSV data: #{inspect(result)}&quot;)
        # Don&apos;t fail if CSV loading fails - ESI fallback will work
        :ok
    end
  end
end</file><file path="lib/wanderer_kills/ship_types/updater.ex"># lib/wanderer_kills/ship_types/updater.ex
defmodule WandererKills.ShipTypes.Updater do
  @moduledoc &quot;&quot;&quot;
  Coordinates ship type updates from multiple sources.

  This module provides a unified interface for updating ship type data by
  delegating to source implementations. It tries CSV first for efficiency,
  then falls back to ESI if needed.

  ## Usage

  ```elixir
  # Update ship types with automatic fallback
  case WandererKills.ShipTypes.Updater.update_ship_types() do
    :ok -&gt; Logger.info(&quot;Ship types updated successfully&quot;)
    {:error, _reason} -&gt; Logger.error(&quot;Failed to update ship types&quot;)
  end

  # Force update from specific source
  WandererKills.ShipTypes.Updater.update_with_csv()
  WandererKills.ShipTypes.Updater.update_with_esi()
  ```

  ## Strategy

  1. **CSV First**: Attempts to update from local/downloaded CSV files for speed
  2. **ESI Fallback**: Falls back to ESI API if CSV update fails
  3. **Error Handling**: Provides detailed error reporting for each method

  ## Dependencies

  - `WandererKills.ShipTypes.CSV` - CSV-based updates
  - `WandererKills.ESI.Client` - ESI-based updates
  &quot;&quot;&quot;

  require Logger
  alias WandererKills.ESI.Client, as: EsiSource
  alias WandererKills.Infrastructure.Error
  alias WandererKills.ShipTypes.CSV

  # ============================================================================
  # Constants
  # ============================================================================

  @doc &quot;&quot;&quot;
  Lists all ship group IDs that contain ship types.

  These group IDs represent different categories of ships in EVE Online:
  - 6: Titan
  - 7: Dreadnought
  - 9: Battleship
  - 11: Battlecruiser
  - 16: Cruiser
  - 17: Destroyer
  - 23: Frigate

  ## Returns
  List of ship group IDs
  &quot;&quot;&quot;
  @spec ship_group_ids() :: [pos_integer()]
  def ship_group_ids, do: [6, 7, 9, 11, 16, 17, 23]

  @doc &quot;&quot;&quot;
  Gets the base URL for EVE DB dumps.

  ## Returns
  String URL for downloading EVE DB dump files
  &quot;&quot;&quot;
  @spec eve_db_dump_url() :: String.t()
  def eve_db_dump_url, do: &quot;https://www.fuzzwork.co.uk/dump/latest&quot;

  @doc &quot;&quot;&quot;
  Lists the required CSV files for ship type data.

  ## Returns
  List of CSV file names required for ship type processing
  &quot;&quot;&quot;
  @spec required_csv_files() :: [String.t()]
  def required_csv_files, do: [&quot;invGroups.csv&quot;, &quot;invTypes.csv&quot;]

  @doc &quot;&quot;&quot;
  Gets the default maximum concurrency for batch operations.

  ## Returns
  Integer representing maximum concurrent tasks
  &quot;&quot;&quot;
  @spec default_max_concurrency() :: pos_integer()
  def default_max_concurrency, do: 10

  @doc &quot;&quot;&quot;
  Gets the default task timeout in milliseconds.

  ## Returns
  Integer representing timeout in milliseconds
  &quot;&quot;&quot;
  @spec default_task_timeout_ms() :: pos_integer()
  def default_task_timeout_ms, do: 30_000

  @doc &quot;&quot;&quot;
  Gets the data directory path for storing CSV files.

  ## Returns
  String path to the data directory
  &quot;&quot;&quot;
  @spec data_directory() :: String.t()
  def data_directory do
    Path.join([:code.priv_dir(:wanderer_kills), &quot;data&quot;])
  end

  # ============================================================================
  # Ship Type Update Operations
  # ============================================================================

  @doc &quot;&quot;&quot;
  Updates ship types by first trying CSV download, then falling back to ESI.

  This is the main entry point for ship type updates. It implements a fallback
  strategy where CSV is attempted first for efficiency, and ESI is used as a
  backup if CSV fails.

  ## Returns
  - `:ok` - If update completed successfully (from either source)
  - `{:error, reason}` - If both update methods failed

  ## Examples

  ```elixir
  case update_ship_types() do
    :ok -&gt;
      Logger.info(&quot;Ship types updated successfully&quot;)
    {:error, _reason} -&gt;
      Logger.error(&quot;All update methods failed&quot;)
  end
  ```
  &quot;&quot;&quot;
  @spec update_ship_types() :: :ok | {:error, term()}
  def update_ship_types do
    Logger.debug(&quot;Starting ship type update with fallback strategy&quot;)

    case update_with_csv() do
      :ok -&gt;
        Logger.info(&quot;Ship type update completed successfully using CSV data&quot;)
        :ok

      csv_result -&gt;
        Logger.warning(&quot;CSV update failed: #{inspect(csv_result)}, falling back to ESI&quot;)

        case update_with_esi() do
          :ok -&gt;
            Logger.info(&quot;Ship type update completed successfully using ESI fallback&quot;)
            :ok

          esi_result -&gt;
            Logger.error(&quot;Both CSV and ESI updates failed&quot;, %{
              csv_error: csv_result,
              esi_error: esi_result
            })

            {:error,
             Error.ship_types_error(
               :all_update_methods_failed,
               &quot;Both CSV and ESI update methods failed&quot;,
               false,
               %{csv_error: csv_result, esi_error: esi_result}
             )}
        end
    end
  end

  @doc &quot;&quot;&quot;
  Updates ship types using CSV data from EVE DB dumps.

  This method downloads (if needed) and processes CSV files containing
  ship type information. It&apos;s generally faster than ESI but requires
  external file downloads.

  ## Returns
  - `:ok` - If CSV update completed successfully
  - `{:error, reason}` - If CSV update failed

  ## Examples

  ```elixir
  case update_with_csv() do
    :ok -&gt; Logger.info(&quot;CSV update successful&quot;)
    {:error, :download_failed} -&gt; Logger.error(&quot;Failed to download CSV files&quot;)
    {:error, :parse_failed} -&gt; Logger.error(&quot;Failed to parse CSV data&quot;)
  end
  ```
  &quot;&quot;&quot;
  @spec update_with_csv() :: :ok | {:error, term()}
  def update_with_csv do
    Logger.debug(&quot;Attempting ship type update from CSV&quot;)

    case CSV.update_ship_types() do
      :ok -&gt;
        Logger.debug(&quot;CSV ship type update completed successfully&quot;)
        :ok

      {:error, reason} -&gt;
        Logger.error(&quot;CSV ship type update failed: #{inspect(reason)}&quot;)

        {:error,
         Error.ship_types_error(:csv_update_failed, &quot;CSV ship type update failed&quot;, false, %{
           underlying_error: reason
         })}
    end
  end

  @doc &quot;&quot;&quot;
  Updates ship types using ESI API data.

  This method fetches ship type information directly from the EVE Swagger
  Interface. It&apos;s slower than CSV but more reliable for getting current data.

  ## Returns
  - `:ok` - If ESI update completed successfully
  - `{:error, reason}` - If ESI update failed

  ## Examples

  ```elixir
  case update_with_esi() do
    :ok -&gt; Logger.info(&quot;ESI update successful&quot;)
    {:error, :batch_processing_failed} -&gt; Logger.error(&quot;Some ship types failed to process&quot;)
    {:error, _reason} -&gt; Logger.error(&quot;ESI update failed&quot;)
  end
  ```
  &quot;&quot;&quot;
  @spec update_with_esi() :: :ok | {:error, term()}
  def update_with_esi do
    Logger.info(&quot;Attempting ship type update from ESI&quot;)

    case EsiSource.update() do
      :ok -&gt;
        Logger.info(&quot;ESI ship type update completed successfully&quot;)
        :ok

      {:error, reason} -&gt;
        Logger.error(&quot;ESI ship type update failed: #{inspect(reason)}&quot;)

        {:error,
         Error.ship_types_error(:esi_update_failed, &quot;ESI ship type update failed&quot;, false, %{
           underlying_error: reason
         })}
    end
  end

  @doc &quot;&quot;&quot;
  Updates ship types for specific ship groups using ESI.

  This method allows targeted updates of specific ship categories rather
  than processing all ship types.

  ## Parameters
  - `group_ids` - List of ship group IDs to update

  ## Returns
  - `:ok` - If update completed successfully
  - `{:error, reason}` - If update failed

  ## Examples

  ```elixir
  # Update only frigates and cruisers
  update_ship_groups([23, 16])

  # Update all known ship groups
  update_ship_groups(ship_group_ids())
  ```
  &quot;&quot;&quot;
  @spec update_ship_groups([integer()]) :: :ok | {:error, term()}
  def update_ship_groups(group_ids) when is_list(group_ids) do
    Logger.info(&quot;Updating specific ship groups: #{inspect(group_ids)}&quot;)
    EsiSource.update(group_ids: group_ids)
  end

  @doc &quot;&quot;&quot;
  Downloads CSV files for offline processing.

  This is a utility function to pre-download CSV files without processing them.
  Useful for ensuring files are available before attempting CSV updates.

  ## Returns
  - `:ok` - If download completed successfully
  - `{:error, reason}` - If download failed

  ## Examples

  ```elixir
  case download_csv_files() do
    :ok -&gt; Logger.info(&quot;CSV files downloaded and ready&quot;)
    {:error, _reason} -&gt; Logger.error(&quot;Download failed&quot;)
  end
  ```
  &quot;&quot;&quot;
  @spec download_csv_files() :: :ok | {:error, term()}
  def download_csv_files do
    Logger.info(&quot;Downloading CSV files for ship type data&quot;)

    case CSV.download_csv_files(force_download: true) do
      {:ok, _file_paths} -&gt;
        Logger.info(&quot;CSV files downloaded successfully&quot;)
        :ok

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to download CSV files: #{inspect(reason)}&quot;)

        {:error,
         Error.ship_types_error(:csv_download_failed, &quot;Failed to download CSV files&quot;, false, %{
           underlying_error: reason
         })}
    end
  end

  @doc &quot;&quot;&quot;
  Gets configuration information for ship type updates.

  ## Returns
  Map containing configuration details

  ## Examples

  ```elixir
  config = get_configuration()
  # =&gt; %{
  #   ship_groups: [6, 7, 9, 11, 16, 17, 23],
  #   sources: %{csv: &quot;CSV&quot;, esi: &quot;ESI&quot;},
  #   csv_files: [&quot;invGroups.csv&quot;, &quot;invTypes.csv&quot;]
  # }
  ```
  &quot;&quot;&quot;
  @spec get_configuration() :: map()
  def get_configuration do
    %{
      ship_groups: EsiSource.ship_group_ids(),
      sources: %{
        csv: &quot;CSV&quot;,
        esi: EsiSource.source_name()
      },
      csv_files: [&quot;invGroups.csv&quot;, &quot;invTypes.csv&quot;]
    }
  end
end</file><file path="lib/wanderer_kills/client_behaviour.ex">defmodule WandererKills.ClientBehaviour do
  @moduledoc &quot;&quot;&quot;
  Behaviour interface for WandererKills service clients.

  This behaviour defines the contract for interacting with zKillboard data,
  including fetching killmails, managing subscriptions, and accessing cached data.
  &quot;&quot;&quot;

  @type kill :: map()
  @type system_id :: integer()
  @type subscriber_id :: String.t()

  @doc &quot;&quot;&quot;
  Fetches killmails for a specific system within the given time window.

  ## Parameters
  - system_id: The solar system ID to fetch kills for
  - since_hours: Number of hours to look back from now
  - limit: Maximum number of kills to return

  ## Returns
  - {:ok, [kill()]} - List of killmail data
  - {:error, term()} - Error occurred during fetch
  &quot;&quot;&quot;
  @callback fetch_system_kills(
              system_id :: integer(),
              since_hours :: integer(),
              limit :: integer()
            ) ::
              {:ok, [kill()]} | {:error, term()}

  @doc &quot;&quot;&quot;
  Fetches killmails for multiple systems within the given time window.

  ## Parameters
  - system_ids: List of solar system IDs to fetch kills for
  - since_hours: Number of hours to look back from now
  - limit: Maximum number of kills to return per system

  ## Returns
  - {:ok, %{integer() =&gt; [kill()]}} - Map of system_id to killmail lists
  - {:error, term()} - Error occurred during fetch
  &quot;&quot;&quot;
  @callback fetch_systems_kills(
              system_ids :: [integer()],
              since_hours :: integer(),
              limit :: integer()
            ) ::
              {:ok, %{integer() =&gt; [kill()]}} | {:error, term()}

  @doc &quot;&quot;&quot;
  Retrieves cached killmails for a specific system.

  ## Parameters
  - system_id: The solar system ID to get cached kills for

  ## Returns
  - [kill()] - List of cached killmail data (empty list if none cached)
  &quot;&quot;&quot;
  @callback fetch_cached_kills(system_id :: integer()) :: [kill()]

  @doc &quot;&quot;&quot;
  Retrieves cached killmails for multiple systems.

  ## Parameters
  - system_ids: List of solar system IDs to get cached kills for

  ## Returns
  - %{integer() =&gt; [kill()]} - Map of system_id to cached killmail lists
  &quot;&quot;&quot;
  @callback fetch_cached_kills_for_systems(system_ids :: [integer()]) :: %{integer() =&gt; [kill()]}

  @doc &quot;&quot;&quot;
  Subscribes to kill updates for specified systems.

  ## Parameters
  - subscriber_id: Unique identifier for the subscriber
  - system_ids: List of system IDs to subscribe to
  - callback_url: Optional webhook URL for notifications (nil for PubSub only)

  ## Returns
  - {:ok, subscription_id} - Subscription created successfully
  - {:error, term()} - Error occurred during subscription
  &quot;&quot;&quot;
  @callback subscribe_to_kills(
              subscriber_id :: String.t(),
              system_ids :: [integer()],
              callback_url :: String.t() | nil
            ) ::
              {:ok, subscription_id :: String.t()} | {:error, term()}

  @doc &quot;&quot;&quot;
  Unsubscribes from all kill updates for a subscriber.

  ## Parameters
  - subscriber_id: Unique identifier for the subscriber to unsubscribe

  ## Returns
  - :ok - Successfully unsubscribed
  - {:error, term()} - Error occurred during unsubscription
  &quot;&quot;&quot;
  @callback unsubscribe_from_kills(subscriber_id :: String.t()) :: :ok | {:error, term()}

  @doc &quot;&quot;&quot;
  Retrieves a specific killmail by ID.

  ## Parameters
  - killmail_id: The killmail ID to retrieve

  ## Returns
  - kill() - The killmail data if found
  - nil - Killmail not found
  &quot;&quot;&quot;
  @callback get_killmail(killmail_id :: integer()) :: kill() | nil

  @doc &quot;&quot;&quot;
  Gets the current kill count for a system.

  ## Parameters
  - system_id: The solar system ID to get kill count for

  ## Returns
  - integer() - Current kill count for the system
  &quot;&quot;&quot;
  @callback get_system_kill_count(system_id :: integer()) :: integer()
end</file><file path="lib/wanderer_kills/client.ex">defmodule WandererKills.Client do
  @moduledoc &quot;&quot;&quot;
  Main client implementation for WandererKills service.

  This module implements the WandererKills.ClientBehaviour and provides
  a unified interface for fetching killmails, managing subscriptions,
  and accessing cached data. It coordinates between the ZKB client,
  cache, and subscription manager.
  &quot;&quot;&quot;

  @behaviour WandererKills.ClientBehaviour

  require Logger
  alias WandererKills.{SubscriptionManager, Types}
  alias WandererKills.Killmails.ZkbClient
  alias WandererKills.Cache.Helper

  @impl true
  def fetch_system_kills(system_id, since_hours, limit) do
    Logger.debug(&quot;Fetching system kills via WandererKills.Client&quot;,
      system_id: system_id,
      since_hours: since_hours,
      limit: limit
    )

    case ZkbClient.fetch_system_killmails(system_id, limit, since_hours) do
      {:ok, kills} -&gt;
        # Filter by time window if needed (since ZKB API doesn&apos;t support time filtering directly)
        filtered_kills = filter_kills_by_time(kills, since_hours)
        limited_kills = Enum.take(filtered_kills, limit)

        Logger.info(&quot;Successfully fetched system kills&quot;,
          system_id: system_id,
          total_kills: length(kills),
          filtered_kills: length(filtered_kills),
          returned_kills: length(limited_kills)
        )

        {:ok, limited_kills}

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to fetch system kills&quot;,
          system_id: system_id,
          since_hours: since_hours,
          limit: limit,
          error: reason
        )

        {:error, reason}
    end
  end

  @impl true
  def fetch_systems_kills(system_ids, since_hours, limit) do
    Logger.debug(&quot;Fetching kills for multiple systems&quot;,
      system_ids: system_ids,
      since_hours: since_hours,
      limit: limit
    )

    # Batch fetch kills for all systems
    tasks =
      system_ids
      |&gt; Enum.map(fn system_id -&gt;
        Task.async(fn -&gt;
          {system_id, fetch_system_kills(system_id, since_hours, limit)}
        end)
      end)

    # Collect results
    results =
      tasks
      |&gt; Enum.map(&amp;Task.await(&amp;1, 30_000))
      |&gt; Enum.reduce(%{}, fn {system_id, result}, acc -&gt;
        case result do
          {:ok, kills} -&gt; Map.put(acc, system_id, kills)
          {:error, _reason} -&gt; Map.put(acc, system_id, [])
        end
      end)

    total_kills =
      case results do
        res when is_map(res) -&gt; res |&gt; Map.values() |&gt; List.flatten() |&gt; length()
        res when is_list(res) -&gt; res |&gt; List.flatten() |&gt; length()
        _ -&gt; 0
      end

    Logger.info(&quot;Fetched kills for multiple systems&quot;,
      requested_systems: length(system_ids),
      successful_systems: map_size(results),
      total_kills: total_kills
    )

    {:ok, results}
  end

  @impl true
  def fetch_cached_kills(system_id) do
    Logger.debug(&quot;Fetching cached kills&quot;, system_id: system_id)

    case Helper.system_get_killmails(system_id) do
      {:ok, kills} when is_list(kills) -&gt;
        Logger.debug(&quot;Retrieved cached kills&quot;,
          system_id: system_id,
          kill_count: length(kills)
        )

        kills

      {:error, reason} -&gt;
        Logger.warning(&quot;Failed to fetch cached kills&quot;,
          system_id: system_id,
          error: reason
        )

        []

      _ -&gt;
        Logger.warning(&quot;Unexpected response from cache&quot;,
          system_id: system_id
        )

        []
    end
  end

  @impl true
  def fetch_cached_kills_for_systems(system_ids) do
    Logger.debug(&quot;Fetching cached kills for multiple systems&quot;,
      system_ids: system_ids
    )

    results =
      system_ids
      |&gt; Enum.map(fn system_id -&gt;
        {system_id, fetch_cached_kills(system_id)}
      end)
      |&gt; Map.new()

    total_kills = results |&gt; Map.values() |&gt; List.flatten() |&gt; length()

    Logger.debug(&quot;Retrieved cached kills for multiple systems&quot;,
      requested_systems: length(system_ids),
      total_cached_kills: total_kills
    )

    results
  end

  @impl true
  def subscribe_to_kills(subscriber_id, system_ids, callback_url \\ nil) do
    Logger.debug(&quot;Creating kill subscription&quot;,
      subscriber_id: subscriber_id,
      system_ids: system_ids,
      has_callback: !is_nil(callback_url)
    )

    case SubscriptionManager.subscribe(subscriber_id, system_ids, callback_url) do
      {:ok, subscription_id} -&gt;
        Logger.info(&quot;Kill subscription created&quot;,
          subscriber_id: subscriber_id,
          subscription_id: subscription_id,
          system_count: length(system_ids)
        )

        {:ok, subscription_id}

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to create kill subscription&quot;,
          subscriber_id: subscriber_id,
          error: reason
        )

        {:error, reason}
    end
  end

  @impl true
  def unsubscribe_from_kills(subscriber_id) do
    Logger.debug(&quot;Removing kill subscription&quot;, subscriber_id: subscriber_id)

    case SubscriptionManager.unsubscribe(subscriber_id) do
      :ok -&gt;
        Logger.info(&quot;Kill subscription removed&quot;, subscriber_id: subscriber_id)
        :ok

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to remove kill subscription&quot;,
          subscriber_id: subscriber_id,
          error: reason
        )

        {:error, reason}
    end
  end

  @impl true
  def get_killmail(killmail_id) do
    Logger.debug(&quot;Fetching specific killmail&quot;, killmail_id: killmail_id)

    case ZkbClient.fetch_killmail(killmail_id) do
      {:ok, killmail} -&gt;
        Logger.debug(&quot;Successfully fetched killmail&quot;, killmail_id: killmail_id)
        killmail

      {:error, reason} -&gt;
        Logger.warning(&quot;Failed to fetch killmail&quot;,
          killmail_id: killmail_id,
          error: reason
        )

        nil
    end
  end

  @impl true
  def get_system_kill_count(system_id) do
    Logger.debug(&quot;Fetching system kill count&quot;, system_id: system_id)

    case Helper.system_get_kill_count(system_id) do
      {:ok, count} when is_integer(count) -&gt;
        Logger.debug(&quot;Retrieved system kill count&quot;,
          system_id: system_id,
          count: count
        )

        count

      {:error, reason} -&gt;
        Logger.warning(&quot;Failed to get system kill count from cache, trying ZKB&quot;,
          system_id: system_id,
          cache_error: reason
        )

        # Fallback to ZKB count
        case ZkbClient.get_system_kill_count(system_id) do
          {:ok, count} -&gt; count
          {:error, _reason} -&gt; 0
        end
    end
  end

  # Private helper functions

  defp filter_kills_by_time(kills, since_hours) do
    cutoff_time = DateTime.utc_now() |&gt; DateTime.add(-since_hours * 3600, :second)

    Enum.filter(kills, fn kill -&gt;
      case get_kill_time(kill) do
        nil -&gt; false
        kill_time -&gt; DateTime.compare(kill_time, cutoff_time) != :lt
      end
    end)
  end

  defp get_kill_time(kill) do
    kill
    |&gt; extract_time_from_killmail()
    |&gt; extract_time_from_kill_time()
    |&gt; extract_time_from_zkb()
  end

  defp extract_time_from_killmail(kill) do
    if is_map(kill) and Map.has_key?(kill, &quot;killmail_time&quot;) do
      {:found, parse_datetime(kill[&quot;killmail_time&quot;])}
    else
      {:continue, kill}
    end
  end

  defp extract_time_from_kill_time({:found, datetime}), do: datetime

  defp extract_time_from_kill_time({:continue, kill}) do
    if is_map(kill) and Map.has_key?(kill, &quot;kill_time&quot;) do
      {:found, parse_datetime(kill[&quot;kill_time&quot;])}
    else
      {:continue, kill}
    end
  end

  defp extract_time_from_zkb({:found, datetime}), do: datetime

  defp extract_time_from_zkb({:continue, kill}) do
    if is_map(kill) and Map.has_key?(kill, &quot;zkb&quot;) do
      extract_time_from_zkb_metadata(kill[&quot;zkb&quot;])
    else
      nil
    end
  end

  defp extract_time_from_zkb_metadata(zkb) when is_map(zkb) do
    if Map.has_key?(zkb, &quot;killmail_time&quot;) do
      parse_datetime(zkb[&quot;killmail_time&quot;])
    else
      nil
    end
  end

  defp extract_time_from_zkb_metadata(_), do: nil

  defp parse_datetime(datetime_string) when is_binary(datetime_string) do
    case DateTime.from_iso8601(datetime_string) do
      {:ok, datetime, _offset} -&gt; datetime
      {:error, _reason} -&gt; nil
    end
  end

  defp parse_datetime(%DateTime{} = datetime), do: datetime
  defp parse_datetime(_), do: nil

  @doc &quot;&quot;&quot;
  Convenience function to broadcast kill updates to subscribers.
  This would typically be called by background processes when new kills are detected.
  &quot;&quot;&quot;
  @spec broadcast_kill_update(integer(), [map()]) :: :ok
  def broadcast_kill_update(system_id, kills) do
    SubscriptionManager.broadcast_kill_update(system_id, kills)
  end

  @doc &quot;&quot;&quot;
  Convenience function to broadcast kill count updates to subscribers.
  This would typically be called by background processes when kill counts change.
  &quot;&quot;&quot;
  @spec broadcast_kill_count_update(integer(), integer()) :: :ok
  def broadcast_kill_count_update(system_id, count) do
    SubscriptionManager.broadcast_kill_count_update(system_id, count)
  end

  @doc &quot;&quot;&quot;
  Lists all active subscriptions (for administrative purposes).
  &quot;&quot;&quot;
  @spec list_subscriptions() :: [Types.subscription()]
  def list_subscriptions do
    SubscriptionManager.list_subscriptions()
  end
end</file><file path="lib/wanderer_kills/core.ex">defmodule WandererKills.Core do
  @moduledoc &quot;&quot;&quot;
  Compatibility module for legacy Core module references.

  This module provides aliases for modules that have been moved to domain-specific
  directories as part of the codebase restructuring. These aliases maintain
  backward compatibility while the codebase is gradually updated.

  ## Migration Guide

  Old modules have been moved to domain-specific locations:

  ### HTTP Modules
  - `WandererKills.Core.Http.Client` → `WandererKills.Http.Client`
  - `WandererKills.Core.Http.ClientProvider` → `WandererKills.Http.ClientProvider`
  - `WandererKills.Core.Http.Util` → **REMOVED** (functionality moved to `WandererKills.Http.Client`)

  ### Processing Modules
  - `WandererKills.Core.BatchProcessor` → `WandererKills.Infrastructure.BatchProcessor`
  - `WandererKills.Core.CSV` → `WandererKills.ShipTypes.CSV`

  ### Cache Modules
  - `WandererKills.Core.CacheUtils` → `WandererKills.Cache.Utils`

  ### Infrastructure Modules
  - `WandererKills.Core.Config` → `WandererKills.Infrastructure.Config`
  - `WandererKills.Core.Retry` → `WandererKills.Infrastructure.Retry`
  - `WandererKills.Core.Clock` → `WandererKills.Infrastructure.Clock`
  - `WandererKills.Core.Constants` → `WandererKills.Infrastructure.Config`
  - `WandererKills.Core.Behaviours` → `WandererKills.Infrastructure.Behaviours`
  - `WandererKills.Core.Error` → `WandererKills.Infrastructure.Error`
  &quot;&quot;&quot;

  # HTTP Module Aliases (consolidated)
  defmodule Http do
    @moduledoc &quot;&quot;&quot;
    Legacy HTTP module aliases for backward compatibility.

    Provides aliases to new HTTP modules in the WandererKills.Http namespace.
    This module maintains backward compatibility while the codebase transitions
    to the new module structure.
    &quot;&quot;&quot;

    defmodule Client do
      @moduledoc false
      # Core HTTP client functions
      defdelegate get(url, headers \\ [], options \\ []), to: WandererKills.Http.Client
      defdelegate get_with_rate_limit(url, opts \\ []), to: WandererKills.Http.Client
      defdelegate handle_status_code(status, resp \\ %{}), to: WandererKills.Http.Client
      defdelegate retriable_error?(error), to: WandererKills.Http.Client

      # Consolidated utility functions (moved from Http.Util)
      defdelegate request_with_telemetry(url, service, opts \\ []), to: WandererKills.Http.Client
      defdelegate parse_json_response(response), to: WandererKills.Http.Client
      defdelegate retry_operation(fun, service, opts \\ []), to: WandererKills.Http.Client

      defdelegate validate_response_structure(data, required_fields),
        to: WandererKills.Http.Client
    end

    defmodule ClientProvider do
      @moduledoc false
      # Enhanced configuration provider
      defdelegate get_client(), to: WandererKills.Http.ClientProvider
      defdelegate default_headers(opts \\ []), to: WandererKills.Http.ClientProvider
      defdelegate eve_api_headers(), to: WandererKills.Http.ClientProvider
      defdelegate default_timeout(), to: WandererKills.Http.ClientProvider
      defdelegate esi_timeout(), to: WandererKills.Http.ClientProvider
      defdelegate build_request_opts(opts \\ []), to: WandererKills.Http.ClientProvider

      # Legacy compatibility
      def get, do: get_client()
    end
  end

  # Processing Module Aliases
  defmodule BatchProcessor do
    @moduledoc false
    defdelegate process_parallel(items, process_fn, opts \\ []),
      to: WandererKills.Infrastructure.BatchProcessor

    defdelegate await_tasks(tasks, opts \\ []), to: WandererKills.Infrastructure.BatchProcessor
  end

  defmodule CSV do
    @moduledoc false
    defdelegate parse_ship_type_csvs(file_paths), to: WandererKills.ShipTypes.CSV
    defdelegate download_csv_files(opts \\ []), to: WandererKills.ShipTypes.CSV
    defdelegate read_file(file_path, parser, opts \\ []), to: WandererKills.ShipTypes.CSV
  end

  # Infrastructure Module Aliases
  defmodule Config do
    @moduledoc false
    defdelegate config(), to: WandererKills.Infrastructure.Config
    defdelegate cache(), to: WandererKills.Infrastructure.Config
    defdelegate retry(), to: WandererKills.Infrastructure.Config
    defdelegate batch(), to: WandererKills.Infrastructure.Config
    defdelegate timeouts(), to: WandererKills.Infrastructure.Config
    defdelegate http_status(), to: WandererKills.Infrastructure.Config
    defdelegate services(), to: WandererKills.Infrastructure.Config
    defdelegate redisq(), to: WandererKills.Infrastructure.Config
    defdelegate parser(), to: WandererKills.Infrastructure.Config
    defdelegate enricher(), to: WandererKills.Infrastructure.Config
    defdelegate killmail_store(), to: WandererKills.Infrastructure.Config
    defdelegate telemetry(), to: WandererKills.Infrastructure.Config
    defdelegate app(), to: WandererKills.Infrastructure.Config

    def start_preloader?, do: WandererKills.Infrastructure.Config.start_preloader?()
  end

  defmodule Retry do
    @moduledoc false
    defdelegate retry_with_backoff(fun, opts \\ []), to: WandererKills.Infrastructure.Retry
    defdelegate retriable_http_error?(reason), to: WandererKills.Infrastructure.Retry
    defdelegate retriable_error?(reason), to: WandererKills.Infrastructure.Retry
    defdelegate retry_http_operation(fun, opts \\ []), to: WandererKills.Infrastructure.Retry
  end

  defmodule Clock do
    @moduledoc false
    defdelegate now(), to: WandererKills.Infrastructure.Clock
    defdelegate system_time(unit), to: WandererKills.Infrastructure.Clock
    defdelegate now_iso8601(), to: WandererKills.Infrastructure.Clock
    defdelegate now_milliseconds(), to: WandererKills.Infrastructure.Clock
    defdelegate hours_ago(hours), to: WandererKills.Infrastructure.Clock
    defdelegate seconds_ago(seconds), to: WandererKills.Infrastructure.Clock
  end

  defmodule Constants do
    @moduledoc false
    defdelegate gen_server_call_timeout(), to: WandererKills.Infrastructure.Config
    defdelegate retry_base_delay(), to: WandererKills.Infrastructure.Config
    defdelegate retry_max_delay(), to: WandererKills.Infrastructure.Config
    defdelegate retry_backoff_factor(), to: WandererKills.Infrastructure.Config
    defdelegate validation(type), to: WandererKills.Infrastructure.Config
  end

  defmodule Error do
    @moduledoc false
    defdelegate esi_error(type, message, retryable \\ false, details \\ nil),
      to: WandererKills.Infrastructure.Error

    defdelegate zkb_error(type, message, retryable \\ false, details \\ nil),
      to: WandererKills.Infrastructure.Error

    defdelegate http_error(type, message, retryable \\ false, details \\ nil),
      to: WandererKills.Infrastructure.Error

    defdelegate validation_error(type, message, details \\ nil),
      to: WandererKills.Infrastructure.Error

    defdelegate parsing_error(type, message, details \\ nil),
      to: WandererKills.Infrastructure.Error

    defdelegate cache_error(type, message, details \\ nil),
      to: WandererKills.Infrastructure.Error

    defdelegate killmail_error(type, message, retryable \\ false, details \\ nil),
      to: WandererKills.Infrastructure.Error

    defdelegate enrichment_error(type, message, retryable \\ false, details \\ nil),
      to: WandererKills.Infrastructure.Error

    defdelegate ship_types_error(type, message, retryable \\ false, details \\ nil),
      to: WandererKills.Infrastructure.Error

    defdelegate not_found_error(message, details \\ nil),
      to: WandererKills.Infrastructure.Error
  end
end</file><file path="lib/wanderer_kills/kill_store.ex">defmodule WandererKills.KillStore do
  @moduledoc &quot;&quot;&quot;
  Simplified ETS-backed killmail storage with pattern-matching query support.

  This module provides a clean API over ETS tables for storing and querying
  killmails. ETS is used here specifically because we need efficient pattern
  matching queries like &quot;give me all kills for system X&quot;.

  The module exposes only the core operations needed for killmail storage
  without the complexity of a GenServer.
  &quot;&quot;&quot;

  require Logger

  # ETS tables for killmail storage
  @killmail_events_table :killmail_events
  @client_offsets_table :client_offsets
  @counters_table :counters
  @killmails_table :killmails
  @system_killmails_table :system_killmails
  @system_kill_counts_table :system_kill_counts
  @system_fetch_timestamps_table :system_fetch_timestamps

  @type kill_id :: integer()
  @type system_id :: integer()
  @type client_id :: term()
  @type event_id :: integer()
  @type kill_data :: map()
  @type client_offsets :: %{system_id() =&gt; event_id()}

  @doc &quot;&quot;&quot;
  Initializes all required ETS tables at application start.

  This should be called from Application.start/2 before starting the supervision tree.
  &quot;&quot;&quot;
  @spec init_tables!() :: :ok
  def init_tables! do
    # Main killmail storage table
    :ets.new(@killmails_table, [:set, :named_table, :public, {:read_concurrency, true}])

    # Event storage for streaming killmails
    :ets.new(@killmail_events_table, [
      :ordered_set,
      :named_table,
      :public,
      {:read_concurrency, true}
    ])

    # Client offset tracking for event streaming
    :ets.new(@client_offsets_table, [:set, :named_table, :public, {:read_concurrency, true}])

    # Counters for event IDs and statistics
    :ets.new(@counters_table, [:set, :named_table, :public, {:read_concurrency, true}])

    # System-specific killmail lists
    :ets.new(@system_killmails_table, [:set, :named_table, :public, {:read_concurrency, true}])

    # System kill counts
    :ets.new(@system_kill_counts_table, [:set, :named_table, :public, {:read_concurrency, true}])

    # System fetch timestamps
    :ets.new(@system_fetch_timestamps_table, [
      :set,
      :named_table,
      :public,
      {:read_concurrency, true}
    ])

    # Initialize counters
    :ets.insert(@counters_table, {:event_counter, 0})
    :ets.insert(@counters_table, {:killmail_seq, 0})

    Logger.info(
      &quot;Initialized KillStore ETS tables: #{inspect([@killmails_table, @killmail_events_table, @client_offsets_table, @counters_table, @system_killmails_table, @system_kill_counts_table, @system_fetch_timestamps_table])}&quot;
    )

    :ok
  end

  # ============================================================================
  # Core Killmail Storage API
  # ============================================================================

  @doc &quot;&quot;&quot;
  Stores a killmail in the store.
  &quot;&quot;&quot;
  @spec put(kill_id(), kill_data()) :: :ok
  def put(kill_id, kill_data) when is_integer(kill_id) and is_map(kill_data) do
    :ets.insert(@killmails_table, {kill_id, kill_data})
    :ok
  end

  @doc &quot;&quot;&quot;
  Stores a killmail in the store with system association.
  &quot;&quot;&quot;
  @spec put(kill_id(), system_id(), kill_data()) :: :ok
  def put(kill_id, system_id, kill_data)
      when is_integer(kill_id) and is_integer(system_id) and is_map(kill_data) do
    # Store the killmail
    :ets.insert(@killmails_table, {kill_id, kill_data})

    # Add to system killmails
    case :ets.lookup(@system_killmails_table, system_id) do
      [] -&gt;
        :ets.insert(@system_killmails_table, {system_id, [kill_id]})

      [{^system_id, existing_ids}] -&gt;
        # Ensure we don&apos;t add duplicates
        if kill_id not in existing_ids do
          :ets.insert(@system_killmails_table, {system_id, [kill_id | existing_ids]})
        end
    end

    :ok
  end

  @doc &quot;&quot;&quot;
  Retrieves a killmail by ID.
  &quot;&quot;&quot;
  @spec get(kill_id()) :: {:ok, kill_data()} | :error
  def get(kill_id) when is_integer(kill_id) do
    case :ets.lookup(@killmails_table, kill_id) do
      [{^kill_id, data}] -&gt; {:ok, data}
      [] -&gt; :error
    end
  end

  @doc &quot;&quot;&quot;
  Lists all killmails for a specific system.
  &quot;&quot;&quot;
  @spec list_by_system(system_id()) :: [kill_data()]
  def list_by_system(system_id) when is_integer(system_id) do
    # Get all killmail IDs for the system from the system_killmails table
    case :ets.lookup(@system_killmails_table, system_id) do
      [{^system_id, killmail_ids}] -&gt;
        # Retrieve the actual killmail data for each ID
        Enum.flat_map(killmail_ids, &amp;get_killmail_data/1)

      [] -&gt;
        []
    end
  end

  defp get_killmail_data(killmail_id) do
    case :ets.lookup(@killmails_table, killmail_id) do
      [{^killmail_id, killmail_data}] -&gt; [killmail_data]
      [] -&gt; []
    end
  end

  @doc &quot;&quot;&quot;
  Deletes a killmail from the store.
  &quot;&quot;&quot;
  @spec delete(kill_id()) :: :ok
  def delete(kill_id) when is_integer(kill_id) do
    :ets.delete(@killmails_table, kill_id)
    :ok
  end

  @doc &quot;&quot;&quot;
  Clears all data for testing (alias method).
  &quot;&quot;&quot;
  @spec clear() :: :ok
  def clear, do: clear_all()

  # ============================================================================
  # Event Streaming API
  # ============================================================================

  @doc &quot;&quot;&quot;
  Inserts a new killmail event for the given system.

  This function:
  1. Generates a sequential event ID
  2. Stores the killmail data
  3. Associates it with the system
  4. Creates an event for streaming
  5. Broadcasts via PubSub (optional)
  &quot;&quot;&quot;
  @spec insert_event(system_id(), kill_data()) :: :ok
  def insert_event(system_id, killmail_map) when is_integer(system_id) and is_map(killmail_map) do
    # Get next event ID
    event_id = get_next_event_id()

    # Store the killmail
    killmail_id = killmail_map[&quot;killmail_id&quot;]
    :ets.insert(@killmails_table, {killmail_id, killmail_map})

    # Add to system killmails
    case :ets.lookup(@system_killmails_table, system_id) do
      [] -&gt;
        :ets.insert(@system_killmails_table, {system_id, [killmail_id]})

      [{^system_id, existing_ids}] -&gt;
        # Ensure we don&apos;t add duplicates
        if killmail_id not in existing_ids do
          :ets.insert(@system_killmails_table, {system_id, [killmail_id | existing_ids]})
        end
    end

    # Insert event for streaming
    :ets.insert(@killmail_events_table, {event_id, system_id, killmail_map})

    # Broadcast via PubSub
    Phoenix.PubSub.broadcast(
      WandererKills.PubSub,
      &quot;system:#{system_id}&quot;,
      {:new_killmail, system_id, killmail_map}
    )

    :ok
  end

  @doc &quot;&quot;&quot;
  Fetches all new events for a client across multiple systems.
  &quot;&quot;&quot;
  @spec fetch_for_client(client_id(), [system_id()]) ::
          {:ok, [{event_id(), system_id(), kill_data()}]}
  def fetch_for_client(client_id, system_ids) when is_list(system_ids) do
    # Get client offsets
    client_offsets = get_client_offsets(client_id)

    # Handle empty system list
    if Enum.empty?(system_ids) do
      {:ok, []}
    else
      # Create conditions for each system
      conditions =
        Enum.map(system_ids, fn sys_id -&gt;
          {:andalso, {:==, :&quot;$2&quot;, sys_id},
           {:&gt;, :&quot;$1&quot;, get_offset_for_system(sys_id, client_offsets)}}
        end)

      # Build the match specification guard
      guard =
        case conditions do
          [single] -&gt; single
          multiple -&gt; List.to_tuple([:orelse | multiple])
        end

      # Create match specification for :ets.select
      match_spec = [
        {
          {:&quot;$1&quot;, :&quot;$2&quot;, :&quot;$3&quot;},
          [guard],
          [{{:&quot;$1&quot;, :&quot;$2&quot;, :&quot;$3&quot;}}]
        }
      ]

      # Get all matching events
      events = :ets.select(@killmail_events_table, match_spec)

      # Sort by event_id ascending
      sorted_events = Enum.sort_by(events, &amp;elem(&amp;1, 0))

      # Update client offsets for each system
      updated_offsets = update_client_offsets(sorted_events, client_offsets)

      # Store updated offsets
      :ets.insert(@client_offsets_table, {client_id, updated_offsets})

      {:ok, sorted_events}
    end
  end

  @doc &quot;&quot;&quot;
  Fetches the next single event for a client across multiple systems.
  &quot;&quot;&quot;
  @spec fetch_one_event(client_id(), [system_id()]) ::
          {:ok, {event_id(), system_id(), kill_data()}} | :empty
  def fetch_one_event(client_id, system_ids) when is_list(system_ids) do
    # Get client offsets
    client_offsets = get_client_offsets(client_id)

    # Handle empty system list
    if Enum.empty?(system_ids) do
      :empty
    else
      # Create conditions for each system
      conditions =
        Enum.map(system_ids, fn sys_id -&gt;
          {:andalso, {:==, :&quot;$2&quot;, sys_id},
           {:&gt;, :&quot;$1&quot;, get_offset_for_system(sys_id, client_offsets)}}
        end)

      # Build the match specification guard
      guard =
        case conditions do
          [single] -&gt; single
          multiple -&gt; List.to_tuple([:orelse | multiple])
        end

      # Create match specification for :ets.select
      match_spec = [
        {
          {:&quot;$1&quot;, :&quot;$2&quot;, :&quot;$3&quot;},
          [guard],
          [{{:&quot;$1&quot;, :&quot;$2&quot;, :&quot;$3&quot;}}]
        }
      ]

      # Use :ets.select to get matching events
      case :ets.select(@killmail_events_table, match_spec, 1) do
        {[{event_id, sys_id, km}], _continuation} -&gt;
          # Update offset for this system only
          updated_offsets = Map.put(client_offsets, sys_id, event_id)
          :ets.insert(@client_offsets_table, {client_id, updated_offsets})

          {:ok, {event_id, sys_id, km}}

        {[], _continuation} -&gt;
          :empty

        :&quot;$end_of_table&quot; -&gt;
          :empty
      end
    end
  end

  # Support single system_id as well as list
  @spec fetch_one_event(client_id(), system_id() | [system_id()]) ::
          {:ok, {event_id(), system_id(), kill_data()}} | :empty
  def fetch_one_event(client_id, system_id) when is_integer(system_id) do
    fetch_one_event(client_id, [system_id])
  end

  @doc &quot;&quot;&quot;
  Fetches events for a client from specific systems since their last offset.
  Legacy function for compatibility with existing tests.
  &quot;&quot;&quot;
  @spec fetch_events(client_id(), [system_id()], non_neg_integer()) :: [kill_data()]
  def fetch_events(client_id, system_ids, limit \\ 100)
      when is_list(system_ids) and is_integer(limit) do
    case fetch_for_client(client_id, system_ids) do
      {:ok, events} -&gt;
        # Return only the killmail data, not the tuple
        events
        |&gt; Enum.take(limit)
        |&gt; Enum.map(&amp;elem(&amp;1, 2))
    end
  end

  @doc &quot;&quot;&quot;
  Gets client offsets for event streaming.
  &quot;&quot;&quot;
  @spec get_client_offsets(client_id()) :: client_offsets()
  def get_client_offsets(client_id) do
    case :ets.lookup(@client_offsets_table, client_id) do
      [{^client_id, offsets}] when is_map(offsets) -&gt; offsets
      [] -&gt; %{}
    end
  end

  @doc &quot;&quot;&quot;
  Updates client offsets for event streaming.
  &quot;&quot;&quot;
  @spec put_client_offsets(client_id(), client_offsets()) :: :ok
  def put_client_offsets(client_id, offsets) when is_map(offsets) do
    :ets.insert(@client_offsets_table, {client_id, offsets})
    :ok
  end

  # ============================================================================
  # System-Specific Operations API (from old GenServer)
  # ============================================================================

  @doc &quot;&quot;&quot;
  Stores a killmail in the store.
  &quot;&quot;&quot;
  @spec store_killmail(kill_data()) :: :ok | {:error, term()}
  def store_killmail(killmail) when is_map(killmail) do
    killmail_id = killmail[&quot;killmail_id&quot;]

    if killmail_id do
      :ets.insert(@killmails_table, {killmail_id, killmail})
      :ok
    else
      {:error, &quot;Killmail missing required killmail_id field&quot;}
    end
  end

  def store_killmail(_invalid) do
    {:error, &quot;Invalid killmail format - must be a map&quot;}
  end

  @doc &quot;&quot;&quot;
  Retrieves a killmail by ID.
  &quot;&quot;&quot;
  @spec get_killmail(kill_id()) :: {:ok, kill_data()} | {:error, term()}
  def get_killmail(killmail_id) when is_integer(killmail_id) do
    case :ets.lookup(@killmails_table, killmail_id) do
      [{^killmail_id, killmail}] -&gt;
        {:ok, killmail}

      [] -&gt;
        {:error, &quot;Killmail not found&quot;}
    end
  end

  @doc &quot;&quot;&quot;
  Deletes a killmail by ID.
  &quot;&quot;&quot;
  @spec delete_killmail(kill_id()) :: :ok
  def delete_killmail(killmail_id) when is_integer(killmail_id) do
    :ets.delete(@killmails_table, killmail_id)
    :ok
  end

  @doc &quot;&quot;&quot;
  Adds a killmail to a system&apos;s list.
  &quot;&quot;&quot;
  @spec add_system_killmail(system_id(), kill_id()) :: :ok
  def add_system_killmail(system_id, killmail_id)
      when is_integer(system_id) and is_integer(killmail_id) do
    case :ets.lookup(@system_killmails_table, system_id) do
      [] -&gt;
        :ets.insert(@system_killmails_table, {system_id, [killmail_id]})

      [{^system_id, existing_ids}] -&gt;
        # Ensure we don&apos;t add duplicates
        if killmail_id not in existing_ids do
          :ets.insert(@system_killmails_table, {system_id, [killmail_id | existing_ids]})
        end
    end

    :ok
  end

  @doc &quot;&quot;&quot;
  Gets all killmails for a system.
  &quot;&quot;&quot;
  @spec get_killmails_for_system(system_id()) :: {:ok, [kill_id()]}
  def get_killmails_for_system(system_id) when is_integer(system_id) do
    case :ets.lookup(@system_killmails_table, system_id) do
      [{^system_id, killmail_ids}] -&gt; {:ok, killmail_ids}
      [] -&gt; {:ok, []}
    end
  end

  @doc &quot;&quot;&quot;
  Removes a killmail from a system&apos;s list.
  &quot;&quot;&quot;
  @spec remove_system_killmail(system_id(), kill_id()) :: :ok
  def remove_system_killmail(system_id, killmail_id)
      when is_integer(system_id) and is_integer(killmail_id) do
    case :ets.lookup(@system_killmails_table, system_id) do
      [] -&gt;
        :ok

      [{^system_id, existing_ids}] -&gt;
        new_ids = Enum.reject(existing_ids, &amp;(&amp;1 == killmail_id))

        if Enum.empty?(new_ids) do
          :ets.delete(@system_killmails_table, system_id)
        else
          :ets.insert(@system_killmails_table, {system_id, new_ids})
        end
    end

    :ok
  end

  @doc &quot;&quot;&quot;
  Increments the kill count for a system.
  &quot;&quot;&quot;
  @spec increment_system_kill_count(system_id()) :: :ok
  def increment_system_kill_count(system_id) when is_integer(system_id) do
    :ets.update_counter(@system_kill_counts_table, system_id, {2, 1}, {system_id, 0})
    :ok
  end

  @doc &quot;&quot;&quot;
  Gets the kill count for a system.
  &quot;&quot;&quot;
  @spec get_system_kill_count(system_id()) :: {:ok, non_neg_integer()}
  def get_system_kill_count(system_id) when is_integer(system_id) do
    case :ets.lookup(@system_kill_counts_table, system_id) do
      [{^system_id, count}] -&gt; {:ok, count}
      [] -&gt; {:ok, 0}
    end
  end

  @doc &quot;&quot;&quot;
  Sets the fetch timestamp for a system.
  &quot;&quot;&quot;
  @spec set_system_fetch_timestamp(system_id(), DateTime.t()) :: :ok
  def set_system_fetch_timestamp(system_id, timestamp) when is_integer(system_id) do
    :ets.insert(@system_fetch_timestamps_table, {system_id, timestamp})
    :ok
  end

  @doc &quot;&quot;&quot;
  Gets the fetch timestamp for a system.
  &quot;&quot;&quot;
  @spec get_system_fetch_timestamp(system_id()) :: {:ok, DateTime.t()} | {:error, term()}
  def get_system_fetch_timestamp(system_id) when is_integer(system_id) do
    case :ets.lookup(@system_fetch_timestamps_table, system_id) do
      [{^system_id, timestamp}] -&gt;
        {:ok, timestamp}

      [] -&gt;
        {:error, &quot;No fetch timestamp found for system&quot;}
    end
  end

  # ============================================================================
  # Cleanup and Utility Functions
  # ============================================================================

  @doc &quot;&quot;&quot;
  Clears all data from all tables (for testing).
  &quot;&quot;&quot;
  @spec clear_all() :: :ok
  def clear_all do
    :ets.delete_all_objects(@killmails_table)
    :ets.delete_all_objects(@killmail_events_table)
    :ets.delete_all_objects(@client_offsets_table)
    :ets.delete_all_objects(@counters_table)
    :ets.delete_all_objects(@system_killmails_table)
    :ets.delete_all_objects(@system_kill_counts_table)
    :ets.delete_all_objects(@system_fetch_timestamps_table)

    # Reinitialize counters
    :ets.insert(@counters_table, {:event_counter, 0})
    :ets.insert(@counters_table, {:killmail_seq, 0})
    :ok
  end

  @doc &quot;&quot;&quot;
  Cleans up ETS tables for testing (alias for clear_all).
  &quot;&quot;&quot;
  @spec cleanup_tables() :: :ok
  def cleanup_tables, do: clear_all()

  # ============================================================================
  # Private Functions
  # ============================================================================

  @spec get_next_event_id() :: event_id()
  defp get_next_event_id do
    :ets.update_counter(@counters_table, :event_counter, 1)
  end

  @spec get_offset_for_system(system_id(), client_offsets()) :: event_id()
  defp get_offset_for_system(system_id, offsets) do
    Map.get(offsets, system_id, 0)
  end

  defp update_client_offsets(sorted_events, client_offsets) do
    Enum.reduce(sorted_events, client_offsets, &amp;update_offset_for_event/2)
  end

  defp update_offset_for_event({event_id, sys_id, _}, acc) do
    current_offset = Map.get(acc, sys_id, 0)
    if event_id &gt; current_offset, do: Map.put(acc, sys_id, event_id), else: acc
  end
end</file><file path="lib/wanderer_kills/redisq.ex">defmodule WandererKills.RedisQ do
  @moduledoc &quot;&quot;&quot;
  Client for interacting with the zKillboard RedisQ API.

  • Idle (no kills):    poll every `:idle_interval_ms`
  • On kill (new):      poll again after `:fast_interval_ms`
  • On kill_older:      poll again after `:idle_interval_ms` (reset backoff)
  • On kill_skipped:    poll again after `:idle_interval_ms` (reset backoff)
  • On error:           exponential backoff up to `:max_backoff_ms`
  &quot;&quot;&quot;

  use GenServer
  require Logger

  alias WandererKills.Killmails.Coordinator
  alias WandererKills.ESI.Client, as: EsiClient
  alias WandererKills.Infrastructure.Clock
  alias WandererKills.Http.Client, as: HttpClient
  alias WandererKills.Infrastructure.Config

  @user_agent &quot;(wanderer-kills@proton.me; +https://github.com/wanderer-industries/wanderer-kills)&quot;

  defmodule State do
    @moduledoc false
    defstruct [:queue_id, :backoff_ms, :stats]
  end

  #
  # Public API
  #

  @doc &quot;&quot;&quot;
  Gets the base URL for RedisQ API calls.
  &quot;&quot;&quot;
  def base_url do
    Config.services().redisq_base_url || &quot;https://zkillredisq.stream/listen.php&quot;
  end

  @doc &quot;&quot;&quot;
  Starts the RedisQ worker as a GenServer.
  &quot;&quot;&quot;
  def start_link(opts \\ []) do
    Logger.info(&quot;[RedisQ] Starting RedisQ worker&quot;)
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  @doc &quot;&quot;&quot;
  Force a synchronous poll &amp; process. Returns one of:
    - `{:ok, :kill_received}`
    - `{:ok, :no_kills}`
    - `{:ok, :kill_older}`
    - `{:ok, :kill_skipped}`
    - `{:error, reason}`
  &quot;&quot;&quot;
  def poll_and_process(opts \\ []) do
    GenServer.call(__MODULE__, {:poll_and_process, opts})
  end

  @doc &quot;&quot;&quot;
  Starts listening to RedisQ killmail stream.
  &quot;&quot;&quot;
  def start_listening do
    url = &quot;#{base_url()}?queueID=wanderer-kills&quot;

    case HttpClient.get(url) do
      {:ok, %{body: body}} -&gt;
        handle_response(body)

      {:error, reason} -&gt;
        Logger.error(&quot;Failed to get RedisQ response: #{inspect(reason)}&quot;)
    end
  end

  #
  # Server Callbacks
  #

  @impl true
  def init(_opts) do
    queue_id = build_queue_id()
    initial_backoff = Config.redisq().initial_backoff_ms

    # Initialize statistics tracking
    stats = %{
      kills_received: 0,
      kills_older: 0,
      kills_skipped: 0,
      legacy_kills: 0,
      errors: 0,
      no_kills_count: 0,
      last_reset: DateTime.utc_now(),
      systems_active: MapSet.new()
    }

    Logger.info(&quot;[RedisQ] Initialized with queue ID: #{queue_id}&quot;)
    # Schedule the very first poll after the idle interval
    schedule_poll(Config.redisq().idle_interval_ms)
    # Schedule the first summary log
    schedule_summary_log()

    state = %State{queue_id: queue_id, backoff_ms: initial_backoff, stats: stats}
    {:ok, state}
  end

  @impl true
  def handle_info(:poll_kills, %State{queue_id: qid, backoff_ms: backoff, stats: stats} = state) do
    Logger.debug(&quot;[RedisQ] Polling RedisQ (queue ID: #{qid})&quot;)
    result = do_poll(qid)

    # Update statistics based on result
    new_stats = update_stats(stats, result)

    {delay_ms, new_backoff} = next_schedule(result, backoff)
    schedule_poll(delay_ms)

    {:noreply, %State{state | backoff_ms: new_backoff, stats: new_stats}}
  end

  @impl true
  def handle_info(:log_summary, %State{stats: stats} = state) do
    log_summary(stats)

    # Reset stats and schedule next summary
    reset_stats = %{
      stats
      | kills_received: 0,
        kills_older: 0,
        kills_skipped: 0,
        legacy_kills: 0,
        errors: 0,
        no_kills_count: 0,
        last_reset: DateTime.utc_now(),
        systems_active: MapSet.new()
    }

    schedule_summary_log()
    {:noreply, %State{state | stats: reset_stats}}
  end

  @impl true
  def handle_info({:track_system, system_id}, %State{stats: stats} = state) do
    new_stats = track_system_activity(stats, system_id)
    {:noreply, %State{state | stats: new_stats}}
  end

  @impl true
  def handle_call({:poll_and_process, _opts}, _from, %State{queue_id: qid} = state) do
    Logger.debug(&quot;[RedisQ] Manual poll requested (queue ID: #{qid})&quot;)
    reply = do_poll(qid)
    {:reply, reply, state}
  end

  @impl true
  def terminate(_reason, _state), do: :ok

  #
  # Private Helpers
  #

  # Schedules the next :poll_kills message in `ms` milliseconds.
  defp schedule_poll(ms) do
    Process.send_after(self(), :poll_kills, ms)
  end

  # Schedules the next :log_summary message in 60 seconds.
  defp schedule_summary_log do
    Process.send_after(self(), :log_summary, 60_000)
  end

  # Updates statistics based on poll result
  defp update_stats(stats, {:ok, :kill_received}) do
    %{stats | kills_received: stats.kills_received + 1}
  end

  defp update_stats(stats, {:ok, :kill_older}) do
    %{stats | kills_older: stats.kills_older + 1}
  end

  defp update_stats(stats, {:ok, :kill_skipped}) do
    %{stats | kills_skipped: stats.kills_skipped + 1}
  end

  defp update_stats(stats, {:ok, :no_kills}) do
    %{stats | no_kills_count: stats.no_kills_count + 1}
  end

  defp update_stats(stats, {:error, _reason}) do
    %{stats | errors: stats.errors + 1}
  end

  # Track active systems
  defp track_system_activity(stats, system_id) when is_integer(system_id) do
    %{stats | systems_active: MapSet.put(stats.systems_active, system_id)}
  end

  defp track_system_activity(stats, _), do: stats

  # Log summary of activity over the past minute
  defp log_summary(stats) do
    duration = DateTime.diff(DateTime.utc_now(), stats.last_reset, :second)

    total_activity =
      stats.kills_received + stats.kills_older + stats.kills_skipped + stats.legacy_kills

    if total_activity &gt; 0 or stats.errors &gt; 0 do
      Logger.info(&quot;📊 REDISQ SUMMARY (#{duration}s):&quot;,
        kills_processed: stats.kills_received,
        kills_older: stats.kills_older,
        kills_skipped: stats.kills_skipped,
        legacy_kills: stats.legacy_kills,
        no_kills_polls: stats.no_kills_count,
        errors: stats.errors,
        active_systems: MapSet.size(stats.systems_active),
        total_polls: total_activity + stats.no_kills_count + stats.errors
      )
    end
  end

  # Perform the actual HTTP GET + parsing and return one of:
  #   - {:ok, :kill_received}
  #   - {:ok, :no_kills}
  #   - {:ok, :kill_older}
  #   - {:ok, :kill_skipped}
  #   - {:error, reason}
  defp do_poll(queue_id) do
    url = &quot;#{base_url()}?queueID=#{queue_id}&amp;ttw=1&quot;
    Logger.debug(&quot;[RedisQ] GET #{url}&quot;)

    headers = [{&quot;user-agent&quot;, @user_agent}]

    case HttpClient.get(url, headers) do
      # No package → no new kills
      {:ok, %{body: %{&quot;package&quot; =&gt; nil}}} -&gt;
        Logger.debug(&quot;[RedisQ] No package received.&quot;)
        {:ok, :no_kills}

      # New‐format: &quot;package&quot; → %{ &quot;killID&quot; =&gt; _, &quot;killmail&quot; =&gt; killmail, &quot;zkb&quot; =&gt; zkb }
      {:ok, %{body: %{&quot;package&quot; =&gt; %{&quot;killID&quot; =&gt; _id, &quot;killmail&quot; =&gt; killmail, &quot;zkb&quot; =&gt; zkb}}}} -&gt;
        process_kill(killmail, zkb)

      # Alternate new‐format (sometimes `killID` is absent, but `killmail`+`zkb` exist)
      {:ok, %{body: %{&quot;package&quot; =&gt; %{&quot;killmail&quot; =&gt; killmail, &quot;zkb&quot; =&gt; zkb}}}} -&gt;
        process_kill(killmail, zkb)

      # Legacy format: { &quot;killID&quot; =&gt; id, &quot;zkb&quot; =&gt; zkb }
      {:ok, %{body: %{&quot;killID&quot; =&gt; id, &quot;zkb&quot; =&gt; zkb}}} -&gt;
        process_legacy_kill(id, zkb)

      # Anything else is unexpected
      {:ok, resp} -&gt;
        Logger.warning(&quot;[RedisQ] Unexpected response shape: #{inspect(resp)}&quot;)
        {:error, :unexpected_format}

      {:error, reason} -&gt;
        Logger.warning(&quot;[RedisQ] HTTP request failed: #{inspect(reason)}&quot;)
        {:error, reason}
    end
  end

  # Handle a &quot;new‐format&quot; killmail JSON blob.  Return one of:
  #   {:ok, :kill_received}   if it&apos;s a brand-new kill
  #   {:ok, :kill_older}      if parser determined it&apos;s older than cutoff
  #   {:ok, :kill_skipped}    if parser determined we already ingested it
  #   {:error, reason}
  #
  # This requires that Coordinator.parse_full_and_store/3 returns exactly
  #   {:ok, :kill_older}   or
  #   {:ok, :kill_skipped}
  # when appropriate—otherwise, we treat any other {:ok, _} as :kill_received.
  defp process_kill(killmail, zkb) do
    cutoff = get_cutoff_time()

    Logger.debug(
      &quot;[RedisQ] Processing new format killmail (cutoff: #{DateTime.to_iso8601(cutoff)})&quot;
    )

    case Coordinator.parse_full_and_store(killmail, %{&quot;zkb&quot; =&gt; zkb}, cutoff) do
      {:ok, :kill_older} -&gt;
        Logger.debug(&quot;[RedisQ] Kill is older than cutoff → skipping.&quot;)
        {:ok, :kill_older}

      {:ok, enriched_killmail} -&gt;
        Logger.debug(&quot;[RedisQ] Successfully parsed &amp; stored new killmail.&quot;)

        # Broadcast kill update via PubSub using the enriched killmail
        broadcast_kill_update_enriched(enriched_killmail)

        {:ok, :kill_received}

      {:error, reason} -&gt;
        Logger.error(&quot;[RedisQ] Failed to parse/store killmail: #{inspect(reason)}&quot;)
        {:error, reason}
    end
  end

  # Handle legacy‐format kill → fetch full payload async and then process.
  # Returns one of:
  #   {:ok, :kill_received}   (if Coordinator.parse... says new)
  #   {:ok, :kill_older}      (if Coordinator returns :kill_older)
  #   {:ok, :kill_skipped}    (if Coordinator returns :kill_skipped)
  #   {:error, reason}
  defp process_legacy_kill(id, zkb) do
    task =
      Task.Supervisor.async(WandererKills.TaskSupervisor, fn -&gt;
        fetch_and_parse_full_kill(id, zkb)
      end)

    task
    |&gt; Task.await(Config.redisq().task_timeout_ms)
    |&gt; case do
      {:ok, :kill_received} -&gt;
        {:ok, :kill_received}

      {:ok, :kill_older} -&gt;
        Logger.debug(&quot;[RedisQ] Legacy kill ID=#{id} is older than cutoff → skipping.&quot;)
        {:ok, :kill_older}

      {:ok, :kill_skipped} -&gt;
        Logger.debug(&quot;[RedisQ] Legacy kill ID=#{id} already ingested → skipping.&quot;)
        {:ok, :kill_skipped}

      {:error, reason} -&gt;
        Logger.error(&quot;[RedisQ] Legacy‐kill #{id} failed: #{inspect(reason)}&quot;)
        {:error, reason}

      other -&gt;
        Logger.error(&quot;[RedisQ] Unexpected task result for legacy kill #{id}: #{inspect(other)}&quot;)
        {:error, :unexpected_task_result}
    end
  end

  # Fetch the full killmail from ESI and then hand off to `process_kill/2`.
  # Returns exactly whatever `process_kill/2` returns.
  defp fetch_and_parse_full_kill(id, zkb) do
    Logger.debug(&quot;[RedisQ] Fetching full killmail for ID=#{id}&quot;)

    case EsiClient.get_killmail_raw(id, zkb[&quot;hash&quot;]) do
      {:ok, full_killmail} -&gt;
        Logger.debug(&quot;[RedisQ] Fetched full killmail ID=#{id}. Now parsing…&quot;)
        process_kill(full_killmail, zkb)

      {:error, reason} -&gt;
        Logger.warning(&quot;[RedisQ] ESI fetch failed for ID=#{id}: #{inspect(reason)}&quot;)
        {:error, reason}
    end
  end

  # Decide the next polling interval and updated backoff based on the last result.
  # Returns: {next_delay_ms, updated_backoff_ms}
  defp next_schedule({:ok, :kill_received}, _old_backoff) do
    fast = Config.redisq().fast_interval_ms
    Logger.debug(&quot;[RedisQ] Kill received → scheduling next poll in #{fast}ms; resetting backoff.&quot;)
    {fast, Config.redisq().initial_backoff_ms}
  end

  defp next_schedule({:ok, :no_kills}, _old_backoff) do
    idle = Config.redisq().idle_interval_ms
    Logger.debug(&quot;[RedisQ] No kills → scheduling next poll in #{idle}ms; resetting backoff.&quot;)
    {idle, Config.redisq().initial_backoff_ms}
  end

  defp next_schedule({:ok, :kill_older}, _old_backoff) do
    idle = Config.redisq().idle_interval_ms

    Logger.debug(
      &quot;[RedisQ] Older kill detected → scheduling next poll in #{idle}ms; resetting backoff.&quot;
    )

    {idle, Config.redisq().initial_backoff_ms}
  end

  defp next_schedule({:ok, :kill_skipped}, _old_backoff) do
    idle = Config.redisq().idle_interval_ms

    Logger.debug(
      &quot;[RedisQ] Skipped kill detected → scheduling next poll in #{idle}ms; resetting backoff.&quot;
    )

    {idle, Config.redisq().initial_backoff_ms}
  end

  defp next_schedule({:error, reason}, old_backoff) do
    factor = Config.redisq().backoff_factor
    max_back = Config.redisq().max_backoff_ms
    next_back = min(old_backoff * factor, max_back)

    Logger.warning(&quot;[RedisQ] Poll error: #{inspect(reason)} → retry in #{next_back}ms (backoff).&quot;)

    {next_back, next_back}
  end

  # Build a unique queue ID: &quot;wanderer_kills_&lt;16_char_string&gt;&quot;
  # Uses a mix of timestamp and random characters for uniqueness
  defp build_queue_id do
    # Generate 16 character random string using alphanumeric characters
    random_chars =
      for _ &lt;- 1..16,
          into: &quot;&quot;,
          do: &lt;&lt;Enum.random(~c&quot;0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz&quot;)&gt;&gt;

    &quot;wanderer_kills_#{random_chars}&quot;
  end

  # Returns cutoff DateTime (e.g. &quot;24 hours ago&quot;)
  defp get_cutoff_time do
    Clock.hours_ago(24)
  end

  defp handle_response(%{&quot;package&quot; =&gt; package}) do
    # Process the killmail package
    Logger.info(&quot;Received killmail package: #{inspect(package)}&quot;)
  end

  defp handle_response(_) do
    # No package in response, continue listening
    start_listening()
  end

  # Broadcast kill update to PubSub subscribers using enriched killmail
  defp broadcast_kill_update_enriched(enriched_killmail) do
    system_id = Map.get(enriched_killmail, &quot;solar_system_id&quot;)
    killmail_id = Map.get(enriched_killmail, &quot;killmail_id&quot;)

    if system_id do
      # Track system activity for statistics
      send(self(), {:track_system, system_id})

      # Broadcast detailed kill update
      WandererKills.SubscriptionManager.broadcast_kill_update(system_id, [enriched_killmail])

      # Also broadcast kill count update (increment by 1)
      WandererKills.SubscriptionManager.broadcast_kill_count_update(system_id, 1)
    else
      Logger.warning(&quot;[RedisQ] Cannot broadcast kill update - missing solar_system_id&quot;,
        killmail_id: killmail_id
      )
    end
  end
end</file><file path="lib/wanderer_kills/subscription_manager.ex">defmodule WandererKills.SubscriptionManager do
  @moduledoc &quot;&quot;&quot;
  GenServer that manages kill subscriptions and notifications.

  This module handles:
  - Tracking active subscriptions
  - Managing subscriber notifications via webhooks and PubSub
  - Broadcasting kill updates to subscribers
  &quot;&quot;&quot;

  use GenServer
  require Logger
  alias WandererKills.Types

  @type subscription_id :: String.t()
  @type subscriber_id :: String.t()
  @type system_id :: integer()

  # Client API

  @doc &quot;&quot;&quot;
  Starts the subscription manager.
  &quot;&quot;&quot;
  @spec start_link(keyword()) :: GenServer.on_start()
  def start_link(opts \\ []) do
    name = Keyword.get(opts, :name, __MODULE__)
    GenServer.start_link(__MODULE__, opts, name: name)
  end

  @doc &quot;&quot;&quot;
  Subscribes to kill updates for specified systems.
  &quot;&quot;&quot;
  @spec subscribe(subscriber_id(), [system_id()], String.t() | nil) ::
          {:ok, subscription_id()} | {:error, term()}
  def subscribe(subscriber_id, system_ids, callback_url \\ nil) do
    GenServer.call(__MODULE__, {:subscribe, subscriber_id, system_ids, callback_url})
  end

  @doc &quot;&quot;&quot;
  Unsubscribes from all kill updates for a subscriber.
  &quot;&quot;&quot;
  @spec unsubscribe(subscriber_id()) :: :ok | {:error, term()}
  def unsubscribe(subscriber_id) do
    GenServer.call(__MODULE__, {:unsubscribe, subscriber_id})
  end

  @doc &quot;&quot;&quot;
  Lists all active subscriptions.
  &quot;&quot;&quot;
  @spec list_subscriptions() :: [Types.subscription()]
  def list_subscriptions do
    GenServer.call(__MODULE__, :list_subscriptions)
  end

  @doc &quot;&quot;&quot;
  Broadcasts a kill update to all relevant subscribers.
  &quot;&quot;&quot;
  @spec broadcast_kill_update(system_id(), [Types.kill()]) :: :ok
  def broadcast_kill_update(system_id, kills) do
    GenServer.cast(__MODULE__, {:broadcast_kill_update, system_id, kills})
  end

  @doc &quot;&quot;&quot;
  Broadcasts a kill count update to all relevant subscribers.
  &quot;&quot;&quot;
  @spec broadcast_kill_count_update(system_id(), integer()) :: :ok
  def broadcast_kill_count_update(system_id, count) do
    GenServer.cast(__MODULE__, {:broadcast_kill_count_update, system_id, count})
  end

  # Server callbacks

  @impl true
  def init(opts) do
    pubsub_name = Keyword.get(opts, :pubsub_name, WandererKills.PubSub)

    state = %{
      subscriptions: %{},
      pubsub_name: pubsub_name
    }

    Logger.info(&quot;SubscriptionManager started&quot;, pubsub_name: pubsub_name)
    {:ok, state}
  end

  @impl true
  def handle_call({:subscribe, subscriber_id, system_ids, callback_url}, _from, state) do
    case validate_subscription_params(subscriber_id, system_ids, callback_url) do
      :ok -&gt;
        subscription_id = generate_subscription_id(subscriber_id)

        subscription = %{
          subscriber_id: subscriber_id,
          system_ids: system_ids,
          callback_url: callback_url,
          created_at: DateTime.utc_now()
        }

        new_subscriptions = Map.put(state.subscriptions, subscription_id, subscription)
        new_state = %{state | subscriptions: new_subscriptions}

        Logger.info(&quot;Subscription created&quot;,
          subscriber_id: subscriber_id,
          subscription_id: subscription_id,
          system_ids: system_ids,
          has_callback: !is_nil(callback_url)
        )

        # Preload and send recent kills for the subscribed systems
        Task.start(fn -&gt;
          try do
            preload_kills_for_new_subscriber(subscription, system_ids)
          rescue
            error -&gt;
              Logger.error(&quot;Preload task crashed&quot;,
                subscriber_id: subscriber_id,
                error: inspect(error),
                stacktrace: Exception.format_stacktrace(__STACKTRACE__)
              )
          catch
            :exit, reason -&gt;
              Logger.error(&quot;Preload task exited&quot;,
                subscriber_id: subscriber_id,
                reason: inspect(reason)
              )
          end
        end)

        {:reply, {:ok, subscription_id}, new_state}

      {:error, reason} -&gt;
        Logger.warning(&quot;Subscription creation failed&quot;,
          subscriber_id: subscriber_id,
          reason: reason
        )

        {:reply, {:error, reason}, state}
    end
  end

  @impl true
  def handle_call({:unsubscribe, subscriber_id}, _from, state) do
    {removed_subscriptions, remaining_subscriptions} =
      Enum.split_with(state.subscriptions, fn {_id, sub} -&gt;
        sub.subscriber_id == subscriber_id
      end)

    if Enum.empty?(removed_subscriptions) do
      Logger.warning(&quot;Unsubscribe failed: subscriber not found&quot;, subscriber_id: subscriber_id)
      {:reply, {:error, :not_found}, state}
    else
      new_state = %{state | subscriptions: Map.new(remaining_subscriptions)}
      removed_count = length(removed_subscriptions)

      Logger.info(&quot;Subscription removed&quot;,
        subscriber_id: subscriber_id,
        removed_count: removed_count
      )

      {:reply, :ok, new_state}
    end
  end

  @impl true
  def handle_call(:list_subscriptions, _from, state) do
    subscriptions =
      state.subscriptions
      |&gt; Enum.map(fn {_id, subscription} -&gt; subscription end)

    {:reply, subscriptions, state}
  end

  @impl true
  def handle_cast({:broadcast_kill_update, system_id, kills}, state) do
    # Find all subscribers interested in this system
    interested_subscriptions =
      state.subscriptions
      |&gt; Enum.filter(fn {_id, sub} -&gt; system_id in sub.system_ids end)

    if Enum.empty?(interested_subscriptions) do
      log_no_subscribers_if_kills_exist(kills, system_id)
    else
      log_broadcast_details(kills, system_id, interested_subscriptions)

      # Broadcast via PubSub
      broadcast_pubsub_update(state.pubsub_name, system_id, kills, :detailed_kill_update)

      # Send webhook notifications
      Task.start(fn -&gt;
        send_webhook_notifications(interested_subscriptions, system_id, kills, :kill_update)
      end)
    end

    {:noreply, state}
  end

  @impl true
  def handle_cast({:broadcast_kill_count_update, system_id, count}, state) do
    # Find all subscribers interested in this system
    interested_subscriptions =
      state.subscriptions
      |&gt; Enum.filter(fn {_id, sub} -&gt; system_id in sub.system_ids end)

    if not Enum.empty?(interested_subscriptions) do
      # Broadcast via PubSub
      broadcast_pubsub_count_update(state.pubsub_name, system_id, count)

      # Send webhook notifications
      Task.start(fn -&gt;
        send_webhook_count_notifications(interested_subscriptions, system_id, count)
      end)

      Logger.debug(&quot;Kill count update broadcasted&quot;,
        system_id: system_id,
        count: count,
        subscriber_count: length(interested_subscriptions)
      )
    end

    {:noreply, state}
  end

  # Private helper functions

  defp validate_subscription_params(subscriber_id, system_ids, callback_url) do
    cond do
      not is_binary(subscriber_id) or String.trim(subscriber_id) == &quot;&quot; -&gt;
        {:error, :invalid_subscriber_id}

      not is_list(system_ids) or Enum.empty?(system_ids) -&gt;
        {:error, :invalid_system_ids}

      not Enum.all?(system_ids, &amp;is_integer/1) -&gt;
        {:error, :invalid_system_ids}

      not is_nil(callback_url) and not is_binary(callback_url) -&gt;
        {:error, :invalid_callback_url}

      true -&gt;
        :ok
    end
  end

  defp generate_subscription_id(subscriber_id) do
    timestamp = System.system_time(:microsecond)

    :crypto.hash(:sha256, &quot;#{subscriber_id}-#{timestamp}&quot;)
    |&gt; Base.encode16(case: :lower)
    |&gt; String.slice(0, 16)
  end

  defp broadcast_pubsub_update(pubsub_name, system_id, kills, type) do
    timestamp = DateTime.utc_now()

    # Global kill updates
    Phoenix.PubSub.broadcast(pubsub_name, &quot;zkb:detailed_kills:updated&quot;, %{
      type: type,
      solar_system_id: system_id,
      kills: kills,
      timestamp: timestamp
    })

    # System-specific updates
    Phoenix.PubSub.broadcast(pubsub_name, &quot;zkb:system:#{system_id}&quot;, %{
      type: type,
      solar_system_id: system_id,
      kills: kills,
      timestamp: timestamp
    })

    Phoenix.PubSub.broadcast(pubsub_name, &quot;zkb:system:#{system_id}:detailed&quot;, %{
      type: type,
      solar_system_id: system_id,
      kills: kills,
      timestamp: timestamp
    })
  end

  defp broadcast_pubsub_count_update(pubsub_name, system_id, count) do
    timestamp = DateTime.utc_now()

    # Global kill count updates
    Phoenix.PubSub.broadcast(pubsub_name, &quot;zkb:kills:updated&quot;, %{
      type: :kill_count_update,
      solar_system_id: system_id,
      kills: count,
      timestamp: timestamp
    })

    # System-specific updates
    Phoenix.PubSub.broadcast(pubsub_name, &quot;zkb:system:#{system_id}&quot;, %{
      type: :kill_count_update,
      solar_system_id: system_id,
      kills: count,
      timestamp: timestamp
    })
  end

  defp send_webhook_notifications(subscriptions, system_id, kills, type) do
    subscriptions
    |&gt; Enum.filter(fn {_id, sub} -&gt; not is_nil(sub.callback_url) end)
    |&gt; Enum.each(fn {_id, sub} -&gt;
      send_webhook_notification(sub, system_id, kills, type)
    end)
  end

  defp send_webhook_count_notifications(subscriptions, system_id, count) do
    subscriptions
    |&gt; Enum.filter(fn {_id, sub} -&gt; not is_nil(sub.callback_url) end)
    |&gt; Enum.each(fn {_id, sub} -&gt;
      send_webhook_count_notification(sub, system_id, count)
    end)
  end

  defp send_webhook_notification(subscription, system_id, kills, type) do
    payload = %{
      type: type,
      data: %{
        solar_system_id: system_id,
        kills: kills,
        timestamp: DateTime.utc_now() |&gt; DateTime.to_iso8601()
      }
    }

    payload_size = byte_size(Jason.encode!(payload))

    Logger.info(&quot;📡 WEBHOOK: Sending notification&quot;,
      subscriber_id: subscription.subscriber_id,
      system_id: system_id,
      type: type,
      kill_count: length(kills),
      payload_size_bytes: payload_size,
      callback_url: String.slice(subscription.callback_url, 0, 50) &lt;&gt; &quot;...&quot;
    )

    case Req.post(subscription.callback_url,
           json: payload,
           headers: [{&quot;Content-Type&quot;, &quot;application/json&quot;}],
           receive_timeout: 10_000
         ) do
      {:ok, %Req.Response{status: status}} when status in 200..299 -&gt;
        Logger.info(&quot;📡 WEBHOOK SUCCESS&quot;,
          subscriber_id: subscription.subscriber_id,
          status: status,
          kill_count: length(kills),
          type: type
        )

      {:ok, %Req.Response{status: status}} -&gt;
        Logger.warning(&quot;📡 WEBHOOK FAILED&quot;,
          subscriber_id: subscription.subscriber_id,
          status: status,
          kill_count: length(kills),
          type: type
        )

      {:error, reason} -&gt;
        Logger.error(&quot;📡 WEBHOOK ERROR&quot;,
          subscriber_id: subscription.subscriber_id,
          error: inspect(reason),
          kill_count: length(kills),
          type: type
        )
    end
  end

  defp send_webhook_count_notification(subscription, system_id, count) do
    payload = %{
      type: &quot;kill_count_update&quot;,
      data: %{
        solar_system_id: system_id,
        count: count,
        timestamp: DateTime.utc_now() |&gt; DateTime.to_iso8601()
      }
    }

    case Req.post(subscription.callback_url,
           json: payload,
           headers: [{&quot;Content-Type&quot;, &quot;application/json&quot;}],
           receive_timeout: 10_000
         ) do
      {:ok, %Req.Response{status: status}} when status in 200..299 -&gt;
        Logger.debug(&quot;Webhook count notification sent successfully&quot;,
          subscriber_id: subscription.subscriber_id,
          callback_url: subscription.callback_url,
          status: status
        )

      {:ok, %Req.Response{status: status}} -&gt;
        Logger.warning(&quot;Webhook count notification failed&quot;,
          subscriber_id: subscription.subscriber_id,
          callback_url: subscription.callback_url,
          status: status
        )

      {:error, reason} -&gt;
        Logger.error(&quot;Webhook count notification error&quot;,
          subscriber_id: subscription.subscriber_id,
          callback_url: subscription.callback_url,
          error: inspect(reason)
        )
    end
  end

  # Preload and send recent kills for a new subscriber
  defp preload_kills_for_new_subscriber(subscription, system_ids) do
    Logger.info(&quot;Preloading kills for new subscriber&quot;,
      subscriber_id: subscription.subscriber_id,
      system_count: length(system_ids)
    )

    # Fetch recent kills for each system (limit to 24 hours and 5 kills per system to avoid too many ESI calls)
    limit_per_system = 5
    since_hours = 24

    total_kills_sent =
      system_ids
      |&gt; Enum.map(fn system_id -&gt;
        preload_system_kills(subscription, system_id, since_hours, limit_per_system)
      end)
      |&gt; Enum.sum()

    Logger.info(&quot;Preload completed for new subscriber&quot;,
      subscriber_id: subscription.subscriber_id,
      total_systems: length(system_ids),
      total_kills_sent: total_kills_sent
    )
  end

  # Preload kills for a specific system and send to subscriber
  defp preload_system_kills(subscription, system_id, since_hours, limit) do
    Logger.info(&quot;🔍 PRELOAD DEBUG: Starting preload for system&quot;,
      subscriber_id: subscription.subscriber_id,
      system_id: system_id,
      since_hours: since_hours,
      limit: limit
    )

    kills = get_kills_for_preload(system_id, limit, since_hours)
    send_preload_kills_to_subscriber(subscription, system_id, kills)
  end

  # Helper function to get kills for preload (cached or fresh)
  defp get_kills_for_preload(system_id, limit, since_hours) do
    case get_cached_enriched_kills(system_id, 25) do
      enriched_kills when enriched_kills != [] -&gt;
        Logger.info(&quot;🔍 PRELOAD DEBUG: Using cached enriched kills&quot;,
          system_id: system_id,
          cached_count: length(enriched_kills)
        )

        enriched_kills

      [] -&gt;
        Logger.info(&quot;🔍 PRELOAD DEBUG: No cached kills, fetching from ZKB&quot;,
          system_id: system_id,
          target_limit: limit
        )

        fetch_and_enrich_kills(system_id, limit, since_hours)
    end
  end

  # Helper function to send preload kills to subscriber
  defp send_preload_kills_to_subscriber(subscription, system_id, kills) do
    if length(kills) &gt; 0 do
      log_preload_summary(subscription, system_id, kills)
      broadcast_preload_kills(subscription, system_id, kills)
      length(kills)
    else
      Logger.info(&quot;🔍 PRELOAD DEBUG: No kills found for system&quot;,
        subscriber_id: subscription.subscriber_id,
        system_id: system_id
      )

      0
    end
  end

  # Helper function to log broadcast details (fixing nesting issue)
  defp log_broadcast_details(kills, system_id, interested_subscriptions) do
    if length(kills) &gt; 0 do
      killmail_ids = Enum.map(kills, &amp; &amp;1[&quot;killmail_id&quot;])
      kill_times = extract_kill_times(kills)
      enriched_count = count_enriched_kills(kills)
      subscriber_ids = Enum.map(interested_subscriptions, fn {_id, sub} -&gt; sub.subscriber_id end)

      Logger.info(&quot;🚀 REAL-TIME BROADCAST: Sending kills to subscribers&quot;,
        system_id: system_id,
        kill_count: length(kills),
        killmail_ids: killmail_ids,
        enriched_count: enriched_count,
        unenriched_count: length(kills) - enriched_count,
        kill_time_range: &quot;#{List.first(kill_times)} to #{List.last(kill_times)}&quot;,
        subscriber_count: length(interested_subscriptions),
        subscriber_ids: subscriber_ids,
        via_pubsub: true,
        via_webhook: true
      )

      log_sample_kill_data(kills)
    end
  end

  # Helper function to log sample kill data
  defp log_sample_kill_data(kills) do
    sample_kill = List.first(kills)

    Logger.info(&quot;🚀 REAL-TIME SAMPLE KILL DATA&quot;,
      killmail_id: sample_kill[&quot;killmail_id&quot;],
      victim_character: sample_kill[&quot;victim&quot;][&quot;character_name&quot;],
      victim_corp: sample_kill[&quot;victim&quot;][&quot;corporation_name&quot;],
      attacker_count: length(sample_kill[&quot;attackers&quot;] || []),
      solar_system_id: sample_kill[&quot;solar_system_id&quot;],
      total_value: sample_kill[&quot;total_value&quot;],
      npc_kill: sample_kill[&quot;npc&quot;]
    )
  end

  # Helper function to extract kill times
  defp extract_kill_times(kills) do
    Enum.map(kills, fn kill -&gt;
      kill[&quot;kill_time&quot;] || kill[&quot;killmail_time&quot;] || &quot;unknown&quot;
    end)
  end

  # Helper function to count enriched kills
  defp count_enriched_kills(kills) do
    Enum.count(kills, fn kill -&gt;
      kill[&quot;victim&quot;][&quot;character_name&quot;] != nil or
        kill[&quot;attackers&quot;] |&gt; Enum.any?(&amp;(&amp;1[&quot;character_name&quot;] != nil))
    end)
  end

  # Helper function to log when no subscribers exist
  defp log_no_subscribers_if_kills_exist(kills, system_id) do
    if length(kills) &gt; 0 do
      Logger.debug(&quot;No subscribers for kill update&quot;,
        system_id: system_id,
        kill_count: length(kills)
      )
    end
  end

  # Helper function to log preload summary
  defp log_preload_summary(subscription, system_id, kills) do
    killmail_ids = Enum.map(kills, &amp; &amp;1[&quot;killmail_id&quot;])
    kill_times = extract_kill_times(kills)
    enriched_count = count_enriched_kills(kills)

    Logger.info(&quot;📦 PRELOAD SUMMARY: Sending kills to subscriber&quot;,
      subscriber_id: subscription.subscriber_id,
      system_id: system_id,
      kill_count: length(kills),
      killmail_ids: killmail_ids,
      enriched_count: enriched_count,
      unenriched_count: length(kills) - enriched_count,
      kill_time_range: &quot;#{List.first(kill_times)} to #{List.last(kill_times)}&quot;,
      via_pubsub: true,
      via_webhook: subscription.callback_url != nil
    )

    log_preload_sample_kill(kills)
  end

  # Helper function to log preload sample kill
  defp log_preload_sample_kill(kills) do
    sample_kill = List.first(kills)

    Logger.info(&quot;📦 PRELOAD SAMPLE KILL DATA&quot;,
      killmail_id: sample_kill[&quot;killmail_id&quot;],
      victim_character: sample_kill[&quot;victim&quot;][&quot;character_name&quot;],
      victim_corp: sample_kill[&quot;victim&quot;][&quot;corporation_name&quot;],
      attacker_count: length(sample_kill[&quot;attackers&quot;] || []),
      solar_system_id: sample_kill[&quot;solar_system_id&quot;],
      total_value: sample_kill[&quot;total_value&quot;]
    )
  end

  # Helper function to broadcast preload kills
  defp broadcast_preload_kills(subscription, system_id, kills) do
    # Send via PubSub
    broadcast_pubsub_update(WandererKills.PubSub, system_id, kills, :preload_kill_update)

    # Send via webhook if configured
    if subscription.callback_url do
      send_webhook_notification(subscription, system_id, kills, :preload_kill_update)
    end
  end

  # Helper function to get cached enriched killmails for a system
  defp get_cached_enriched_kills(system_id, limit) do
    case WandererKills.Cache.Helper.system_get_killmails(system_id) do
      {:ok, killmail_ids} when is_list(killmail_ids) -&gt;
        fetch_enriched_killmails(killmail_ids, limit)

      {:error, _reason} -&gt;
        []
    end
  end

  # Helper function to fetch enriched killmails from cache
  defp fetch_enriched_killmails(killmail_ids, limit) do
    killmail_ids
    |&gt; Enum.take(limit)
    |&gt; Enum.map(&amp;get_single_enriched_killmail/1)
    |&gt; Enum.filter(&amp;(&amp;1 != nil))
  end

  # Helper function to fetch a single enriched killmail from cache
  defp get_single_enriched_killmail(killmail_id) do
    case WandererKills.Cache.Helper.killmail_get(killmail_id) do
      {:ok, enriched_killmail} -&gt; enriched_killmail
      {:error, _reason} -&gt; nil
    end
  end

  # Helper function to fetch and enrich kills from ZKB
  defp fetch_and_enrich_kills(system_id, limit, since_hours) do
    Logger.debug(&quot;No cached kills available, fetching fresh kills for preload&quot;,
      system_id: system_id,
      limit: limit,
      since_hours: since_hours
    )

    try do
      # Fetch recent kills from ZKB API
      case fetch_zkb_kills_for_system(system_id, limit, since_hours) do
        {:ok, zkb_kills} when is_list(zkb_kills) and length(zkb_kills) &gt; 0 -&gt;
          Logger.info(&quot;🔍 PRELOAD DEBUG: Fetched kills from ZKB API&quot;,
            system_id: system_id,
            kill_count: length(zkb_kills)
          )

          # Parse and enrich the kills
          enrich_zkb_kills_for_preload(zkb_kills)

        {:ok, []} -&gt;
          Logger.info(&quot;🔍 PRELOAD DEBUG: ZKB API returned empty list&quot;, system_id: system_id)
          []

        {:error, reason} -&gt;
          Logger.info(&quot;🔍 PRELOAD DEBUG: ZKB API error&quot;,
            system_id: system_id,
            error: inspect(reason)
          )

          []
      end
    rescue
      error -&gt;
        Logger.error(&quot;Error during preload kill fetch&quot;,
          system_id: system_id,
          error: inspect(error)
        )

        []
    end
  end

  # Fetch recent kills from ZKB API for a specific system
  defp fetch_zkb_kills_for_system(system_id, limit, _since_hours) do
    # Build ZKB API URL for system kills
    zkb_url = &quot;https://zkillboard.com/api/systemID/#{system_id}/&quot;

    Logger.info(&quot;🔍 PRELOAD DEBUG: Fetching from ZKB API&quot;,
      system_id: system_id,
      url: zkb_url,
      limit: limit
    )

    case Req.get(zkb_url,
           headers: [{&quot;User-Agent&quot;, &quot;wanderer-kills/1.0&quot;}],
           receive_timeout: 30_000
         ) do
      {:ok, %Req.Response{status: 200, body: kills}} when is_list(kills) -&gt;
        # Take first N kills without time filtering (ZKB data has no timestamps)
        # We&apos;ll filter by time after fetching full killmail data from ESI
        limited_kills = Enum.take(kills, limit)

        Logger.info(&quot;🔍 PRELOAD DEBUG: ZKB API response&quot;,
          system_id: system_id,
          total_kills_available: length(kills),
          kills_taken: length(limited_kills)
        )

        {:ok, limited_kills}

      {:ok, %Req.Response{status: status}} -&gt;
        {:error, &quot;ZKB API returned status #{status}&quot;}

      {:error, reason} -&gt;
        {:error, reason}
    end
  end

  # Check if a kill is recent enough for preload
  defp kill_recent?(kill, cutoff_time) do
    case WandererKills.Infrastructure.Clock.get_killmail_time(kill) do
      {:ok, kill_time} -&gt;
        is_recent = DateTime.compare(kill_time, cutoff_time) != :lt

        if is_recent do
          true
        else
          Logger.info(&quot;🔍 PRELOAD DEBUG: Kill filtered as too old&quot;,
            killmail_id: Map.get(kill, &quot;killmail_id&quot;),
            kill_time: DateTime.to_iso8601(kill_time),
            cutoff_time: DateTime.to_iso8601(cutoff_time)
          )

          false
        end

      {:error, reason} -&gt;
        Logger.info(&quot;🔍 PRELOAD DEBUG: Kill filtered due to missing time data&quot;,
          killmail_id: Map.get(kill, &quot;killmail_id&quot;),
          error: reason,
          kill_keys: Map.keys(kill),
          zkb_keys: Map.keys(Map.get(kill, &quot;zkb&quot;, %{}))
        )

        false
    end
  end

  # Enrich ZKB kills for preload (with error handling)
  defp enrich_zkb_kills_for_preload(zkb_kills) do
    Logger.info(&quot;🔍 PRELOAD DEBUG: Processing ZKB kills for enrichment&quot;,
      kill_count: length(zkb_kills)
    )

    # Use a reasonable cutoff (24 hours ago) for preload
    cutoff = DateTime.utc_now() |&gt; DateTime.add(-24 * 3600, :second)

    zkb_kills
    |&gt; Enum.map(&amp;enrich_single_zkb_kill_for_preload(&amp;1, cutoff))
    |&gt; Enum.filter(&amp;(&amp;1 != nil))
  end

  # Enrich a single ZKB kill for preload with graceful error handling
  defp enrich_single_zkb_kill_for_preload(zkb_kill, cutoff) do
    killmail_id = Map.get(zkb_kill, &quot;killmail_id&quot;)
    zkb_hash = get_in(zkb_kill, [&quot;zkb&quot;, &quot;hash&quot;])

    try do
      Logger.info(&quot;🔍 PRELOAD DEBUG: Processing kill #{killmail_id}&quot;)

      # First fetch the full killmail from ESI to get timestamp
      case WandererKills.ESI.Client.get_killmail_raw(killmail_id, zkb_hash) do
        {:ok, full_killmail} -&gt;
          # Use existing time filtering logic
          if kill_recent?(full_killmail, cutoff) do
            Logger.info(&quot;🔍 PRELOAD DEBUG: Kill #{killmail_id} is recent, enriching&quot;)

            # Parse and enrich the kill
            case WandererKills.Killmails.Coordinator.parse_full_and_store(
                   full_killmail,
                   zkb_kill,
                   cutoff
                 ) do
              {:ok, enriched_killmail} when is_map(enriched_killmail) -&gt;
                enriched_killmail

              {:ok, :kill_older} -&gt;
                Logger.info(&quot;🔍 PRELOAD DEBUG: Kill #{killmail_id} filtered as too old&quot;)
                nil

              {:error, reason} -&gt;
                Logger.info(
                  &quot;🔍 PRELOAD DEBUG: Failed to enrich kill #{killmail_id}: #{inspect(reason)}&quot;
                )

                nil
            end
          else
            Logger.info(&quot;🔍 PRELOAD DEBUG: Kill #{killmail_id} filtered as not recent&quot;)
            nil
          end

        {:error, reason} -&gt;
          Logger.info(
            &quot;🔍 PRELOAD DEBUG: Failed to fetch full killmail #{killmail_id} from ESI: #{inspect(reason)}&quot;
          )

          nil
      end
    rescue
      error -&gt;
        Logger.info(&quot;🔍 PRELOAD DEBUG: Error processing kill #{killmail_id}: #{inspect(error)}&quot;)
        nil
    end
  end
end</file><file path="lib/wanderer_kills/types.ex">defmodule WandererKills.Types do
  @moduledoc &quot;&quot;&quot;
  Type definitions for WandererKills service data structures.

  This module defines the standard data structures used throughout the WandererKills service,
  ensuring consistency between API responses, cache storage, and client interfaces.
  &quot;&quot;&quot;

  @typedoc &quot;&quot;&quot;
  A complete killmail record with victim, attackers, and metadata.

  This represents the canonical killmail format used throughout the service.
  &quot;&quot;&quot;
  @type kill :: %{
          killmail_id: integer(),
          kill_time: DateTime.t(),
          solar_system_id: integer(),
          victim: victim(),
          attackers: [attacker()],
          zkb: zkb_metadata()
        }

  @typedoc &quot;&quot;&quot;
  Victim information in a killmail.
  &quot;&quot;&quot;
  @type victim :: %{
          character_id: integer() | nil,
          corporation_id: integer(),
          alliance_id: integer() | nil,
          ship_type_id: integer(),
          damage_taken: integer()
        }

  @typedoc &quot;&quot;&quot;
  Attacker information in a killmail.
  &quot;&quot;&quot;
  @type attacker :: %{
          character_id: integer() | nil,
          corporation_id: integer() | nil,
          alliance_id: integer() | nil,
          ship_type_id: integer() | nil,
          weapon_type_id: integer() | nil,
          damage_done: integer(),
          final_blow: boolean()
        }

  @typedoc &quot;&quot;&quot;
  zKillboard-specific metadata for a killmail.
  &quot;&quot;&quot;
  @type zkb_metadata :: %{
          location_id: integer() | nil,
          hash: String.t(),
          fitted_value: float(),
          total_value: float(),
          points: integer(),
          npc: boolean(),
          solo: boolean(),
          awox: boolean()
        }

  @typedoc &quot;&quot;&quot;
  Standard error response structure.
  &quot;&quot;&quot;
  @type error_response :: %{
          error: String.t(),
          code: String.t(),
          details: map() | nil,
          timestamp: DateTime.t()
        }

  @typedoc &quot;&quot;&quot;
  Standard success response wrapper.
  &quot;&quot;&quot;
  @type success_response(data_type) :: %{
          data: data_type,
          timestamp: DateTime.t()
        }

  @typedoc &quot;&quot;&quot;
  Subscription information.
  &quot;&quot;&quot;
  @type subscription :: %{
          subscriber_id: String.t(),
          system_ids: [integer()],
          callback_url: String.t() | nil,
          created_at: DateTime.t()
        }

  @typedoc &quot;&quot;&quot;
  Kill count information for a system.
  &quot;&quot;&quot;
  @type kill_count :: %{
          system_id: integer(),
          count: integer(),
          timestamp: DateTime.t()
        }

  @typedoc &quot;&quot;&quot;
  Multi-system kill data response.
  &quot;&quot;&quot;
  @type systems_kills :: %{
          systems_kills: %{integer() =&gt; [kill()]},
          timestamp: DateTime.t()
        }

  @doc &quot;&quot;&quot;
  Creates a standard success response envelope.
  &quot;&quot;&quot;
  @spec success_response(any()) :: success_response(any())
  def success_response(data) do
    %{
      data: data,
      timestamp: DateTime.utc_now()
    }
  end

  @doc &quot;&quot;&quot;
  Creates a standard error response envelope.
  &quot;&quot;&quot;
  @spec error_response(String.t(), String.t(), map() | nil) :: error_response()
  def error_response(message, code, details \\ nil) do
    %{
      error: message,
      code: code,
      details: details,
      timestamp: DateTime.utc_now()
    }
  end

  @doc &quot;&quot;&quot;
  Creates a kill count response.
  &quot;&quot;&quot;
  @spec kill_count_response(integer(), integer()) :: kill_count()
  def kill_count_response(system_id, count) do
    %{
      system_id: system_id,
      count: count,
      timestamp: DateTime.utc_now()
    }
  end

  @doc &quot;&quot;&quot;
  Creates a systems kills response.
  &quot;&quot;&quot;
  @spec systems_kills_response(%{integer() =&gt; [kill()]}) :: systems_kills()
  def systems_kills_response(systems_kills) do
    %{
      systems_kills: systems_kills,
      timestamp: DateTime.utc_now()
    }
  end
end</file><file path="lib/wanderer_kills_web/api/helpers.ex">defmodule WandererKillsWeb.Api.Helpers do
  @moduledoc &quot;&quot;&quot;
  Helper functions for API controllers.
  &quot;&quot;&quot;

  import Plug.Conn
  alias WandererKills.Types

  @doc &quot;&quot;&quot;
  Parses an integer parameter from the request.
  Returns {:ok, integer} or {:error, :invalid_id}.
  &quot;&quot;&quot;
  @spec parse_integer_param(Plug.Conn.t(), String.t()) :: {:ok, integer()} | {:error, :invalid_id}
  def parse_integer_param(conn, param_name) do
    case Map.get(conn.params, param_name) do
      nil -&gt;
        {:error, :invalid_id}

      &quot;&quot; -&gt;
        {:error, :invalid_id}

      value when is_binary(value) -&gt;
        case Integer.parse(value) do
          {int, &quot;&quot;} when int &gt; 0 -&gt;
            {:ok, int}

          {int, &quot;&quot;} when int &lt;= 0 -&gt;
            # Allow negative numbers for some use cases
            {:ok, int}

          _ -&gt;
            {:error, :invalid_id}
        end

      value when is_integer(value) -&gt;
        {:ok, value}

      _ -&gt;
        {:error, :invalid_id}
    end
  end

  @doc &quot;&quot;&quot;
  Sends a JSON response.
  &quot;&quot;&quot;
  @spec send_json_resp(Plug.Conn.t(), integer(), term()) :: Plug.Conn.t()
  def send_json_resp(conn, status, data) do
    conn
    |&gt; put_resp_content_type(&quot;application/json&quot;)
    |&gt; send_resp(status, Jason.encode!(data))
  end

  @doc &quot;&quot;&quot;
  Renders a success response with standard envelope format.
  &quot;&quot;&quot;
  @spec render_success(Plug.Conn.t(), term()) :: Plug.Conn.t()
  def render_success(conn, data) do
    response = Types.success_response(data)
    send_json_resp(conn, 200, response)
  end

  @doc &quot;&quot;&quot;
  Renders an error response with standard envelope format.
  &quot;&quot;&quot;
  @spec render_error(Plug.Conn.t(), integer(), String.t(), String.t(), map() | nil) ::
          Plug.Conn.t()
  def render_error(conn, status_code, message, error_code, details \\ nil) do
    response = Types.error_response(message, error_code, details)
    send_json_resp(conn, status_code, response)
  end

  @doc &quot;&quot;&quot;
  Validates and parses system_id parameter.
  &quot;&quot;&quot;
  @spec validate_system_id(String.t()) :: {:ok, integer()} | {:error, :invalid_format}
  def validate_system_id(system_id_str) when is_binary(system_id_str) do
    case Integer.parse(system_id_str) do
      {system_id, &quot;&quot;} when system_id &gt; 0 -&gt;
        {:ok, system_id}

      _ -&gt;
        {:error, :invalid_format}
    end
  end

  def validate_system_id(_), do: {:error, :invalid_format}

  @doc &quot;&quot;&quot;
  Validates and parses killmail_id parameter.
  &quot;&quot;&quot;
  @spec validate_killmail_id(String.t()) :: {:ok, integer()} | {:error, :invalid_format}
  def validate_killmail_id(killmail_id_str) when is_binary(killmail_id_str) do
    case Integer.parse(killmail_id_str) do
      {killmail_id, &quot;&quot;} when killmail_id &gt; 0 -&gt;
        {:ok, killmail_id}

      _ -&gt;
        {:error, :invalid_format}
    end
  end

  def validate_killmail_id(_), do: {:error, :invalid_format}

  @doc &quot;&quot;&quot;
  Validates and parses since_hours parameter.
  &quot;&quot;&quot;
  @spec validate_since_hours(String.t() | integer()) ::
          {:ok, integer()} | {:error, :invalid_format}
  def validate_since_hours(since_hours) when is_integer(since_hours) and since_hours &gt; 0 do
    {:ok, since_hours}
  end

  def validate_since_hours(since_hours_str) when is_binary(since_hours_str) do
    case Integer.parse(since_hours_str) do
      {since_hours, &quot;&quot;} when since_hours &gt; 0 -&gt;
        {:ok, since_hours}

      _ -&gt;
        {:error, :invalid_format}
    end
  end

  def validate_since_hours(_), do: {:error, :invalid_format}

  @doc &quot;&quot;&quot;
  Validates and parses limit parameter.
  &quot;&quot;&quot;
  @spec validate_limit(String.t() | integer() | nil) ::
          {:ok, integer()} | {:error, :invalid_format}
  # default limit
  def validate_limit(nil), do: {:ok, 50}

  def validate_limit(limit) when is_integer(limit) and limit &gt; 0 and limit &lt;= 1000 do
    {:ok, limit}
  end

  def validate_limit(limit_str) when is_binary(limit_str) do
    case Integer.parse(limit_str) do
      {limit, &quot;&quot;} when limit &gt; 0 and limit &lt;= 1000 -&gt;
        {:ok, limit}

      _ -&gt;
        {:error, :invalid_format}
    end
  end

  def validate_limit(_), do: {:error, :invalid_format}

  @doc &quot;&quot;&quot;
  Validates system_ids array from request body.
  &quot;&quot;&quot;
  @spec validate_system_ids(list() | nil) :: {:ok, [integer()]} | {:error, :invalid_system_ids}
  def validate_system_ids(system_ids) when is_list(system_ids) do
    if Enum.all?(system_ids, &amp;is_integer/1) and not Enum.empty?(system_ids) do
      {:ok, system_ids}
    else
      {:error, :invalid_system_ids}
    end
  end

  def validate_system_ids(_), do: {:error, :invalid_system_ids}

  @doc &quot;&quot;&quot;
  Validates subscriber_id parameter.
  &quot;&quot;&quot;
  @spec validate_subscriber_id(String.t() | nil) ::
          {:ok, String.t()} | {:error, :invalid_subscriber_id}
  def validate_subscriber_id(subscriber_id) when is_binary(subscriber_id) do
    if String.trim(subscriber_id) != &quot;&quot; do
      {:ok, String.trim(subscriber_id)}
    else
      {:error, :invalid_subscriber_id}
    end
  end

  def validate_subscriber_id(_), do: {:error, :invalid_subscriber_id}

  @doc &quot;&quot;&quot;
  Validates callback_url parameter (optional).
  &quot;&quot;&quot;
  @spec validate_callback_url(String.t() | nil) ::
          {:ok, String.t() | nil} | {:error, :invalid_callback_url}
  def validate_callback_url(nil), do: {:ok, nil}

  def validate_callback_url(url) when is_binary(url) do
    case URI.parse(url) do
      %URI{scheme: scheme, host: host} when scheme in [&quot;http&quot;, &quot;https&quot;] and not is_nil(host) -&gt;
        {:ok, url}

      _ -&gt;
        {:error, :invalid_callback_url}
    end
  end

  def validate_callback_url(_), do: {:error, :invalid_callback_url}
end</file><file path="lib/wanderer_kills_web/api/killfeed_controller.ex">defmodule WandererKillsWeb.Api.KillfeedController do
  @moduledoc &quot;&quot;&quot;
  Controller for handling killfeed access.

  Provides basic killmail retrieval by system using the simplified KillStore API.
  &quot;&quot;&quot;

  require Logger
  import Plug.Conn
  import WandererKillsWeb.Api.Helpers, only: [send_json_resp: 3]

  alias WandererKills.Infrastructure.Config
  alias WandererKills.Killmails.Store

  # System ID validation
  defp validate_system_ids(system_ids) when is_list(system_ids) do
    Enum.reduce_while(system_ids, {:ok, []}, fn system_id, {:ok, acc} -&gt;
      case validate_system_id(system_id) do
        {:ok, valid_id} -&gt; {:cont, {:ok, [valid_id | acc]}}
        {:error, reason} -&gt; {:halt, {:error, reason}}
      end
    end)
    |&gt; case do
      {:ok, valid_ids} -&gt; {:ok, Enum.reverse(valid_ids)}
      error -&gt; error
    end
  end

  defp validate_system_ids(_), do: {:error, :systems_invalid_type}

  defp validate_system_id(system_id) when is_integer(system_id) do
    if system_id &gt;= 30_000_000 and system_id &lt;= Config.validation(:max_system_id) do
      {:ok, system_id}
    else
      {:error, :system_id_out_of_range}
    end
  end

  defp validate_system_id(_), do: {:error, :system_id_invalid_type}

  # Error response helpers
  defp handle_error(conn, :systems_invalid_type) do
    send_json_resp(conn, 400, %{error: &quot;Systems must be an array&quot;})
  end

  defp handle_error(conn, :system_id_invalid_type) do
    send_json_resp(conn, 400, %{error: &quot;System ID must be an integer&quot;})
  end

  defp handle_error(conn, :system_id_out_of_range) do
    send_json_resp(conn, 400, %{error: &quot;System ID is out of valid range&quot;})
  end

  defp handle_error(conn, :internal_error) do
    send_json_resp(conn, 500, %{error: &quot;Internal server error&quot;})
  end

  # Controller actions
  def poll(conn, %{&quot;systems&quot; =&gt; systems}) do
    case validate_system_ids(systems) do
      {:ok, valid_systems} -&gt;
        killmails =
          valid_systems
          |&gt; Enum.flat_map(&amp;Store.list_by_system/1)
          # Limit to prevent large responses
          |&gt; Enum.take(100)

        if Enum.empty?(killmails) do
          send_resp(conn, 204, &quot;&quot;)
        else
          send_json_resp(conn, 200, %{
            killmails: killmails
          })
        end

      {:error, reason} -&gt;
        handle_error(conn, reason)
    end
  end

  def poll(conn, _params) do
    send_json_resp(conn, 400, %{error: &quot;Missing required parameters&quot;})
  end

  def next(conn, %{&quot;systems&quot; =&gt; systems}) do
    case validate_system_ids(systems) do
      {:ok, valid_systems} -&gt;
        case valid_systems
             |&gt; Enum.flat_map(&amp;Store.list_by_system/1)
             |&gt; Enum.take(1) do
          [killmail] -&gt;
            send_json_resp(conn, 200, killmail)

          [] -&gt;
            send_resp(conn, 204, &quot;&quot;)
        end

      {:error, reason} -&gt;
        handle_error(conn, reason)
    end
  end

  def next(conn, _params) do
    send_json_resp(conn, 400, %{error: &quot;Missing required parameters&quot;})
  end
end</file><file path="lib/wanderer_kills_web/controllers/kills_controller.ex">defmodule WandererKillsWeb.KillsController do
  @moduledoc &quot;&quot;&quot;
  Controller for kill-related API endpoints.

  This controller provides endpoints for fetching killmails, cached data,
  and kill counts as specified in the WandererKills API interface.
  &quot;&quot;&quot;

  import WandererKillsWeb.Api.Helpers
  require Logger
  alias WandererKills.Client

  @doc &quot;&quot;&quot;
  Lists kills for a specific system with time filtering.

  GET /api/v1/kills/system/:system_id?since_hours=X&amp;limit=Y
  &quot;&quot;&quot;
  def list(conn, %{&quot;system_id&quot; =&gt; system_id_str} = params) do
    with {:ok, system_id} &lt;- validate_system_id(system_id_str),
         {:ok, since_hours} &lt;- validate_since_hours(Map.get(params, &quot;since_hours&quot;, &quot;24&quot;)),
         {:ok, limit} &lt;- validate_limit(Map.get(params, &quot;limit&quot;)) do
      Logger.info(&quot;Fetching system kills&quot;,
        system_id: system_id,
        since_hours: since_hours,
        limit: limit
      )

      case Client.fetch_system_kills(system_id, since_hours, limit) do
        {:ok, kills} -&gt;
          response = %{
            kills: kills,
            # This is a fresh fetch
            cached: false,
            timestamp: DateTime.utc_now() |&gt; DateTime.to_iso8601(),
            error: nil
          }

          render_success(conn, response)

        {:error, reason} -&gt;
          Logger.error(&quot;Failed to fetch system kills&quot;,
            system_id: system_id,
            error: reason
          )

          render_error(conn, 500, &quot;Failed to fetch system kills&quot;, &quot;FETCH_ERROR&quot;, %{
            reason: inspect(reason)
          })
      end
    else
      {:error, :invalid_format} -&gt;
        render_error(conn, 400, &quot;Invalid system ID format&quot;, &quot;INVALID_SYSTEM_ID&quot;)
    end
  end

  @doc &quot;&quot;&quot;
  Fetches kills for multiple systems.

  POST /api/v1/kills/systems
  Body: {&quot;system_ids&quot;: [int], &quot;since_hours&quot;: int, &quot;limit&quot;: int}
  &quot;&quot;&quot;
  def bulk(conn, params) do
    with {:ok, system_ids} &lt;- validate_system_ids(Map.get(params, &quot;system_ids&quot;)),
         {:ok, since_hours} &lt;- validate_since_hours(Map.get(params, &quot;since_hours&quot;, 24)),
         {:ok, limit} &lt;- validate_limit(Map.get(params, &quot;limit&quot;)) do
      Logger.info(&quot;Fetching kills for multiple systems&quot;,
        system_count: length(system_ids),
        since_hours: since_hours,
        limit: limit
      )

      {:ok, systems_kills} = Client.fetch_systems_kills(system_ids, since_hours, limit)

      response = %{
        systems_kills: systems_kills,
        timestamp: DateTime.utc_now() |&gt; DateTime.to_iso8601()
      }

      render_success(conn, response)
    else
      {:error, :invalid_system_ids} -&gt;
        render_error(conn, 400, &quot;Invalid system IDs&quot;, &quot;INVALID_SYSTEM_IDS&quot;)

      {:error, :invalid_format} -&gt;
        render_error(conn, 400, &quot;Invalid parameters&quot;, &quot;INVALID_PARAMETERS&quot;)
    end
  end

  @doc &quot;&quot;&quot;
  Returns cached kills for a system.

  GET /api/v1/kills/cached/:system_id
  &quot;&quot;&quot;
  def cached(conn, %{&quot;system_id&quot; =&gt; system_id_str}) do
    case validate_system_id(system_id_str) do
      {:ok, system_id} -&gt;
        Logger.debug(&quot;Fetching cached kills&quot;, system_id: system_id)

        kills = Client.fetch_cached_kills(system_id)

        response = %{
          kills: kills,
          timestamp: DateTime.utc_now() |&gt; DateTime.to_iso8601(),
          error: nil
        }

        render_success(conn, response)

      {:error, :invalid_format} -&gt;
        render_error(conn, 400, &quot;Invalid system ID format&quot;, &quot;INVALID_SYSTEM_ID&quot;)
    end
  end

  @doc &quot;&quot;&quot;
  Shows a specific killmail by ID.

  GET /api/v1/killmail/:killmail_id
  &quot;&quot;&quot;
  def show(conn, %{&quot;killmail_id&quot; =&gt; killmail_id_str}) do
    case validate_killmail_id(killmail_id_str) do
      {:ok, killmail_id} -&gt;
        Logger.debug(&quot;Fetching specific killmail&quot;, killmail_id: killmail_id)

        case Client.get_killmail(killmail_id) do
          nil -&gt;
            render_error(conn, 404, &quot;Killmail not found&quot;, &quot;NOT_FOUND&quot;)

          killmail -&gt;
            render_success(conn, killmail)
        end

      {:error, :invalid_format} -&gt;
        render_error(conn, 400, &quot;Invalid killmail ID format&quot;, &quot;INVALID_KILLMAIL_ID&quot;)
    end
  end

  @doc &quot;&quot;&quot;
  Returns kill count for a system.

  GET /api/v1/kills/count/:system_id
  &quot;&quot;&quot;
  def count(conn, %{&quot;system_id&quot; =&gt; system_id_str}) do
    case validate_system_id(system_id_str) do
      {:ok, system_id} -&gt;
        Logger.debug(&quot;Fetching system kill count&quot;, system_id: system_id)

        count = Client.get_system_kill_count(system_id)

        response = %{
          system_id: system_id,
          count: count,
          timestamp: DateTime.utc_now() |&gt; DateTime.to_iso8601()
        }

        render_success(conn, response)

      {:error, :invalid_format} -&gt;
        render_error(conn, 400, &quot;Invalid system ID format&quot;, &quot;INVALID_SYSTEM_ID&quot;)
    end
  end
end</file><file path="lib/wanderer_kills_web/controllers/subscriptions_controller.ex">defmodule WandererKillsWeb.SubscriptionsController do
  @moduledoc &quot;&quot;&quot;
  Controller for subscription management endpoints.

  This controller provides endpoints for creating and managing kill subscriptions
  as specified in the WandererKills API interface.
  &quot;&quot;&quot;

  import WandererKillsWeb.Api.Helpers
  require Logger
  alias WandererKills.Client

  @doc &quot;&quot;&quot;
  Creates a new kill subscription.

  POST /api/v1/subscriptions
  Body: {&quot;subscriber_id&quot;: &quot;string&quot;, &quot;system_ids&quot;: [int], &quot;callback_url&quot;: &quot;string&quot;}
  &quot;&quot;&quot;
  def create(conn, params) do
    with {:ok, subscriber_id} &lt;- validate_subscriber_id(Map.get(params, &quot;subscriber_id&quot;)),
         {:ok, system_ids} &lt;- validate_system_ids(Map.get(params, &quot;system_ids&quot;)),
         {:ok, callback_url} &lt;- validate_callback_url(Map.get(params, &quot;callback_url&quot;)) do
      Logger.info(&quot;Creating kill subscription&quot;,
        subscriber_id: subscriber_id,
        system_count: length(system_ids),
        has_callback: !is_nil(callback_url)
      )

      case Client.subscribe_to_kills(subscriber_id, system_ids, callback_url) do
        {:ok, subscription_id} -&gt;
          response = %{
            subscription_id: subscription_id,
            status: &quot;active&quot;,
            error: nil
          }

          render_success(conn, response)

        {:error, reason} -&gt;
          Logger.error(&quot;Failed to create subscription&quot;,
            subscriber_id: subscriber_id,
            error: reason
          )

          render_error(conn, 500, &quot;Failed to create subscription&quot;, &quot;SUBSCRIPTION_ERROR&quot;, %{
            reason: inspect(reason)
          })
      end
    else
      {:error, :invalid_subscriber_id} -&gt;
        render_error(conn, 400, &quot;Invalid subscriber ID&quot;, &quot;INVALID_SUBSCRIBER_ID&quot;)

      {:error, :invalid_system_ids} -&gt;
        render_error(conn, 400, &quot;Invalid system IDs&quot;, &quot;INVALID_SYSTEM_IDS&quot;)

      {:error, :invalid_callback_url} -&gt;
        render_error(conn, 400, &quot;Invalid callback URL&quot;, &quot;INVALID_CALLBACK_URL&quot;)
    end
  end

  @doc &quot;&quot;&quot;
  Deletes a kill subscription.

  DELETE /api/v1/subscriptions/:subscriber_id
  &quot;&quot;&quot;
  def delete(conn, %{&quot;subscriber_id&quot; =&gt; subscriber_id}) do
    case validate_subscriber_id(subscriber_id) do
      {:ok, validated_subscriber_id} -&gt;
        Logger.info(&quot;Deleting kill subscription&quot;, subscriber_id: validated_subscriber_id)

        case Client.unsubscribe_from_kills(validated_subscriber_id) do
          :ok -&gt;
            response = %{
              status: &quot;deleted&quot;,
              error: nil
            }

            render_success(conn, response)

          {:error, :not_found} -&gt;
            render_error(conn, 404, &quot;Subscription not found&quot;, &quot;NOT_FOUND&quot;)

          {:error, reason} -&gt;
            Logger.error(&quot;Failed to delete subscription&quot;,
              subscriber_id: validated_subscriber_id,
              error: reason
            )

            render_error(conn, 500, &quot;Failed to delete subscription&quot;, &quot;SUBSCRIPTION_ERROR&quot;, %{
              reason: inspect(reason)
            })
        end

      {:error, :invalid_subscriber_id} -&gt;
        render_error(conn, 400, &quot;Invalid subscriber ID&quot;, &quot;INVALID_SUBSCRIBER_ID&quot;)
    end
  end

  @doc &quot;&quot;&quot;
  Lists all active subscriptions (for administrative purposes).

  GET /api/v1/subscriptions
  &quot;&quot;&quot;
  def index(conn, _params) do
    Logger.debug(&quot;Listing all active subscriptions&quot;)

    subscriptions = Client.list_subscriptions()

    # Transform subscriptions for API response
    formatted_subscriptions =
      Enum.map(subscriptions, fn subscription -&gt;
        %{
          subscriber_id: subscription.subscriber_id,
          system_ids: subscription.system_ids,
          created_at: DateTime.to_iso8601(subscription.created_at)
        }
      end)

    response = %{
      subscriptions: formatted_subscriptions
    }

    render_success(conn, response)
  end
end</file><file path="lib/wanderer_kills_web/api.ex">defmodule WandererKillsWeb.Api do
  @moduledoc &quot;&quot;&quot;
  HTTP API for the Wanderer Kills service.
  &quot;&quot;&quot;

  use Plug.Router
  require Logger
  import Plug.Conn
  import WandererKillsWeb.Api.Helpers, only: [send_json_resp: 3]

  alias WandererKills.Cache.Helper
  alias WandererKills.Killmails.Coordinator
  alias WandererKills.Killmails.ZkbClient
  alias WandererKills.Observability.Monitoring

  # Moved from WandererKillsWeb.Plugs.RequestId to eliminate single-file directory
  defp request_id_plug(conn, _opts) do
    request_id = UUID.uuid4()
    Process.put(:request_id, request_id)
    Logger.metadata(request_id: request_id)
    Plug.Conn.put_resp_header(conn, &quot;x-request-id&quot;, request_id)
  end

  plug(Plug.Logger, log: :info)
  plug(:request_id_plug)
  plug(:match)
  plug(Plug.Parsers, parsers: [:json], json_decoder: Jason)
  plug(:dispatch)

  # Health check endpoint
  get &quot;/ping&quot; do
    conn
    |&gt; put_resp_content_type(&quot;text/plain&quot;)
    |&gt; send_resp(200, &quot;pong&quot;)
  end

  # Health endpoint with cache status
  get &quot;/health&quot; do
    case Monitoring.check_health() do
      {:ok, health_status} -&gt;
        status_code = if health_status.healthy, do: 200, else: 503
        response = Map.put(health_status, :timestamp, DateTime.utc_now() |&gt; DateTime.to_iso8601())
        send_json_resp(conn, status_code, response)

      {:error, reason} -&gt;
        response = %{
          error: &quot;Health check failed&quot;,
          reason: inspect(reason),
          timestamp: DateTime.utc_now() |&gt; DateTime.to_iso8601()
        }

        send_json_resp(conn, 503, response)
    end
  end

  # Status endpoint with detailed service information
  get &quot;/status&quot; do
    subscriptions = WandererKills.Client.list_subscriptions()

    response = %{
      cache_stats: %{
        # This would be populated by your monitoring system
        # For now, we&apos;ll return basic info
        status: &quot;operational&quot;
      },
      active_subscriptions: length(subscriptions),
      # This would be from your WebSocket manager
      websocket_connected: false,
      # This would be from your kill tracking system
      last_kill_received: nil,
      timestamp: DateTime.utc_now() |&gt; DateTime.to_iso8601()
    }

    send_json_resp(conn, 200, response)
  end

  # Metrics endpoint
  get &quot;/metrics&quot; do
    case Monitoring.get_metrics() do
      {:ok, metrics} -&gt;
        send_json_resp(conn, 200, metrics)

      {:error, reason} -&gt;
        send_json_resp(conn, 500, %{error: &quot;Metrics collection failed&quot;, reason: inspect(reason)})
    end
  end

  # Get a killmail by ID
  get &quot;/killmail/:id&quot; do
    case validate_killmail_id(id) do
      {:ok, killmail_id} -&gt;
        case fetch_and_cache_killmail(killmail_id) do
          {:ok, killmail} -&gt;
            handle_killmail_response({:ok, killmail}, killmail_id, conn)

          {:error, :not_found} -&gt;
            handle_killmail_response({:error, :not_found}, killmail_id, conn)

          {:error, reason} -&gt;
            handle_killmail_response({:error, reason}, killmail_id, conn)
        end

      {:error, :invalid_format} -&gt;
        send_json_resp(conn, 400, %{error: &quot;Invalid killmail ID&quot;})
    end
  end

  # Get killmails for a system
  get &quot;/system_killmails/:system_id&quot; do
    case validate_system_id(system_id) do
      {:ok, id} -&gt;
        case fetch_killmails_for_system(id) do
          {:ok, killmails} -&gt;
            handle_system_killmails_response({:ok, killmails}, id, conn)

          {:error, reason} -&gt;
            handle_system_killmails_response({:error, reason}, id, conn)
        end

      {:error, :invalid_format} -&gt;
        send_json_resp(conn, 400, %{error: &quot;Invalid system ID&quot;})
    end
  end

  # Get kill count for a system
  get &quot;/system_kill_count/:system_id&quot; do
    case validate_system_id(system_id) do
      {:ok, id} -&gt;
        case Helper.system_get_kill_count(id) do
          {:ok, count} when is_integer(count) -&gt;
            Logger.info(&quot;Successfully fetched kill count for system&quot;, %{
              system_id: id,
              kill_count: count,
              status: :success
            })

            send_json_resp(conn, 200, %{count: count})

          {:error, reason} -&gt;
            Logger.error(&quot;Failed to fetch kill count for system&quot;, %{
              system_id: id,
              error: reason,
              status: :error
            })

            send_json_resp(conn, 500, %{error: &quot;Internal server error&quot;})
        end

      {:error, :invalid_format} -&gt;
        send_json_resp(conn, 400, %{error: &quot;Invalid system ID&quot;})
    end
  end

  # Get killmails for a system (alternative route)
  get &quot;/system/:id/killmails&quot; do
    case validate_system_id(id) do
      {:ok, system_id} -&gt;
        case fetch_killmails_for_system(system_id) do
          {:ok, killmails} -&gt;
            Logger.info(&quot;Successfully fetched killmails for system&quot;, %{
              system_id: system_id,
              killmail_count: length(killmails),
              status: :success,
              route: :alternative
            })

            send_json_resp(conn, 200, %{killmails: killmails})

          {:error, reason} -&gt;
            Logger.error(&quot;Failed to fetch killmails for system&quot;, %{
              system_id: system_id,
              error: reason,
              status: :error,
              route: :alternative
            })

            send_json_resp(conn, 500, %{error: &quot;Internal server error&quot;})
        end

      {:error, :invalid_format} -&gt;
        send_json_resp(conn, 400, %{error: &quot;Invalid system ID&quot;})
    end
  end

  # Legacy endpoint that redirects to /system_killmails/:system_id
  get &quot;/kills_for_system/:system_id&quot; do
    case validate_system_id(system_id) do
      {:ok, id} -&gt;
        conn
        |&gt; put_status(302)
        |&gt; put_resp_header(&quot;location&quot;, &quot;/system_killmails/#{id}&quot;)
        |&gt; send_resp(302, &quot;&quot;)

      {:error, :invalid_format} -&gt;
        send_json_resp(conn, 400, %{error: &quot;Invalid system ID&quot;})
    end
  end

  # New API v1 endpoints - Kill management
  get &quot;/api/v1/kills/system/:system_id&quot; do
    WandererKillsWeb.KillsController.list(conn, Map.merge(conn.params, conn.query_params))
  end

  post &quot;/api/v1/kills/systems&quot; do
    WandererKillsWeb.KillsController.bulk(conn, conn.params)
  end

  get &quot;/api/v1/kills/cached/:system_id&quot; do
    WandererKillsWeb.KillsController.cached(conn, conn.params)
  end

  get &quot;/api/v1/killmail/:killmail_id&quot; do
    WandererKillsWeb.KillsController.show(conn, conn.params)
  end

  get &quot;/api/v1/kills/count/:system_id&quot; do
    WandererKillsWeb.KillsController.count(conn, conn.params)
  end

  # New API v1 endpoints - Subscription management
  post &quot;/api/v1/subscriptions&quot; do
    WandererKillsWeb.SubscriptionsController.create(conn, conn.params)
  end

  delete &quot;/api/v1/subscriptions/:subscriber_id&quot; do
    WandererKillsWeb.SubscriptionsController.delete(conn, conn.params)
  end

  get &quot;/api/v1/subscriptions&quot; do
    WandererKillsWeb.SubscriptionsController.index(conn, conn.params)
  end

  # Killfeed endpoints
  get &quot;/api/killfeed&quot; do
    WandererKillsWeb.Api.KillfeedController.poll(conn, conn.query_params)
  end

  get &quot;/api/killfeed/next&quot; do
    WandererKillsWeb.Api.KillfeedController.next(conn, conn.query_params)
  end

  # Catch-all route
  match _ do
    Logger.warning(&quot;Invalid request path&quot;, %{
      path: conn.request_path,
      method: conn.method,
      error_type: :not_found
    })

    send_json_resp(conn, 404, %{error: &quot;Not found&quot;})
  end

  @spec handle_killmail_response({:ok, map()} | {:error, term()}, integer(), Plug.Conn.t()) ::
          Plug.Conn.t()
  defp handle_killmail_response({:ok, killmail}, killmail_id, conn) do
    Logger.info(&quot;Successfully fetched killmail&quot;,
      killmail_id: killmail_id,
      status: :success
    )

    send_json_resp(conn, 200, killmail)
  end

  defp handle_killmail_response({:error, :not_found}, killmail_id, conn) do
    Logger.info(&quot;Killmail not found&quot;,
      killmail_id: killmail_id,
      status: :not_found
    )

    send_json_resp(conn, 404, %{error: &quot;Killmail not found&quot;})
  end

  defp handle_killmail_response(
         {:error, %WandererKills.Infrastructure.Error{domain: :zkb, type: :not_found}},
         killmail_id,
         conn
       ) do
    Logger.info(&quot;Killmail not found&quot;,
      killmail_id: killmail_id,
      status: :not_found
    )

    send_json_resp(conn, 404, %{error: &quot;Killmail not found&quot;})
  end

  defp handle_killmail_response({:error, reason}, killmail_id, conn) do
    Logger.error(&quot;Failed to fetch killmail&quot;,
      killmail_id: killmail_id,
      error: reason,
      status: :error
    )

    send_json_resp(conn, 500, %{error: &quot;Internal server error&quot;})
  end

  @spec validate_killmail_id(String.t()) :: {:ok, integer()} | {:error, :invalid_format}
  defp validate_killmail_id(id_str) do
    case Integer.parse(id_str) do
      {id, &quot;&quot;} when id &gt; 0 -&gt;
        {:ok, id}

      _ -&gt;
        Logger.warning(&quot;Invalid killmail ID format&quot;,
          provided_id: id_str,
          status: :invalid_format
        )

        {:error, :invalid_format}
    end
  end

  @spec handle_system_killmails_response(
          {:ok, list()} | {:error, term()},
          integer(),
          Plug.Conn.t()
        ) :: Plug.Conn.t()
  defp handle_system_killmails_response({:ok, killmails}, system_id, conn) do
    Logger.info(&quot;Successfully fetched killmails for system&quot;,
      system_id: system_id,
      killmail_count: length(killmails),
      status: :success
    )

    send_json_resp(conn, 200, killmails)
  end

  defp handle_system_killmails_response({:error, reason}, system_id, conn) do
    Logger.error(&quot;Failed to fetch killmails for system&quot;,
      system_id: system_id,
      error: reason,
      status: :error
    )

    send_json_resp(conn, 500, %{error: &quot;Internal server error&quot;})
  end

  @spec validate_system_id(String.t()) :: {:ok, integer()} | {:error, :invalid_format}
  defp validate_system_id(id_str) do
    case Integer.parse(id_str) do
      {id, &quot;&quot;} when id &gt; 0 -&gt;
        {:ok, id}

      _ -&gt;
        Logger.warning(&quot;Invalid system ID format&quot;,
          provided_id: id_str,
          status: :invalid_format
        )

        {:error, :invalid_format}
    end
  end

  # Helper functions to replace Fetching.Coordinator functionality

  @spec fetch_and_cache_killmail(integer()) :: {:ok, map()} | {:error, term()}
  defp fetch_and_cache_killmail(killmail_id) do
    with {:ok, raw_killmail} &lt;- ZkbClient.fetch_killmail(killmail_id),
         {:ok, processed_killmail} &lt;- Coordinator.process_single_killmail(raw_killmail),
         {:ok, _} &lt;- Helper.killmail_put(killmail_id, processed_killmail) do
      {:ok, processed_killmail}
    else
      {:error, reason} -&gt; {:error, reason}
    end
  end

  @spec fetch_killmails_for_system(integer()) :: {:ok, list()} | {:error, term()}
  defp fetch_killmails_for_system(system_id) do
    # Check cache first
    case Helper.system_recently_fetched?(system_id) do
      {:ok, true} -&gt;
        # Cache is fresh, get cached data
        case Helper.system_get_killmails(system_id) do
          {:ok, killmail_ids} -&gt; {:ok, killmail_ids}
          {:error, _reason} -&gt; fetch_remote_killmails(system_id)
        end

      {:ok, false} -&gt;
        # Cache is stale, fetch from remote
        fetch_remote_killmails(system_id)

      {:error, _reason} -&gt;
        # Cache check failed, fetch from remote
        fetch_remote_killmails(system_id)
    end
  end

  @spec fetch_remote_killmails(integer()) :: {:ok, list()} | {:error, term()}
  defp fetch_remote_killmails(system_id) do
    # Default limit
    limit = 5
    # Default since hours
    since_hours = 24

    with {:ok, raw_killmails} &lt;- ZkbClient.fetch_system_killmails(system_id, limit, since_hours),
         {:ok, processed_killmails} &lt;-
           Coordinator.process_killmails(raw_killmails, system_id, since_hours),
         :ok &lt;-
           Helper.cache_killmails_for_system(
             system_id,
             processed_killmails
           ) do
      {:ok, processed_killmails}
    else
      {:error, reason} -&gt; {:error, reason}
    end
  end
end</file><file path="lib/wanderer_kills_web.ex">defmodule WandererKillsWeb do
  @moduledoc &quot;&quot;&quot;
  The entrypoint for defining your web interface, such
  as controllers, views, channels and so on.

  This can be used in your application as:

      use WandererKillsWeb, :controller
      use WandererKillsWeb, :view
  &quot;&quot;&quot;

  def controller do
    quote do
      use Phoenix.Controller, namespace: WandererKillsWeb

      import Plug.Conn
      import WandererKills.Gettext
      alias WandererKillsWeb.Router.Helpers, as: Routes
    end
  end

  def view do
    quote do
      use Phoenix.View,
        root: &quot;lib/wanderer_kills_web/templates&quot;,
        namespace: WandererKillsWeb

      # Import convenience functions from controllers
      import Phoenix.Controller,
        only: [get_flash: 1, get_flash: 2, view_module: 1, view_template: 1]

      # Include shared imports and aliases for views
      unquote(view_helpers())
    end
  end

  def router do
    quote do
      use Phoenix.Router

      import Plug.Conn
      import Phoenix.Controller
    end
  end

  def channel do
    quote do
      use Phoenix.Channel
      import WandererKills.Gettext
    end
  end

  defp view_helpers do
    quote do
      # Import basic rendering functionality (render, render_layout, etc)
      import Phoenix.View

      import WandererKills.ErrorHelpers
      import WandererKills.Gettext
      alias WandererKillsWeb.Router.Helpers, as: Routes
    end
  end

  @doc &quot;&quot;&quot;
  When used, dispatch to the appropriate controller/view/etc.
  &quot;&quot;&quot;
  defmacro __using__(which) when is_atom(which) do
    apply(__MODULE__, which, [])
  end
end</file><file path="plans/interface.md">- [ ] “Define the ZKB client behaviour interface. Create `lib/zkb_service/client_behaviour.ex` with exactly these callbacks from the spec:

  ````elixir
  defmodule ZkbService.ClientBehaviour do
    @callback fetch_system_kills(system_id :: integer(), since_hours :: integer(), limit :: integer())
      :: {:ok, [kill()]} | {:error, term()}
    @callback fetch_systems_kills(system_ids :: [integer()], since_hours :: integer(), limit :: integer())
      :: {:ok, %{integer() =&gt; [kill()]}} | {:error, term()}
    @callback fetch_cached_kills(system_id :: integer()) :: [kill()]
    @callback fetch_cached_kills_for_systems(system_ids :: [integer()]) :: %{integer() =&gt; [kill()]}
    @callback subscribe_to_kills(subscriber_id :: String.t(), system_ids :: [integer()], callback_url :: String.t() | nil)
      :: {:ok, subscription_id :: String.t()} | {:error, term()}
    @callback unsubscribe_from_kills(subscriber_id :: String.t()) :: :ok | {:error, term()}
    @callback get_killmail(killmail_id :: integer()) :: kill() | nil
    @callback get_system_kill_count(system_id :: integer()) :: integer()
  end
  ```”

  ````

- [ ] “Implement the behaviour in your existing ZKB client. In `lib/wanderer_kills/zkb/client.ex` add `@behaviour ZkbService.ClientBehaviour` and implement:

  - `fetch_systems_kills/3` by batching calls to `fetch_system_kills/3` and aggregating results
  - `fetch_cached_kills/1` and `fetch_cached_kills_for_systems/1` by delegating to your cache module
  - `subscribe_to_kills/3` and `unsubscribe_from_kills/1` via a new `ZkbService.SubscriptionManager` GenServer
  - `get_killmail/1` by reusing your existing killmail lookup
  - `get_system_kill_count/1` via your cache or direct store”

- [ ] “Scope and add HTTP routes under `/api/v1`. In `lib/wanderer_kills_web/router.ex`:

  ````elixir
  scope &quot;/api/v1&quot;, WandererKillsWeb do
    get   &quot;/kills/system/:system_id&quot;,       KillsController, :list
    post  &quot;/kills/systems&quot;,                KillsController, :bulk
    get   &quot;/kills/cached/:system_id&quot;,      KillsController, :cached
    get   &quot;/killmail/:killmail_id&quot;,        KillsController, :show
    get   &quot;/kills/count/:system_id&quot;,       KillsController, :count
    post  &quot;/subscriptions&quot;,                SubscriptionsController, :create
    delete &quot;/subscriptions/:subscriber_id&quot;, SubscriptionsController, :delete
  end
  ```”

  ````

- [ ] “Create `Api.KillsController` (`lib/wanderer_kills_web/controllers/kills_controller.ex`) with actions:

  - `list(conn, %{&quot;system_id&quot; =&gt; id, &quot;since_hours&quot; =&gt; h, &quot;limit&quot; =&gt; l})` → call `fetch_system_kills/3`, render JSON array
  - `bulk(conn, params)` → validate `system_ids`, `since_hours`, `limit`, call `fetch_systems_kills/3`, render `%{systems_kills: …, timestamp: …}`
  - `cached(conn, %{&quot;system_id&quot; =&gt; id})` → call `fetch_cached_kills/1`, render JSON array
  - `show(conn, %{&quot;killmail_id&quot; =&gt; id})` → call `get_killmail/1`, 404 if nil
  - `count(conn, %{&quot;system_id&quot; =&gt; id})` → call `get_system_kill_count/1`, render `%{system_id: id, count: n, timestamp: …}`”

- [ ] “Create `Api.SubscriptionsController` (`lib/wanderer_kills_web/controllers/subscriptions_controller.ex`) with:

  - `create(conn, %{&quot;subscriber_id&quot; =&gt; sid, &quot;system_ids&quot; =&gt; ids, &quot;callback_url&quot; =&gt; url})` → call `subscribe_to_kills/3`, render `{subscription_id, status}`
  - `delete(conn, %{&quot;subscriber_id&quot; =&gt; sid})` → call `unsubscribe_from_kills/1`, render status”

- [ ] “Implement `ZkbService.SubscriptionManager` (`lib/zkb_service/subscription_manager.ex`) as a GenServer or ETS-backed store to:

  - Track `{subscriber_id, system_ids, callback_url}`
  - Expose `subscribe/3` and `unsubscribe/1` that satisfy the behaviour
  - On each new fetch, dispatch HTTP POSTs to `callback_url` (if given) and broadcast via Phoenix.PubSub”

- [ ] “Add PubSub broadcasts in your fetch loop. After caching new kills or counts, call:
  ```elixir
  Phoenix.PubSub.broadcast(ZkbService.PubSub, &quot;zkb:kills:updated&quot;, %{…})
  Phoenix.PubSub.broadcast(ZkbService.PubSub, &quot;zkb:system:#{system_id}&quot;, %{…})
  Use the message formats (:systems_kill_update, etc.) exactly as in the spec.”
  ```

“Define the kill() struct/type. Create lib/zkb_service/types.ex:

elixir
Copy
Edit
@type kill :: %{
killmail_id: integer(),
kill_time: DateTime.t(),
solar_system_id: integer(),
victim: %{…},
attackers: [ … ]
}

```”

 “Centralize JSON response envelopes. In lib/wanderer_kills_web/controllers/helpers.ex add:

elixir
Copy
Edit
def render_success(conn, data), do: json(conn, %{data: data, timestamp: …})
def render_error(conn, code, msg, details \\ nil), do: json(conn, %{error: msg, code: code, details: details, timestamp: …})
And use these in all controller actions.”

 “Implement the WebSocket channel. In lib/wanderer_kills_web/channels/kills_channel.ex:

Handle join(&quot;zkb:subscriber:&quot; &lt;&gt; subscriber_id, _payload, socket)

Handle &quot;subscribe&quot;/&quot;unsubscribe&quot; messages, routing them to your SubscriptionManager

Push inbound PubSub messages to the socket client”

Copy
Edit
```</file><file path="plans/zkb_service_interfaces.md"># ZKB Service Interface Specification

## Overview

This document defines the interfaces that the new ZKB (zKillboard) service should provide to support killmail data fetching, caching, and real-time updates for the Wanderer application.

## Core Service Interfaces

### 1. HTTP REST API

#### Kill Data Endpoints

**Fetch System Kills**
```
GET /api/v1/kills/system/{system_id}
Query Parameters:
  - since_hours: integer (required) - Hours to look back for kills
  - limit: integer (optional) - Maximum number of kills to return

Response:
{
  &quot;kills&quot;: [...],           // Array of kill objects
  &quot;cached&quot;: boolean,        // Whether data came from cache
  &quot;timestamp&quot;: &quot;ISO8601&quot;,   // Response timestamp
  &quot;error&quot;: null             // Error message if any
}
```

**Fetch Multiple Systems Kills**
```
POST /api/v1/kills/systems
Body:
{
  &quot;system_ids&quot;: [integer],  // Array of system IDs
  &quot;since_hours&quot;: integer,   // Hours to look back
  &quot;limit&quot;: integer          // Optional limit per system
}

Response:
{
  &quot;systems_kills&quot;: {
    &quot;system_id&quot;: [...],     // Kill arrays keyed by system ID
    ...
  },
  &quot;timestamp&quot;: &quot;ISO8601&quot;,
  &quot;error&quot;: null
}
```

**Get Cached Kills**
```
GET /api/v1/kills/cached/{system_id}

Response:
{
  &quot;kills&quot;: [...],           // Cached kills for system
  &quot;timestamp&quot;: &quot;ISO8601&quot;,
  &quot;error&quot;: null
}
```

**Get Specific Killmail**
```
GET /api/v1/killmail/{killmail_id}

Response:
{
  &quot;killmail_id&quot;: integer,
  &quot;kill_time&quot;: &quot;ISO8601&quot;,
  &quot;solar_system_id&quot;: integer,
  &quot;victim&quot;: {...},
  &quot;attackers&quot;: [...],
  &quot;zkb&quot;: {...}
}
```

**Get System Kill Count**
```
GET /api/v1/kills/count/{system_id}

Response:
{
  &quot;system_id&quot;: integer,
  &quot;count&quot;: integer,         // Current kill count for system
  &quot;timestamp&quot;: &quot;ISO8601&quot;
}
```

#### Subscription Management Endpoints

**Create Subscription**
```
POST /api/v1/subscriptions
Body:
{
  &quot;subscriber_id&quot;: &quot;string&quot;,    // Unique subscriber identifier
  &quot;system_ids&quot;: [integer],      // Systems to subscribe to
  &quot;callback_url&quot;: &quot;string&quot;      // Optional webhook URL
}

Response:
{
  &quot;subscription_id&quot;: &quot;string&quot;,
  &quot;status&quot;: &quot;active&quot;,
  &quot;error&quot;: null
}
```

**Remove Subscription**
```
DELETE /api/v1/subscriptions/{subscriber_id}

Response:
{
  &quot;status&quot;: &quot;deleted&quot;,
  &quot;error&quot;: null
}
```

**List Active Subscriptions**
```
GET /api/v1/subscriptions

Response:
{
  &quot;subscriptions&quot;: [
    {
      &quot;subscriber_id&quot;: &quot;string&quot;,
      &quot;system_ids&quot;: [integer],
      &quot;created_at&quot;: &quot;ISO8601&quot;
    }
  ]
}
```

### 2. WebSocket Interface

**Connection**
```
WS /ws/kills

Messages:
{
  &quot;type&quot;: &quot;subscribe&quot;,
  &quot;subscriber_id&quot;: &quot;string&quot;,
  &quot;system_ids&quot;: [integer]
}

{
  &quot;type&quot;: &quot;unsubscribe&quot;,
  &quot;subscriber_id&quot;: &quot;string&quot;
}
```

**Real-time Updates**
```
{
  &quot;type&quot;: &quot;kill_update&quot;,
  &quot;data&quot;: {
    &quot;solar_system_id&quot;: integer,
    &quot;kills&quot;: integer,
    &quot;timestamp&quot;: &quot;ISO8601&quot;
  }
}

{
  &quot;type&quot;: &quot;detailed_kill_update&quot;,
  &quot;data&quot;: {
    &quot;solar_system_id&quot;: integer,
    &quot;kills&quot;: [...],           // Full kill objects
    &quot;timestamp&quot;: &quot;ISO8601&quot;
  }
}
```

### 3. Phoenix PubSub Topics

**Topic Structure**
```elixir
# Global kill updates
&quot;zkb:kills:updated&quot;                    # Basic kill count changes
&quot;zkb:detailed_kills:updated&quot;           # Detailed killmail updates

# System-specific updates
&quot;zkb:system:#{system_id}&quot;              # All updates for a system
&quot;zkb:system:#{system_id}:detailed&quot;     # Detailed kills for a system

# Subscriber-specific updates
&quot;zkb:subscriber:#{subscriber_id}&quot;      # Updates for specific subscriber
```

**Message Formats**
```elixir
# Kill count update
%{
  type: :kill_count_update,
  solar_system_id: integer(),
  kills: integer(),
  timestamp: DateTime.t()
}

# Detailed kill update
%{
  type: :detailed_kill_update,
  solar_system_id: integer(),
  kills: [kill_map()],
  timestamp: DateTime.t()
}

# Multiple systems update
%{
  type: :systems_kill_update,
  systems_kills: %{integer() =&gt; [kill_map()]},
  timestamp: DateTime.t()
}
```

## Data Structures

### Kill Object
```elixir
%{
  killmail_id: integer(),
  kill_time: DateTime.t(),
  solar_system_id: integer(),
  victim: %{
    character_id: integer() | nil,
    corporation_id: integer(),
    alliance_id: integer() | nil,
    ship_type_id: integer(),
    damage_taken: integer()
  },
  attackers: [
    %{
      character_id: integer() | nil,
      corporation_id: integer() | nil,
      alliance_id: integer() | nil,
      ship_type_id: integer() | nil,
      weapon_type_id: integer() | nil,
      damage_done: integer(),
      final_blow: boolean()
    }
  ],
  zkb: %{
    location_id: integer() | nil,
    hash: String.t(),
    fitted_value: float(),
    total_value: float(),
    points: integer(),
    npc: boolean(),
    solo: boolean(),
    awox: boolean()
  }
}
```

### Error Response
```elixir
%{
  error: String.t(),
  code: String.t(),           # &quot;RATE_LIMITED&quot;, &quot;NOT_FOUND&quot;, etc.
  details: map() | nil,       # Additional error context
  timestamp: DateTime.t()
}
```

## Client Behaviour Interface

The service should be consumable through a client that implements this behaviour:

```elixir
defmodule ZkbService.ClientBehaviour do
  @type kill :: map()
  @type system_id :: integer()
  @type subscriber_id :: String.t()

  # Direct fetch operations
  @callback fetch_system_kills(system_id(), integer()) :: 
    {:ok, [kill()]} | {:error, term()}
  
  @callback fetch_systems_kills([system_id()], integer()) :: 
    {:ok, %{system_id() =&gt; [kill()]}} | {:error, term()}
  
  @callback fetch_cached_kills(system_id()) :: [kill()]
  
  @callback fetch_cached_kills_for_systems([system_id()]) :: 
    %{system_id() =&gt; [kill()]}

  # Subscription management
  @callback subscribe_to_kills(subscriber_id(), [system_id()]) :: 
    :ok | {:error, term()}
  
  @callback unsubscribe_from_kills(subscriber_id()) :: :ok

  # Cache operations
  @callback get_killmail(integer()) :: kill() | nil
  @callback get_system_kill_count(system_id()) :: integer()
end
```

## Service Configuration

The service should accept configuration for:

```elixir
%{
  # HTTP server
  http_port: 4001,
  
  # PubSub configuration
  pubsub_adapter: Phoenix.PubSub.PG2,
  pubsub_name: ZkbService.PubSub,
  
  # Caching
  cache_ttl: :timer.hours(24),           # Killmail cache TTL
  system_kills_ttl: :timer.hours(1),     # System kill count TTL
  
  # Fetching behavior
  fetch_interval: :timer.seconds(15),     # Background fetch interval
  max_concurrent_fetches: 10,             # Concurrent fetch limit
  preload_cycle_ticks: 120,              # Full preload cycle
  
  # External services
  zkb_websocket_url: &quot;wss://zkillboard.com/websocket/&quot;,
  esi_base_url: &quot;https://esi.evetech.net&quot;,
  
  # Rate limiting
  max_requests_per_minute: 1000,
  burst_limit: 100
}
```

## Health and Monitoring Endpoints

```
GET /health
Response: {&quot;status&quot;: &quot;ok&quot;, &quot;timestamp&quot;: &quot;ISO8601&quot;}

GET /metrics
Response: Prometheus-formatted metrics

GET /status
Response: {
  &quot;cache_stats&quot;: {...},
  &quot;active_subscriptions&quot;: integer,
  &quot;websocket_connected&quot;: boolean,
  &quot;last_kill_received&quot;: &quot;ISO8601&quot;
}
```

## Error Handling

The service should return appropriate HTTP status codes:

- `200` - Success
- `400` - Bad Request (invalid parameters)
- `404` - Not Found (system/killmail not found)
- `429` - Rate Limited
- `500` - Internal Server Error
- `503` - Service Unavailable

Error responses should include:
```json
{
  &quot;error&quot;: &quot;Human-readable error message&quot;,
  &quot;code&quot;: &quot;ERROR_CODE&quot;,
  &quot;timestamp&quot;: &quot;ISO8601&quot;,
  &quot;details&quot;: {}
}
```

## Rate Limiting

The service should implement rate limiting:
- Per-IP limits for public endpoints
- Per-subscriber limits for authenticated endpoints
- Graceful degradation under load
- Proper retry-after headers

## Security Considerations

- API key authentication for subscription management
- Request validation and sanitization
- CORS configuration for web clients
- Rate limiting and DDoS protection
- Secure WebSocket connections (WSS in production)</file><file path="test/external/esi_cache_test.exs">defmodule WandererKills.EsiCacheTest do
  # Disable async to avoid cache interference
  use ExUnit.Case, async: false
  alias WandererKills.Cache.Helper

  setup do
    WandererKills.TestHelpers.clear_all_caches()

    # Set the http_client for this test
    Application.put_env(:wanderer_kills, :http_client, WandererKills.Http.Client.Mock)

    on_exit(fn -&gt;
      Application.put_env(:wanderer_kills, :http_client, WandererKills.MockHttpClient)
      WandererKills.TestHelpers.clear_all_caches()
    end)
  end

  describe &quot;character info&quot; do
    test &quot;unified cache interface works for character data&quot; do
      character_id = 123

      expected_data = %{
        character_id: character_id,
        name: &quot;Test Character&quot;,
        corporation_id: 456,
        alliance_id: 789,
        faction_id: nil,
        security_status: 5.0
      }

      # Store test data using unified cache interface
      assert {:ok, true} = Helper.character_put(character_id, expected_data)
      assert {:ok, actual_data} = Helper.character_get(character_id)
      assert actual_data.character_id == expected_data.character_id
      assert actual_data.name == expected_data.name
    end
  end

  describe &quot;corporation info&quot; do
    test &quot;unified cache interface works for corporation data&quot; do
      corporation_id = 456

      corp_data = %{
        corporation_id: corporation_id,
        name: &quot;Test Corp&quot;,
        ticker: &quot;TEST&quot;,
        member_count: 100
      }

      assert {:ok, true} = Helper.corporation_put(corporation_id, corp_data)
      assert {:ok, cached_data} = Helper.corporation_get(corporation_id)
      assert cached_data.corporation_id == corporation_id
      assert cached_data.name == &quot;Test Corp&quot;
    end
  end

  describe &quot;alliance info&quot; do
    test &quot;unified cache interface works for alliance data&quot; do
      alliance_id = 789

      alliance_data = %{
        alliance_id: alliance_id,
        name: &quot;Test Alliance&quot;,
        ticker: &quot;TESTA&quot;,
        creator_corporation_id: 456
      }

      assert {:ok, true} = Helper.alliance_put(alliance_id, alliance_data)
      assert {:ok, cached_data} = Helper.alliance_get(alliance_id)
      assert cached_data.alliance_id == alliance_id
      assert cached_data.name == &quot;Test Alliance&quot;
    end
  end

  describe &quot;type info&quot; do
    test &quot;unified cache interface works for type data&quot; do
      type_id = 1234

      type_data = %{
        type_id: type_id,
        name: &quot;Test Type&quot;,
        group_id: 5678,
        published: true
      }

      assert {:ok, true} = Helper.ship_type_put(type_id, type_data)
      assert {:ok, cached_data} = Helper.ship_type_get(type_id)
      assert cached_data.type_id == type_id
      assert cached_data.name == &quot;Test Type&quot;
    end
  end

  describe &quot;group info&quot; do
    test &quot;unified cache interface works for group data&quot; do
      group_id = 5678

      group_data = %{
        group_id: group_id,
        name: &quot;Test Group&quot;,
        category_id: 91,
        published: true,
        types: [1234, 5678]
      }

      assert {:ok, true} = Helper.put(&quot;groups&quot;, to_string(group_id), group_data)
      assert {:ok, cached_data} = Helper.get_with_error(&quot;groups&quot;, to_string(group_id))
      assert cached_data.group_id == group_id
      assert cached_data.name == &quot;Test Group&quot;
    end
  end

  describe &quot;clear cache&quot; do
    test &quot;clear removes all entries&quot; do
      # Add some data first to ensure the namespaces exist
      assert {:ok, true} = Helper.character_put(123, %{name: &quot;test&quot;})
      assert {:ok, true} = Helper.corporation_put(456, %{name: &quot;test corp&quot;})
      assert {:ok, true} = Helper.alliance_put(789, %{name: &quot;test alliance&quot;})

      # Test clearing specific namespace
      Helper.clear_namespace(&quot;characters&quot;)
      Helper.clear_namespace(&quot;corporations&quot;)
      Helper.clear_namespace(&quot;alliances&quot;)
    end
  end
end</file><file path="test/fetcher/zkb_service_test.exs">defmodule WandererKills.Killmails.ZkbClientTest do
  use ExUnit.Case, async: true
  import Mox

  @moduletag :external

  alias WandererKills.Killmails.ZkbClient, as: ZKB
  alias WandererKills.TestHelpers
  alias WandererKills.Http.Client.Mock, as: HttpClientMock

  setup :verify_on_exit!

  setup do
    TestHelpers.clear_all_caches()

    # Configure the HTTP client to use the mock
    Application.put_env(:wanderer_kills, :http_client, HttpClientMock)

    on_exit(fn -&gt;
      # Reset to default
      Application.delete_env(:wanderer_kills, :http_client)
    end)

    :ok
  end

  describe &quot;fetch_killmail/1&quot; do
    test &quot;successfully fetches a killmail&quot; do
      killmail_id = 123_456
      killmail = TestHelpers.generate_test_data(:killmail, killmail_id)

      HttpClientMock
      |&gt; expect(:get_with_rate_limit, fn
        &quot;https://zkillboard.com/api/killID/123456/&quot;, _opts -&gt;
          {:ok, %{status: 200, body: Jason.encode!([killmail])}}
      end)

      assert {:ok, ^killmail} = ZKB.fetch_killmail(killmail_id)
    end

    test &quot;handles killmail not found (nil response)&quot; do
      killmail_id = 999_999

      HttpClientMock
      |&gt; expect(:get_with_rate_limit, fn
        &quot;https://zkillboard.com/api/killID/999999/&quot;, _opts -&gt;
          {:ok, %{status: 200, body: &quot;[]&quot;}}
      end)

      assert {:error, error} = ZKB.fetch_killmail(killmail_id)
      assert error.domain == :zkb
      assert error.type == :not_found
      assert String.contains?(error.message, &quot;not found&quot;)
    end

    test &quot;handles client errors&quot; do
      killmail_id = 123_456

      HttpClientMock
      |&gt; expect(:get_with_rate_limit, fn
        &quot;https://zkillboard.com/api/killID/123456/&quot;, _opts -&gt;
          {:error, :rate_limited}
      end)

      assert {:error, :rate_limited} = ZKB.fetch_killmail(killmail_id)
    end

    test &quot;validates killmail ID format&quot; do
      assert {:error, error} = ZKB.fetch_killmail(&quot;invalid&quot;)
      assert error.domain == :validation
      assert String.contains?(error.message, &quot;Invalid killmail ID format&quot;)
    end

    test &quot;validates positive killmail ID&quot; do
      assert {:error, error} = ZKB.fetch_killmail(-1)
      assert error.domain == :validation
    end
  end

  describe &quot;fetch_system_killmails/3&quot; do
    test &quot;successfully fetches system killmails&quot; do
      system_id = 30_000_142
      killmail1 = TestHelpers.generate_test_data(:killmail, 123)
      killmail2 = TestHelpers.generate_test_data(:killmail, 456)
      killmails = [killmail1, killmail2]

      HttpClientMock
      |&gt; expect(:get_with_rate_limit, fn
        &quot;https://zkillboard.com/api/systemID/30000142/&quot;, _opts -&gt;
          {:ok, %{status: 200, body: Jason.encode!(killmails)}}
      end)

      assert {:ok, ^killmails} = ZKB.fetch_system_killmails(system_id, 10, 24)
    end

    test &quot;handles empty killmail list&quot; do
      system_id = 30_000_142

      HttpClientMock
      |&gt; expect(:get_with_rate_limit, fn
        &quot;https://zkillboard.com/api/systemID/30000142/&quot;, _opts -&gt;
          {:ok, %{status: 200, body: &quot;[]&quot;}}
      end)

      assert {:ok, []} = ZKB.fetch_system_killmails(system_id, 10, 24)
    end

    test &quot;handles client errors&quot; do
      system_id = 30_000_142

      HttpClientMock
      |&gt; expect(:get_with_rate_limit, fn
        &quot;https://zkillboard.com/api/systemID/30000142/&quot;, _opts -&gt;
          {:error, :timeout}
      end)

      assert {:error, :timeout} = ZKB.fetch_system_killmails(system_id, 10, 24)
    end

    test &quot;validates system ID format&quot; do
      assert {:error, error} = ZKB.fetch_system_killmails(&quot;invalid&quot;, 10, 24)
      assert error.domain == :validation
      assert String.contains?(error.message, &quot;Invalid system ID format&quot;)
    end

    test &quot;validates positive system ID&quot; do
      assert {:error, error} = ZKB.fetch_system_killmails(-1, 10, 24)
      assert error.domain == :validation
    end
  end

  describe &quot;get_system_kill_count/1&quot; do
    test &quot;successfully gets kill count&quot; do
      system_id = 30_000_142
      expected_count = 42

      # Create a list with 42 items (the function counts list length)
      kill_list = Enum.map(1..expected_count, fn id -&gt; %{&quot;killmail_id&quot; =&gt; id} end)

      HttpClientMock
      |&gt; expect(:get_with_rate_limit, fn
        &quot;https://zkillboard.com/api/systemID/30000142/&quot;, _opts -&gt;
          {:ok, %{status: 200, body: Jason.encode!(kill_list)}}
      end)

      assert {:ok, ^expected_count} = ZKB.get_system_kill_count(system_id)
    end

    test &quot;handles client errors&quot; do
      system_id = 30_000_142

      HttpClientMock
      |&gt; expect(:get_with_rate_limit, fn
        &quot;https://zkillboard.com/api/systemID/30000142/&quot;, _opts -&gt;
          {:error, :not_found}
      end)

      assert {:error, :not_found} = ZKB.get_system_kill_count(system_id)
    end

    test &quot;validates system ID format&quot; do
      assert {:error, error} = ZKB.get_system_kill_count(&quot;invalid&quot;)
      assert error.domain == :validation
      assert String.contains?(error.message, &quot;Invalid system ID format&quot;)
    end

    test &quot;validates positive system ID&quot; do
      assert {:error, error} = ZKB.get_system_kill_count(-1)
      assert error.domain == :validation
    end
  end
end</file><file path="test/fixtures/.gitkeep"># This file ensures the test/fixtures directory is tracked by git</file><file path="test/integration/api_helpers_test.exs">defmodule WandererKills.Api.HelpersTest do
  use ExUnit.Case, async: true
  import Plug.Test
  import Plug.Conn

  alias WandererKillsWeb.Api.Helpers

  describe &quot;parse_integer_param/2&quot; do
    test &quot;returns {:ok, integer} for valid integer string&quot; do
      conn = conn(:get, &quot;/test?id=123&quot;) |&gt; fetch_query_params()
      assert {:ok, 123} = Helpers.parse_integer_param(conn, &quot;id&quot;)
    end

    test &quot;returns {:ok, integer} for valid integer string with leading zeros&quot; do
      conn = conn(:get, &quot;/test?id=000123&quot;) |&gt; fetch_query_params()
      assert {:ok, 123} = Helpers.parse_integer_param(conn, &quot;id&quot;)
    end

    test &quot;returns {:error, :invalid_id} for non-integer string&quot; do
      conn = conn(:get, &quot;/test?id=abc&quot;) |&gt; fetch_query_params()
      assert {:error, :invalid_id} = Helpers.parse_integer_param(conn, &quot;id&quot;)
    end

    test &quot;returns {:error, :invalid_id} for empty string&quot; do
      conn = conn(:get, &quot;/test?id=&quot;) |&gt; fetch_query_params()
      assert {:error, :invalid_id} = Helpers.parse_integer_param(conn, &quot;id&quot;)
    end

    test &quot;returns {:error, :invalid_id} for missing parameter&quot; do
      conn = conn(:get, &quot;/test&quot;) |&gt; fetch_query_params()
      assert {:error, :invalid_id} = Helpers.parse_integer_param(conn, &quot;id&quot;)
    end

    test &quot;returns {:error, :invalid_id} for integer with trailing characters&quot; do
      conn = conn(:get, &quot;/test?id=123abc&quot;) |&gt; fetch_query_params()
      assert {:error, :invalid_id} = Helpers.parse_integer_param(conn, &quot;id&quot;)
    end

    test &quot;returns {:ok, integer} for negative integer&quot; do
      conn = conn(:get, &quot;/test?id=-123&quot;) |&gt; fetch_query_params()
      assert {:ok, -123} = Helpers.parse_integer_param(conn, &quot;id&quot;)
    end

    test &quot;works with path parameters&quot; do
      conn = %Plug.Conn{params: %{&quot;id&quot; =&gt; &quot;456&quot;}}
      assert {:ok, 456} = Helpers.parse_integer_param(conn, &quot;id&quot;)
    end
  end

  describe &quot;send_json_resp/3&quot; do
    test &quot;sends JSON response with correct content type and status&quot; do
      conn = conn(:get, &quot;/test&quot;)
      data = %{message: &quot;hello&quot;, status: &quot;success&quot;}

      result = Helpers.send_json_resp(conn, 200, data)

      assert result.status == 200
      assert result.resp_body == Jason.encode!(data)
      assert [&quot;application/json; charset=utf-8&quot;] = get_resp_header(result, &quot;content-type&quot;)
    end

    test &quot;sends error JSON response&quot; do
      conn = conn(:get, &quot;/test&quot;)
      error_data = %{error: &quot;Not found&quot;, code: 404}

      result = Helpers.send_json_resp(conn, 404, error_data)

      assert result.status == 404
      assert result.resp_body == Jason.encode!(error_data)
      assert [&quot;application/json; charset=utf-8&quot;] = get_resp_header(result, &quot;content-type&quot;)
    end

    test &quot;handles complex nested data structures&quot; do
      conn = conn(:get, &quot;/test&quot;)

      complex_data = %{
        killmails: [
          %{id: 123, victim: %{ship_id: 456}, attackers: [%{char_id: 789}]},
          %{id: 124, victim: %{ship_id: 457}, attackers: [%{char_id: 790}]}
        ],
        meta: %{total: 2, page: 1}
      }

      result = Helpers.send_json_resp(conn, 200, complex_data)

      assert result.status == 200
      assert result.resp_body == Jason.encode!(complex_data)
      assert [&quot;application/json; charset=utf-8&quot;] = get_resp_header(result, &quot;content-type&quot;)
    end
  end
end</file><file path="test/integration/api_smoke_test.exs">defmodule WandererKills.ApiSmokeTest do
  use ExUnit.Case, async: false
  import Plug.Test

  @opts WandererKillsWeb.Api.init([])

  test &quot;GET /ping returns pong&quot; do
    conn = conn(:get, &quot;/ping&quot;)
    response = WandererKillsWeb.Api.call(conn, @opts)
    assert response.status == 200
    assert response.resp_body == &quot;pong&quot;
  end
end</file><file path="test/integration/api_test.exs">defmodule WandererKills.ApiTest do
  use ExUnit.Case, async: true
  import Plug.Test
  import Mox

  alias WandererKills.TestHelpers
  alias WandererKillsWeb.Api

  @opts Api.init([])

  setup do
    WandererKills.TestHelpers.clear_all_caches()
    TestHelpers.setup_http_mocks()
    :ok
  end

  setup :verify_on_exit!

  describe &quot;GET /ping&quot; do
    test &quot;returns pong&quot; do
      conn = conn(:get, &quot;/ping&quot;)
      conn = Api.call(conn, @opts)

      assert conn.status == 200
      assert conn.resp_body == &quot;pong&quot;
    end
  end

  describe &quot;GET /killmail/:id&quot; do
    test &quot;returns 400 for invalid ID&quot; do
      conn = conn(:get, &quot;/killmail/invalid&quot;)
      conn = Api.call(conn, @opts)

      assert conn.status == 400
      assert Jason.decode!(conn.resp_body)[&quot;error&quot;] == &quot;Invalid killmail ID&quot;
    end

    test &quot;returns 404 for non-existent killmail&quot; do
      # Mock the HTTP client that ZKB actually uses internally
      WandererKills.Http.Client.Mock
      |&gt; expect(:get_with_rate_limit, fn _url, _opts -&gt;
        # ZKB returns empty array for not found
        {:ok, %{status: 200, body: &quot;[]&quot;}}
      end)

      conn = conn(:get, &quot;/killmail/999999999&quot;)
      conn = Api.call(conn, @opts)

      assert conn.status == 404
      assert Jason.decode!(conn.resp_body)[&quot;error&quot;] == &quot;Killmail not found&quot;
    end
  end

  describe &quot;GET /system/:id/killmails&quot; do
    test &quot;returns 400 for invalid system ID&quot; do
      conn = conn(:get, &quot;/system/invalid/killmails&quot;)
      conn = Api.call(conn, @opts)

      assert conn.status == 400
      assert Jason.decode!(conn.resp_body)[&quot;error&quot;] == &quot;Invalid system ID&quot;
    end
  end

  describe &quot;GET /system_kill_count/:system_id&quot; do
    test &quot;returns 400 for invalid system ID&quot; do
      conn = conn(:get, &quot;/system_kill_count/invalid&quot;)
      conn = Api.call(conn, @opts)

      assert conn.status == 400
      assert Jason.decode!(conn.resp_body)[&quot;error&quot;] == &quot;Invalid system ID&quot;
    end
  end

  describe &quot;GET /kills_for_system/:system_id&quot; do
    test &quot;returns 400 for invalid system ID&quot; do
      conn = conn(:get, &quot;/kills_for_system/invalid&quot;)
      conn = Api.call(conn, @opts)

      assert conn.status == 400
      assert Jason.decode!(conn.resp_body)[&quot;error&quot;] == &quot;Invalid system ID&quot;
    end

    test &quot;redirects to /system_killmails/:system_id for valid ID&quot; do
      conn = conn(:get, &quot;/kills_for_system/123&quot;)
      conn = Api.call(conn, @opts)

      assert conn.status == 302
      assert Plug.Conn.get_resp_header(conn, &quot;location&quot;) == [&quot;/system_killmails/123&quot;]
    end
  end

  describe &quot;catch-all route&quot; do
    test &quot;returns 404 for unknown routes&quot; do
      conn = conn(:get, &quot;/unknown&quot;)
      conn = Api.call(conn, @opts)

      assert conn.status == 404
      assert Jason.decode!(conn.resp_body)[&quot;error&quot;] == &quot;Not found&quot;
    end
  end
end</file><file path="test/integration/cache_migration_test.exs">defmodule WandererKills.Integration.CacheMigrationTest do
  use ExUnit.Case, async: false
  use WandererKills.TestCase

  alias WandererKills.Cache.Helper
  alias WandererKills.ESI.DataFetcher

  describe &quot;Cachex migration integration tests&quot; do
    test &quot;ESI cache preserves character data with proper TTL&quot; do
      character_id = 123_456

      character_data = %{
        &quot;character_id&quot; =&gt; character_id,
        &quot;name&quot; =&gt; &quot;Test Character&quot;,
        &quot;corporation_id&quot; =&gt; 98_000_001
      }

      # Test cache miss then hit
      assert {:error, _} = Helper.character_get(character_id)

      # Put data and verify it can be retrieved
      assert {:ok, true} = Helper.character_put(character_id, character_data)
      assert {:ok, ^character_data} = Helper.character_get(character_id)

      # Verify cache namespace
      assert Helper.exists?(&quot;characters&quot;, character_id)
    end

    test &quot;ESI cache handles corporation data correctly&quot; do
      corporation_id = 98_000_002

      corporation_data = %{
        &quot;corporation_id&quot; =&gt; corporation_id,
        &quot;name&quot; =&gt; &quot;Test Corporation&quot;,
        &quot;ticker&quot; =&gt; &quot;TEST&quot;
      }

      # Test get_or_set functionality
      result =
        Helper.corporation_get_or_set(corporation_id, fn -&gt;
          corporation_data
        end)

      assert {:ok, ^corporation_data} = result

      # Verify it&apos;s now cached
      assert {:ok, ^corporation_data} = Helper.corporation_get(corporation_id)
    end

    test &quot;ESI cache handles alliance data correctly&quot; do
      alliance_id = 99_000_001

      alliance_data = %{
        &quot;alliance_id&quot; =&gt; alliance_id,
        &quot;name&quot; =&gt; &quot;Test Alliance&quot;,
        &quot;ticker&quot; =&gt; &quot;TESTA&quot;
      }

      # Test get_or_set functionality
      result =
        Helper.alliance_get_or_set(alliance_id, fn -&gt;
          alliance_data
        end)

      assert {:ok, ^alliance_data} = result

      # Verify it&apos;s now cached
      assert {:ok, ^alliance_data} = Helper.alliance_get(alliance_id)
    end

    test &quot;ship types cache preserves behavior&quot; do
      type_id = 587

      ship_data = %{
        &quot;type_id&quot; =&gt; type_id,
        &quot;name&quot; =&gt; &quot;Rifter&quot;,
        &quot;group_id&quot; =&gt; 25
      }

      # Test cache miss then hit
      assert {:error, _} = Helper.ship_type_get(type_id)

      # Put data and verify it can be retrieved
      assert {:ok, true} = Helper.ship_type_put(type_id, ship_data)
      assert {:ok, ^ship_data} = Helper.ship_type_get(type_id)

      # Test get_or_set functionality
      result =
        Helper.ship_type_get_or_set(type_id, fn -&gt;
          ship_data
        end)

      assert {:ok, ^ship_data} = result
    end

    test &quot;systems cache handles killmail associations correctly&quot; do
      system_id = 30_000_142
      killmail_ids = [12_345, 67_890, 54_321]

      # Test empty system initially - should return not found error
      assert {:error, _} = Helper.system_get_killmails(system_id)

      # Add killmails to system
      Enum.each(killmail_ids, fn killmail_id -&gt;
        assert {:ok, true} = Helper.system_add_killmail(system_id, killmail_id)
      end)

      # Verify killmails are associated
      assert {:ok, retrieved_ids} = Helper.system_get_killmails(system_id)
      assert length(retrieved_ids) == length(killmail_ids)

      # All original IDs should be present (order may vary)
      Enum.each(killmail_ids, fn id -&gt;
        assert id in retrieved_ids
      end)
    end

    test &quot;systems cache handles active systems correctly&quot; do
      system_id = 30_000_143

      # Add system to active list first
      assert {:ok, _} = Helper.system_add_active(system_id)

      # Test that the system is marked as active
      assert {:ok, true} = Helper.system_is_active?(system_id)

      # Note: get_active_systems() has a streaming issue in test environment
      # but the core functionality (is_active?) works correctly
    end

    test &quot;systems cache handles fetch timestamps correctly&quot; do
      system_id = 30_000_144
      timestamp = DateTime.utc_now()

      # Should not have timestamp initially
      assert {:error, _} = Helper.system_get_fetch_timestamp(system_id)

      # Set timestamp
      assert {:ok, _} = Helper.system_set_fetch_timestamp(system_id, timestamp)

      # Verify timestamp is retrieved correctly
      assert {:ok, retrieved_timestamp} = Helper.system_get_fetch_timestamp(system_id)

      # Allow small time difference due to serialization
      time_diff = DateTime.diff(timestamp, retrieved_timestamp, :millisecond)
      assert abs(time_diff) &lt; 1000
    end

    test &quot;systems cache handles recently fetched checks correctly&quot; do
      system_id = 30_000_145

      # Should not be recently fetched initially
      assert {:ok, false} = Helper.system_recently_fetched?(system_id)

      # Set current timestamp
      assert {:ok, _} = Helper.system_set_fetch_timestamp(system_id, DateTime.utc_now())

      # Should now be recently fetched (within default threshold)
      assert {:ok, true} = Helper.system_recently_fetched?(system_id)

      # Test with custom threshold
      assert {:ok, true} = Helper.system_recently_fetched?(system_id, 60)
    end

    test &quot;killmail cache handles individual killmails correctly&quot; do
      killmail_id = 98_765

      killmail_data = %{
        &quot;killmail_id&quot; =&gt; killmail_id,
        &quot;solar_system_id&quot; =&gt; 30_000_142,
        &quot;victim&quot; =&gt; %{&quot;character_id&quot; =&gt; 123_456}
      }

      # Test cache miss then hit
      assert {:error, _} = Helper.killmail_get(killmail_id)

      # Put data and verify it can be retrieved
      assert {:ok, true} = Helper.killmail_put(killmail_id, killmail_data)
      assert {:ok, ^killmail_data} = Helper.killmail_get(killmail_id)
    end

    test &quot;unified ESI DataFetcher works correctly&quot; do
      # Test character fetching
      character_id = 98_765_432
      character_data = %{&quot;character_id&quot; =&gt; character_id, &quot;name&quot; =&gt; &quot;DataFetcher Test&quot;}

      # Mock ESI response
      {:ok, true} = Helper.character_put(character_id, character_data)

      # Test DataFetcher behavior implementation
      assert {:ok, ^character_data} = DataFetcher.fetch({:character, character_id})
      assert DataFetcher.supports?({:character, character_id})
      refute DataFetcher.supports?({:unsupported, character_id})
    end

    test &quot;cache namespaces work correctly with Helper module&quot; do
      # Test different namespaces
      namespaces = [&quot;characters&quot;, &quot;corporations&quot;, &quot;alliances&quot;, &quot;ship_types&quot;, &quot;systems&quot;]

      Enum.each(namespaces, fn namespace -&gt;
        key = &quot;test_key_#{namespace}&quot;
        value = %{&quot;test&quot; =&gt; &quot;data&quot;, &quot;namespace&quot; =&gt; namespace}

        # Put and get should work
        assert {:ok, true} = Helper.put(namespace, key, value)
        assert {:ok, ^value} = Helper.get(namespace, key)
        assert true = Helper.exists?(namespace, key)

        # Delete should work
        assert {:ok, _} = Helper.delete(namespace, key)
        refute Helper.exists?(namespace, key)
      end)
    end

    test &quot;telemetry events are emitted for cache operations&quot; do
      # This test would require telemetry test helpers to capture events
      # For now, we verify the operations work without errors

      test_data = %{&quot;test&quot; =&gt; &quot;telemetry&quot;}

      # These operations should emit telemetry events
      assert {:ok, true} = Helper.put(&quot;test&quot;, &quot;telemetry_key&quot;, test_data)
      assert {:ok, ^test_data} = Helper.get(&quot;test&quot;, &quot;telemetry_key&quot;)

      # Miss should also emit telemetry (returns nil for missing keys)
      assert {:ok, nil} = Helper.get(&quot;test&quot;, &quot;nonexistent_key&quot;)
    end

    test &quot;cache stats work correctly&quot; do
      # Test basic stats functionality
      case Helper.stats() do
        {:ok, %{}} -&gt; :ok
        {:error, :stats_disabled} -&gt; :ok
        other -&gt; flunk(&quot;Unexpected stats result: #{inspect(other)}&quot;)
      end
    end

    test &quot;TTL functionality works correctly&quot; do
      key = &quot;ttl_test&quot;
      value = %{&quot;test&quot; =&gt; &quot;ttl&quot;}

      # Put with short TTL (test environment should respect this)
      assert {:ok, true} = Helper.put(&quot;test&quot;, key, value)

      # Should be immediately available
      assert {:ok, ^value} = Helper.get(&quot;test&quot;, key)

      # For longer integration test, we&apos;d wait for expiration
      # but for unit tests, we verify the structure works
    end
  end

  describe &quot;fallback function behavior&quot; do
    test &quot;ESI cache get_or_set fallback works correctly&quot; do
      character_id = 555_666
      fallback_data = %{&quot;character_id&quot; =&gt; character_id, &quot;name&quot; =&gt; &quot;Fallback Character&quot;}

      # Should call fallback on cache miss
      result =
        Helper.character_get_or_set(character_id, fn -&gt;
          fallback_data
        end)

      assert {:ok, ^fallback_data} = result

      # Should now be cached and not call fallback again
      assert {:ok, ^fallback_data} = Helper.character_get(character_id)
    end

    test &quot;ship types fallback preserves behavior&quot; do
      type_id = 999_888
      fallback_data = %{&quot;type_id&quot; =&gt; type_id, &quot;name&quot; =&gt; &quot;Fallback Ship&quot;}

      # Should call fallback on cache miss
      result =
        Helper.ship_type_get_or_set(type_id, fn -&gt;
          fallback_data
        end)

      assert {:ok, ^fallback_data} = result

      # Should now be cached
      assert {:ok, ^fallback_data} = Helper.ship_type_get(type_id)
    end
  end

  describe &quot;error handling preservation&quot; do
    test &quot;cache errors are handled gracefully&quot; do
      # Test with invalid data types where appropriate
      # Note: These functions have guard clauses that will raise FunctionClauseError
      # for invalid input types, which is the expected behavior

      # Test with non-existent valid IDs instead
      assert {:error, _} = Helper.character_get(999_999_999)
      assert {:error, _} = Helper.ship_type_get(999_999_999)
      assert {:error, _} = Helper.system_get_killmails(999_999_999)
    end

    test &quot;fallback function errors are handled correctly&quot; do
      character_id = 777_888

      # Fallback function that raises an error
      # Note: The current implementation has a bug where {:ignore, error} is not handled
      # This test documents the current behavior
      assert_raise CaseClauseError, fn -&gt;
        Helper.character_get_or_set(character_id, fn -&gt;
          raise &quot;Fallback error&quot;
        end)
      end
    end
  end
end</file><file path="test/killmails/store_test.exs">defmodule WandererKills.Killmails.StoreTest do
  use WandererKills.TestCase

  alias WandererKills.KillStore
  alias WandererKills.TestHelpers

  @system_id_1 30_000_142

  @test_killmail_1 %{
    &quot;killmail_id&quot; =&gt; 12_345,
    &quot;solar_system_id&quot; =&gt; @system_id_1,
    &quot;victim&quot; =&gt; %{&quot;character_id&quot; =&gt; 123},
    &quot;attackers&quot; =&gt; [],
    &quot;zkb&quot; =&gt; %{&quot;totalValue&quot; =&gt; 1000}
  }

  @test_killmail_2 %{
    &quot;killmail_id&quot; =&gt; 12_346,
    &quot;solar_system_id&quot; =&gt; @system_id_1,
    &quot;victim&quot; =&gt; %{&quot;character_id&quot; =&gt; 124},
    &quot;attackers&quot; =&gt; [],
    &quot;zkb&quot; =&gt; %{&quot;totalValue&quot; =&gt; 2000}
  }

  setup do
    WandererKills.TestHelpers.clear_all_caches()
    # Clear KillStore tables before each test
    KillStore.clear()
    :ok
  end

  describe &quot;killmail operations&quot; do
    test &quot;can store and retrieve a killmail&quot; do
      killmail = @test_killmail_1
      :ok = KillStore.put(12_345, @system_id_1, killmail)
      assert {:ok, ^killmail} = KillStore.get(12_345)
    end

    test &quot;returns error for non-existent killmail&quot; do
      assert :error = KillStore.get(999)
    end

    test &quot;can delete a killmail&quot; do
      killmail = TestHelpers.create_test_killmail(123)
      :ok = KillStore.put(123, @system_id_1, killmail)
      :ok = KillStore.delete(123)
      assert :error = KillStore.get(123)
    end
  end

  describe &quot;system operations&quot; do
    test &quot;can store and retrieve system killmails&quot; do
      killmail1 = Map.put(@test_killmail_1, &quot;killmail_id&quot;, 123)
      killmail2 = Map.put(@test_killmail_2, &quot;killmail_id&quot;, 456)

      assert :ok = KillStore.put(123, @system_id_1, killmail1)
      assert :ok = KillStore.put(456, @system_id_1, killmail2)

      killmails = KillStore.list_by_system(@system_id_1)
      killmail_ids = Enum.map(killmails, &amp; &amp;1[&quot;killmail_id&quot;])
      assert Enum.sort(killmail_ids) == [123, 456]
    end

    test &quot;returns empty list for system with no killmails&quot; do
      killmails = KillStore.list_by_system(@system_id_1)
      assert killmails == []
    end

    test &quot;can remove killmail from system&quot; do
      killmail = Map.put(@test_killmail_1, &quot;killmail_id&quot;, 123)
      assert :ok = KillStore.put(123, @system_id_1, killmail)
      assert :ok = KillStore.delete(123)

      killmails = KillStore.list_by_system(@system_id_1)
      assert killmails == []
    end
  end

  describe &quot;edge cases&quot; do
    test &quot;handles non-existent system&quot; do
      non_existent_system = 99_999_999

      # Fetch for non-existent system should return empty list
      killmails = KillStore.list_by_system(non_existent_system)
      assert killmails == []
    end

    test &quot;handles multiple systems correctly&quot; do
      system_2 = 30_000_143

      killmail1 = Map.put(@test_killmail_1, &quot;killmail_id&quot;, 123)
      killmail2 = Map.put(@test_killmail_2, &quot;killmail_id&quot;, 456)

      # Store killmails in different systems
      assert :ok = KillStore.put(123, @system_id_1, killmail1)
      assert :ok = KillStore.put(456, system_2, killmail2)

      # Each system should only return its own killmails
      system_1_killmails = KillStore.list_by_system(@system_id_1)
      system_2_killmails = KillStore.list_by_system(system_2)

      assert length(system_1_killmails) == 1
      assert length(system_2_killmails) == 1
      assert hd(system_1_killmails)[&quot;killmail_id&quot;] == 123
      assert hd(system_2_killmails)[&quot;killmail_id&quot;] == 456
    end

    test &quot;handles killmail updates correctly&quot; do
      killmail = @test_killmail_1
      updated_killmail = Map.put(killmail, &quot;updated&quot;, true)

      # Store initial killmail
      assert :ok = KillStore.put(12_345, @system_id_1, killmail)
      assert {:ok, ^killmail} = KillStore.get(12_345)

      # Update with new data
      assert :ok = KillStore.put(12_345, @system_id_1, updated_killmail)
      assert {:ok, ^updated_killmail} = KillStore.get(12_345)
    end
  end
end</file><file path="test/shared/cache_key_test.exs">defmodule WandererKills.CacheKeyTest do
  # Disable async to avoid cache interference
  use ExUnit.Case, async: false
  alias WandererKills.Cache.Helper
  alias WandererKills.TestHelpers

  setup do
    TestHelpers.clear_all_caches()

    on_exit(fn -&gt;
      TestHelpers.clear_all_caches()
    end)
  end

  describe &quot;cache key patterns&quot; do
    test &quot;killmail keys follow expected pattern&quot; do
      # Test that the cache operations use consistent key patterns
      killmail_data = %{&quot;killmail_id&quot; =&gt; 123, &quot;solar_system_id&quot; =&gt; 456}

      # Store and retrieve to verify key pattern works
      assert {:ok, true} = Helper.killmail_put(123, killmail_data)
      assert {:ok, ^killmail_data} = Helper.killmail_get(123)
      assert {:ok, true} = Helper.killmail_delete(123)

      assert {:error, :not_found} = Helper.killmail_get(123)
    end

    test &quot;system keys follow expected pattern&quot; do
      # Test system-related cache operations
      assert {:ok, :added} = Helper.system_add_active(456)
      # Note: get_active_systems() has streaming issues in test environment

      # No killmails initially
      assert {:error, _} = Helper.system_get_killmails(456)
      assert {:ok, true} = Helper.system_add_killmail(456, 123)
      assert {:ok, [123]} = Helper.system_get_killmails(456)

      assert {:ok, 0} = Helper.system_get_kill_count(456)
      assert {:ok, 1} = Helper.system_increment_kill_count(456)
      assert {:ok, 1} = Helper.system_get_kill_count(456)
    end

    test &quot;esi keys follow expected pattern&quot; do
      character_data = %{&quot;character_id&quot; =&gt; 123, &quot;name&quot; =&gt; &quot;Test Character&quot;}
      corporation_data = %{&quot;corporation_id&quot; =&gt; 456, &quot;name&quot; =&gt; &quot;Test Corp&quot;}
      alliance_data = %{&quot;alliance_id&quot; =&gt; 789, &quot;name&quot; =&gt; &quot;Test Alliance&quot;}
      type_data = %{&quot;type_id&quot; =&gt; 101, &quot;name&quot; =&gt; &quot;Test Type&quot;}
      group_data = %{&quot;group_id&quot; =&gt; 102, &quot;name&quot; =&gt; &quot;Test Group&quot;}

      # Test ESI cache operations - verify set operations work
      assert {:ok, true} = Helper.character_put(123, character_data)
      assert {:ok, true} = Helper.corporation_put(456, corporation_data)
      assert {:ok, true} = Helper.alliance_put(789, alliance_data)
      assert {:ok, true} = Helper.ship_type_put(101, type_data)
      assert {:ok, true} = Helper.put(&quot;groups&quot;, &quot;102&quot;, group_data)

      # Verify retrieval works using unified interface
      case Helper.character_get(123) do
        {:ok, ^character_data} -&gt; :ok
        # Acceptable in test environment
        {:error, %WandererKills.Infrastructure.Error{type: :not_found}} -&gt; :ok
        {:error, _} -&gt; :ok
      end
    end
  end

  describe &quot;cache functionality&quot; do
    test &quot;basic cache operations work correctly&quot; do
      key = &quot;test:key&quot;
      value = %{&quot;test&quot; =&gt; &quot;data&quot;}

      # Use Helper cache for basic operations
      assert {:ok, nil} = Helper.get(&quot;esi&quot;, key)

      assert {:ok, true} = Helper.put(&quot;esi&quot;, key, value)
      assert {:ok, ^value} = Helper.get(&quot;esi&quot;, key)
      assert {:ok, _} = Helper.delete(&quot;esi&quot;, key)

      assert {:ok, nil} = Helper.get(&quot;esi&quot;, key)
    end

    test &quot;system fetch timestamp operations work&quot; do
      # Use a unique system ID to avoid conflicts with other tests
      system_id = 99_789_123
      timestamp = DateTime.utc_now()

      # Ensure cache is completely clear for this specific system
      TestHelpers.clear_all_caches()

      assert {:ok, false} = Helper.system_recently_fetched?(system_id)
      assert {:ok, :set} = Helper.system_set_fetch_timestamp(system_id, timestamp)
      assert {:ok, true} = Helper.system_recently_fetched?(system_id)
    end
  end

  describe &quot;cache health and stats&quot; do
    test &quot;cache reports as healthy&quot; do
      # Test that caches are accessible - stats may not be available in test env
      case Helper.stats() do
        {:ok, _} -&gt; :ok
        {:error, :stats_disabled} -&gt; :ok
      end
    end

    test &quot;cache stats are retrievable&quot; do
      # stats may not be available in test environment
      case Helper.stats() do
        {:ok, stats} when is_map(stats) -&gt; :ok
        {:error, :stats_disabled} -&gt; :ok
      end
    end
  end
end</file><file path="test/shared/cache_test.exs">defmodule WandererKills.CacheTest do
  use WandererKills.TestCase

  alias WandererKills.Cache.Helper
  alias WandererKills.TestHelpers

  setup do
    WandererKills.TestHelpers.clear_all_caches()
    :ok
  end

  describe &quot;killmail operations&quot; do
    test &quot;can store and retrieve a killmail&quot; do
      killmail = TestHelpers.create_test_killmail(123)
      assert {:ok, true} = Helper.killmail_put(123, killmail)
      assert {:ok, ^killmail} = Helper.killmail_get(123)
    end

    test &quot;returns error for non-existent killmail&quot; do
      assert {:error, :not_found} = Helper.killmail_get(999)
    end

    test &quot;can delete a killmail&quot; do
      killmail = TestHelpers.create_test_killmail(123)
      assert {:ok, true} = Helper.killmail_put(123, killmail)
      assert {:ok, true} = Helper.killmail_delete(123)

      assert {:error, :not_found} = Helper.killmail_get(123)
    end
  end

  describe &quot;system operations&quot; do
    test &quot;can store and retrieve system killmails&quot; do
      killmail1 = TestHelpers.create_test_killmail(123)
      killmail2 = TestHelpers.create_test_killmail(456)

      assert {:ok, true} = Helper.killmail_put(123, killmail1)
      assert {:ok, true} = Helper.killmail_put(456, killmail2)
      assert {:ok, true} = Helper.system_add_killmail(789, 123)
      assert {:ok, true} = Helper.system_add_killmail(789, 456)
      assert {:ok, killmail_ids} = Helper.system_get_killmails(789)
      assert 123 in killmail_ids
      assert 456 in killmail_ids
    end

    test &quot;returns error for system with no killmails&quot; do
      assert {:error, _} = Helper.system_get_killmails(999)
    end

    test &quot;can manage system killmails&quot; do
      killmail = TestHelpers.create_test_killmail(123)
      assert {:ok, true} = Helper.killmail_put(123, killmail)
      assert {:ok, true} = Helper.system_add_killmail(789, 123)
      assert {:ok, killmail_ids} = Helper.system_get_killmails(789)
      assert 123 in killmail_ids
      assert {:ok, killmail_ids} = Helper.system_get_killmails(789)
      assert 123 in killmail_ids
    end
  end

  describe &quot;kill count operations&quot; do
    test &quot;can increment and get system kill count&quot; do
      assert {:ok, 1} = Helper.system_increment_kill_count(789)
      assert {:ok, 2} = Helper.system_increment_kill_count(789)
      assert {:ok, 2} = Helper.system_get_kill_count(789)
    end

    test &quot;returns 0 for system with no kills&quot; do
      assert {:ok, 0} = Helper.system_get_kill_count(999)
    end
  end

  describe &quot;fetch timestamp operations&quot; do
    test &quot;can set and check system fetch timestamp&quot; do
      timestamp = DateTime.utc_now()
      assert {:ok, :set} = Helper.system_set_fetch_timestamp(789, timestamp)
      assert {:ok, true} = Helper.system_recently_fetched?(789)
    end

    test &quot;returns false for system with no fetch timestamp&quot; do
      assert {:ok, false} = Helper.system_recently_fetched?(999)
    end
  end
end</file><file path="test/shared/csv_test.exs">defmodule WandererKills.ShipTypes.CSVTest do
  use ExUnit.Case, async: true

  alias WandererKills.ShipTypes.CSV

  describe &quot;read_file/3&quot; do
    test &quot;handles missing file&quot; do
      parser = fn _row -&gt; %{id: 1, name: &quot;test&quot;} end

      result = CSV.read_file(&quot;nonexistent.csv&quot;, parser)
      assert {:error, _reason} = result
    end

    test &quot;handles empty file&quot; do
      parser = fn _row -&gt; %{id: 1, name: &quot;test&quot;} end
      file_path = &quot;test/fixtures/empty.csv&quot;
      File.mkdir_p!(Path.dirname(file_path))
      File.write!(file_path, &quot;&quot;)

      result = CSV.read_file(file_path, parser)
      assert {:ok, []} = result

      File.rm!(file_path)
    end

    test &quot;handles invalid CSV&quot; do
      parser = fn _row -&gt; %{id: 1, name: &quot;test&quot;} end
      file_path = &quot;test/fixtures/invalid.csv&quot;
      File.mkdir_p!(Path.dirname(file_path))
      File.write!(file_path, &quot;invalid\&quot;csv,\&quot;content\nunclosed\&quot;quote&quot;)

      result = CSV.read_file(file_path, parser)
      assert {:error, %WandererKills.Infrastructure.Error{type: :parse_error}} = result

      File.rm!(file_path)
    end

    test &quot;parses valid CSV&quot; do
      parser = fn row -&gt; %{id: String.to_integer(row[&quot;id&quot;]), name: row[&quot;name&quot;]} end
      file_path = &quot;test/fixtures/valid.csv&quot;
      File.mkdir_p!(Path.dirname(file_path))
      File.write!(file_path, &quot;id,name\n1,test1\n2,test2&quot;)

      result = CSV.read_file(file_path, parser)
      assert {:ok, records} = result
      assert length(records) == 2
      assert records == [%{id: 1, name: &quot;test1&quot;}, %{id: 2, name: &quot;test2&quot;}]

      File.rm!(file_path)
    end
  end

  describe &quot;parse_row/2&quot; do
    test &quot;creates map from headers and row data&quot; do
      headers = [&quot;id&quot;, &quot;name&quot;, &quot;value&quot;]
      row = [&quot;1&quot;, &quot;test&quot;, &quot;100&quot;]

      result = CSV.parse_row(row, headers)
      assert result == %{&quot;id&quot; =&gt; &quot;1&quot;, &quot;name&quot; =&gt; &quot;test&quot;, &quot;value&quot; =&gt; &quot;100&quot;}
    end
  end

  describe &quot;parse_integer/1&quot; do
    test &quot;parses valid integers&quot; do
      assert {:ok, 123} = CSV.parse_integer(&quot;123&quot;)
      assert {:ok, 0} = CSV.parse_integer(&quot;0&quot;)
      assert {:ok, -45} = CSV.parse_integer(&quot;-45&quot;)
    end

    test &quot;handles invalid integers&quot; do
      assert {:error, %WandererKills.Infrastructure.Error{type: :invalid_integer}} =
               CSV.parse_integer(&quot;abc&quot;)

      assert {:error, %WandererKills.Infrastructure.Error{type: :invalid_integer}} =
               CSV.parse_integer(&quot;12.5&quot;)

      assert {:error, %WandererKills.Infrastructure.Error{type: :missing_value}} =
               CSV.parse_integer(&quot;&quot;)

      assert {:error, %WandererKills.Infrastructure.Error{type: :missing_value}} =
               CSV.parse_integer(nil)
    end
  end

  describe &quot;parse_float/1&quot; do
    test &quot;parses valid floats&quot; do
      assert {:ok, 123.45} = CSV.parse_float(&quot;123.45&quot;)
      assert {:ok, +0.0} = CSV.parse_float(&quot;0.0&quot;)
      assert {:ok, -12.34} = CSV.parse_float(&quot;-12.34&quot;)
    end

    test &quot;handles invalid floats&quot; do
      assert {:error, %WandererKills.Infrastructure.Error{type: :invalid_float}} =
               CSV.parse_float(&quot;abc&quot;)

      assert {:error, %WandererKills.Infrastructure.Error{type: :missing_value}} =
               CSV.parse_float(&quot;&quot;)

      assert {:error, %WandererKills.Infrastructure.Error{type: :missing_value}} =
               CSV.parse_float(nil)
    end
  end

  describe &quot;parse_number_with_default/3&quot; do
    test &quot;parses valid floats&quot; do
      assert 123.45 = CSV.parse_number_with_default(&quot;123.45&quot;, :float, 0.0)
      assert +0.0 = CSV.parse_number_with_default(&quot;0.0&quot;, :float, 0.0)
    end

    test &quot;returns default for invalid floats&quot; do
      assert +0.0 = CSV.parse_number_with_default(&quot;abc&quot;, :float, 0.0)
      assert 5.0 = CSV.parse_number_with_default(&quot;invalid&quot;, :float, 5.0)
    end
  end
end</file><file path="test/shared/http_util_test.exs">defmodule WandererKills.Http.UtilTest do
  use ExUnit.Case, async: true

  alias WandererKills.Http.Client

  setup do
    WandererKills.TestHelpers.setup_mocks()
    :ok
  end

  describe &quot;retriable_error?/1&quot; do
    test &quot;returns true for retriable errors&quot; do
      # Note: retriable_error? was removed with Http.Util consolidation
      # These tests are now obsolete as the functionality was simplified
      # Placeholder - this functionality was removed
      assert true
    end

    test &quot;returns false for non-retriable errors&quot; do
      # Note: retriable_error? was removed with Http.Util consolidation
      # These tests are now obsolete as the functionality was simplified
      # Placeholder - this functionality was removed
      assert true
    end
  end

  describe &quot;handle_status_code/2&quot; do
    test &quot;handles success responses&quot; do
      result = Client.handle_status_code(200, %{&quot;test&quot; =&gt; &quot;data&quot;})
      assert {:ok, %{&quot;test&quot; =&gt; &quot;data&quot;}} = result
    end

    test &quot;handles not found responses&quot; do
      result = Client.handle_status_code(404, %{})
      assert {:error, :not_found} = result
    end

    test &quot;handles rate limited responses&quot; do
      result = Client.handle_status_code(429, %{})
      assert {:error, :rate_limited} = result
    end

    test &quot;handles other error responses&quot; do
      result = Client.handle_status_code(500, %{})
      assert {:error, _} = result
    end
  end
end</file><file path="test/support/helpers.ex">defmodule WandererKills.TestHelpers do
  @moduledoc &quot;&quot;&quot;
  Consolidated test helper functions for the WandererKills application.

  This module combines functionality from multiple test helper files:
  - Cache management and mocking
  - HTTP mocking and response generation
  - Test data factories and utilities
  - Setup and cleanup functions

  ## Features

  - Unified cache clearing functionality
  - HTTP client mocking and response generation
  - Test data factories for killmails, ESI data, etc.
  - Common assertions and test utilities
  - Test environment setup and teardown

  ## Usage

  ```elixir
  defmodule MyTest do
    use ExUnit.Case
    import WandererKills.TestHelpers

    setup do
      clear_all_caches()
      setup_mocks()
      :ok
    end
  end
  ```
  &quot;&quot;&quot;

  import ExUnit.Assertions

  #
  # Cache Management Functions
  #

  @doc &quot;&quot;&quot;
  Cleans up any existing processes before tests.
  &quot;&quot;&quot;
  def cleanup_processes do
    # Clear KillStore ETS tables
    WandererKills.Killmails.Store.cleanup_tables()

    # Clear test caches
    Cachex.clear(:killmails_cache_test)
    Cachex.clear(:system_cache_test)
    Cachex.clear(:esi_cache_test)

    :ok
  end

  @doc &quot;&quot;&quot;
  Clears all caches used in the application.
  &quot;&quot;&quot;
  @spec clear_all_caches() :: :ok
  def clear_all_caches do
    # Clear test-specific caches
    clear_test_caches()

    # Clear production caches (if they exist and are running)
    clear_production_caches()

    # Clear any additional caches
    clear_additional_caches()

    :ok
  end

  @doc &quot;&quot;&quot;
  Clears only the test-specific cache instances.
  &quot;&quot;&quot;
  @spec clear_test_caches() :: :ok
  def clear_test_caches do
    # Clear the unified cache used in both test and production environments
    safe_clear_cache(:unified_cache)
    :ok
  end

  @doc &quot;&quot;&quot;
  Clears production cache instances (used when tests run against production caches).
  &quot;&quot;&quot;
  @spec clear_production_caches() :: :ok
  def clear_production_caches do
    safe_clear_cache(:wanderer_cache)
    :ok
  end

  @doc &quot;&quot;&quot;
  Clears additional caches that may be used in some tests.
  &quot;&quot;&quot;
  @spec clear_additional_caches() :: :ok
  def clear_additional_caches do
    safe_clear_cache(:active_systems_cache)

    # Clear namespace-specific caches from CacheHelpers
    additional_cache_names = [
      :esi,
      :ship_types,
      :systems,
      :killmails,
      :characters,
      :corporations,
      :alliances
    ]

    Enum.each(additional_cache_names, &amp;safe_clear_cache/1)
    :ok
  end

  # Private helper function that safely clears a cache, ignoring errors
  @spec safe_clear_cache(atom()) :: :ok
  defp safe_clear_cache(cache_name) do
    case Cachex.clear(cache_name) do
      {:ok, _} -&gt; :ok
      # Ignore errors (cache might not exist)
      {:error, _} -&gt; :ok
    end
  catch
    # Ignore process exit errors
    :exit, _ -&gt; :ok
    # Ignore any other errors
    _, _ -&gt; :ok
  end

  @doc &quot;&quot;&quot;
  Sets up cache for testing with mock data.
  &quot;&quot;&quot;
  @spec setup_cache_test() :: :ok
  def setup_cache_test do
    clear_all_caches()
    :ok
  end

  @doc &quot;&quot;&quot;
  Asserts that a cache operation was successful.
  &quot;&quot;&quot;
  @spec assert_cache_success(term(), term()) :: :ok
  def assert_cache_success(result, expected_value \\ nil) do
    case result do
      {:ok, value} -&gt;
        if expected_value, do: assert(value == expected_value)
        :ok

      :ok -&gt;
        :ok

      other -&gt;
        flunk(&quot;Expected cache operation to succeed, got: #{inspect(other)}&quot;)
    end
  end

  @doc &quot;&quot;&quot;
  Sets up cache entries for testing.

  ## Parameters
  - `cache_name` - The cache namespace to use
  - `entries` - A list of {key, value} tuples to insert

  ## Examples

      setup_cache_entries(:ship_types, [
        {&quot;ship_type:123&quot;, %{name: &quot;Rifter&quot;}},
        {&quot;ship_type:456&quot;, %{name: &quot;Crow&quot;}}
      ])
  &quot;&quot;&quot;
  @spec setup_cache_entries(atom(), [{String.t(), term()}]) :: :ok
  def setup_cache_entries(cache_name, entries) when is_list(entries) do
    Enum.each(entries, fn {key, value} -&gt;
      Cachex.put(cache_name, key, value)
    end)

    :ok
  end

  @doc &quot;&quot;&quot;
  Gets a value from cache for testing assertions.
  &quot;&quot;&quot;
  @spec get_cache_value(atom(), String.t()) :: {:ok, term()} | {:error, term()}
  def get_cache_value(cache_name, key) do
    case Cachex.get(cache_name, key) do
      {:ok, nil} -&gt; {:error, :not_found}
      {:ok, value} -&gt; {:ok, value}
      error -&gt; error
    end
  end

  @doc &quot;&quot;&quot;
  Checks if a cache entry exists.
  &quot;&quot;&quot;
  @spec cache_exists?(atom(), String.t()) :: boolean()
  def cache_exists?(cache_name, key) do
    case Cachex.exists?(cache_name, key) do
      {:ok, exists?} -&gt; exists?
      {:error, _} -&gt; false
    end
  end

  @doc &quot;&quot;&quot;
  Gets the size of a cache for testing assertions.
  &quot;&quot;&quot;
  @spec cache_size(atom()) :: {:ok, non_neg_integer()} | {:error, term()}
  def cache_size(cache_name) do
    case Cachex.size(cache_name) do
      {:ok, size} -&gt; {:ok, size}
      error -&gt; error
    end
  end

  @doc &quot;&quot;&quot;
  Verifies cache state matches expected values.

  ## Parameters
  - `cache_name` - The cache namespace to check
  - `expected` - A map of key =&gt; value pairs that should exist

  Returns `:ok` if all expected entries exist with correct values,
  or `{:error, details}` if there are mismatches.
  &quot;&quot;&quot;
  @spec verify_cache_state(atom(), %{String.t() =&gt; term()}) :: :ok | {:error, term()}
  def verify_cache_state(cache_name, expected) when is_map(expected) do
    mismatches =
      Enum.reduce(expected, [], fn {key, expected_value}, acc -&gt;
        case get_cache_value(cache_name, key) do
          {:ok, ^expected_value} -&gt;
            acc

          {:ok, actual_value} -&gt;
            [{:value_mismatch, key, expected_value, actual_value} | acc]

          {:error, :not_found} -&gt;
            [{:missing_key, key, expected_value} | acc]

          {:error, reason} -&gt;
            [{:cache_error, key, reason} | acc]
        end
      end)

    case mismatches do
      [] -&gt; :ok
      mismatches -&gt; {:error, mismatches}
    end
  end

  #
  # HTTP Mocking Functions
  #

  @doc &quot;&quot;&quot;
  Sets up default mocks for HTTP client and other services.
  &quot;&quot;&quot;
  @spec setup_mocks() :: :ok
  def setup_mocks do
    setup_http_mocks()
    :ok
  end

  @doc &quot;&quot;&quot;
  Sets up HTTP client mocks with default responses.
  &quot;&quot;&quot;
  @spec setup_http_mocks() :: :ok
  def setup_http_mocks do
    :ok
  end

  @doc &quot;&quot;&quot;
  Creates a mock HTTP response with given status and body.
  &quot;&quot;&quot;
  @spec mock_http_response(integer(), term()) :: {:ok, map()} | {:error, term()}
  def mock_http_response(status, body \\ nil) do
    case status do
      200 -&gt; {:ok, %{status: 200, body: body || %{}}}
      404 -&gt; {:error, :not_found}
      429 -&gt; {:error, :rate_limited}
      500 -&gt; {:error, :server_error}
      _ -&gt; {:error, &quot;HTTP #{status}&quot;}
    end
  end

  @doc &quot;&quot;&quot;
  Expects an HTTP request to succeed with specific response body.
  &quot;&quot;&quot;
  @spec expect_http_success(String.t(), map()) :: :ok
  def expect_http_success(_url_pattern, _response_body) do
    :ok
  end

  @doc &quot;&quot;&quot;
  Expects an HTTP request to be rate limited.
  &quot;&quot;&quot;
  @spec expect_http_rate_limit(String.t(), non_neg_integer()) :: :ok
  def expect_http_rate_limit(_url_pattern, _retry_count \\ 3) do
    :ok
  end

  @doc &quot;&quot;&quot;
  Expects an HTTP request to fail with specific error.
  &quot;&quot;&quot;
  @spec expect_http_error(String.t(), atom()) :: :ok
  def expect_http_error(_url_pattern, _error_type) do
    :ok
  end

  @doc &quot;&quot;&quot;
  Asserts that an HTTP response has expected status and body keys.
  &quot;&quot;&quot;
  @spec assert_http_response(map(), integer(), [String.t()]) :: :ok
  def assert_http_response(response, expected_status, expected_body_keys \\ []) do
    assert %{status: ^expected_status} = response

    if expected_body_keys != [] do
      for key &lt;- expected_body_keys do
        assert Map.has_key?(response.body, key), &quot;Response body missing key: #{key}&quot;
      end
    end

    :ok
  end

  #
  # Test Data Generation Functions
  #

  @doc &quot;&quot;&quot;
  Generates test data for various entity types.
  &quot;&quot;&quot;
  @spec generate_test_data(atom(), integer() | nil) :: map()
  def generate_test_data(entity_type, id \\ nil)

  def generate_test_data(:killmail, killmail_id) do
    killmail_id = killmail_id || random_killmail_id()

    %{
      &quot;killmail_id&quot; =&gt; killmail_id,
      &quot;killmail_time&quot; =&gt; &quot;2024-01-01T12:00:00Z&quot;,
      &quot;solar_system_id&quot; =&gt; random_system_id(),
      &quot;victim&quot; =&gt; %{
        &quot;character_id&quot; =&gt; random_character_id(),
        &quot;corporation_id&quot; =&gt; 98_000_001,
        &quot;alliance_id&quot; =&gt; 99_000_001,
        &quot;faction_id&quot; =&gt; nil,
        &quot;ship_type_id&quot; =&gt; 670,
        &quot;damage_taken&quot; =&gt; 1000
      },
      &quot;attackers&quot; =&gt; [
        %{
          &quot;character_id&quot; =&gt; random_character_id(),
          &quot;corporation_id&quot; =&gt; 98_000_002,
          &quot;alliance_id&quot; =&gt; 99_000_002,
          &quot;faction_id&quot; =&gt; nil,
          &quot;ship_type_id&quot; =&gt; 671,
          &quot;weapon_type_id&quot; =&gt; 2456,
          &quot;damage_done&quot; =&gt; 1000,
          &quot;final_blow&quot; =&gt; true,
          &quot;security_status&quot; =&gt; 5.0
        }
      ]
    }
  end

  def generate_test_data(:character, character_id) do
    character_id = character_id || random_character_id()

    %{
      &quot;character_id&quot; =&gt; character_id,
      &quot;name&quot; =&gt; &quot;Test Character #{character_id}&quot;,
      &quot;corporation_id&quot; =&gt; 98_000_001,
      &quot;alliance_id&quot; =&gt; 99_000_001,
      &quot;faction_id&quot; =&gt; nil,
      &quot;security_status&quot; =&gt; 5.0
    }
  end

  def generate_test_data(:corporation, corporation_id) do
    corporation_id = corporation_id || 98_000_001

    %{
      &quot;corporation_id&quot; =&gt; corporation_id,
      &quot;name&quot; =&gt; &quot;Test Corp #{corporation_id}&quot;,
      &quot;ticker&quot; =&gt; &quot;TEST&quot;,
      &quot;member_count&quot; =&gt; 100,
      &quot;alliance_id&quot; =&gt; 99_000_001,
      &quot;ceo_id&quot; =&gt; random_character_id()
    }
  end

  def generate_test_data(:alliance, alliance_id) do
    alliance_id = alliance_id || 99_000_001

    %{
      &quot;alliance_id&quot; =&gt; alliance_id,
      &quot;name&quot; =&gt; &quot;Test Alliance #{alliance_id}&quot;,
      &quot;ticker&quot; =&gt; &quot;TEST&quot;,
      &quot;creator_corporation_id&quot; =&gt; 98_000_001,
      &quot;creator_id&quot; =&gt; random_character_id(),
      &quot;date_founded&quot; =&gt; &quot;2024-01-01T00:00:00Z&quot;,
      &quot;executor_corporation_id&quot; =&gt; 98_000_001
    }
  end

  def generate_test_data(:type, type_id) do
    type_id = type_id || 670

    %{
      &quot;type_id&quot; =&gt; type_id,
      &quot;name&quot; =&gt; &quot;Test Type #{type_id}&quot;,
      &quot;description&quot; =&gt; &quot;A test type for unit testing&quot;,
      &quot;group_id&quot; =&gt; 25,
      &quot;market_group_id&quot; =&gt; 1,
      &quot;mass&quot; =&gt; 1000.0,
      &quot;packaged_volume&quot; =&gt; 500.0,
      &quot;portion_size&quot; =&gt; 1,
      &quot;published&quot; =&gt; true,
      &quot;radius&quot; =&gt; 100.0,
      &quot;volume&quot; =&gt; 500.0
    }
  end

  def generate_test_data(:system, system_id) do
    system_id = system_id || random_system_id()

    %{
      &quot;system_id&quot; =&gt; system_id,
      &quot;name&quot; =&gt; &quot;Test System #{system_id}&quot;,
      &quot;constellation_id&quot; =&gt; 20_000_001,
      &quot;security_status&quot; =&gt; 0.5,
      &quot;star_id&quot; =&gt; 40_000_001
    }
  end

  @doc &quot;&quot;&quot;
  Generates ZKB-style response data.
  &quot;&quot;&quot;
  @spec generate_zkb_response(atom(), non_neg_integer()) :: map()
  def generate_zkb_response(type, count \\ 1)

  def generate_zkb_response(:killmail, count) do
    killmails = for _ &lt;- 1..count, do: generate_test_data(:killmail)
    killmails
  end

  def generate_zkb_response(:system_killmails, count) do
    system_id = random_system_id()

    killmails =
      for _ &lt;- 1..count do
        killmail = generate_test_data(:killmail)
        put_in(killmail[&quot;solar_system_id&quot;], system_id)
      end

    killmails
  end

  @doc &quot;&quot;&quot;
  Generates ESI-style response data.
  &quot;&quot;&quot;
  @spec generate_esi_response(atom(), integer()) :: map()
  def generate_esi_response(type, id) do
    generate_test_data(type, id)
  end

  @doc &quot;&quot;&quot;
  Creates a test killmail with specific ID.
  &quot;&quot;&quot;
  @spec create_test_killmail(integer()) :: map()
  def create_test_killmail(killmail_id) do
    generate_test_data(:killmail, killmail_id)
  end

  @doc &quot;&quot;&quot;
  Creates test ESI data for different entity types.
  &quot;&quot;&quot;
  @spec create_test_esi_data(atom(), integer(), keyword()) :: map()
  def create_test_esi_data(type, id, opts \\ [])

  def create_test_esi_data(:character, character_id, opts) do
    base_data = generate_test_data(:character, character_id)

    Enum.reduce(opts, base_data, fn {key, value}, acc -&gt;
      Map.put(acc, to_string(key), value)
    end)
  end

  def create_test_esi_data(:corporation, corporation_id, opts) do
    base_data = generate_test_data(:corporation, corporation_id)

    Enum.reduce(opts, base_data, fn {key, value}, acc -&gt;
      Map.put(acc, to_string(key), value)
    end)
  end

  def create_test_esi_data(:alliance, alliance_id, opts) do
    base_data = generate_test_data(:alliance, alliance_id)

    Enum.reduce(opts, base_data, fn {key, value}, acc -&gt;
      Map.put(acc, to_string(key), value)
    end)
  end

  def create_test_esi_data(:type, type_id, opts) do
    base_data = generate_test_data(:type, type_id)

    Enum.reduce(opts, base_data, fn {key, value}, acc -&gt;
      Map.put(acc, to_string(key), value)
    end)
  end

  #
  # Random ID Generators
  #

  @doc &quot;&quot;&quot;
  Generates a random system ID.
  &quot;&quot;&quot;
  @spec random_system_id() :: integer()
  def random_system_id do
    Enum.random(30_000_001..30_005_000)
  end

  @doc &quot;&quot;&quot;
  Generates a random character ID.
  &quot;&quot;&quot;
  @spec random_character_id() :: integer()
  def random_character_id do
    Enum.random(90_000_001..99_999_999)
  end

  @doc &quot;&quot;&quot;
  Generates a random killmail ID.
  &quot;&quot;&quot;
  @spec random_killmail_id() :: integer()
  def random_killmail_id do
    Enum.random(100_000_001..999_999_999)
  end

  #
  # Utility Functions
  #

  @doc &quot;&quot;&quot;
  Cleans up KillStore data for testing.
  &quot;&quot;&quot;
  @spec stop_killmail_store() :: :ok
  def stop_killmail_store do
    WandererKills.Killmails.Store.cleanup_tables()
    :ok
  end
end</file><file path="test/wanderer_kills/killmails/enricher_test.exs">defmodule WandererKills.Killmails.EnricherTest do
  use ExUnit.Case, async: true

  # Sample killmail with enriched data (as it would be after ESI enrichment)
  @enriched_killmail %{
    &quot;killmail_id&quot; =&gt; 123_456_789,
    &quot;kill_time&quot; =&gt; &quot;2024-01-15T14:30:00Z&quot;,
    &quot;solar_system_id&quot; =&gt; 30_000_142,
    &quot;victim&quot; =&gt; %{
      &quot;character_id&quot; =&gt; 987_654_321,
      &quot;corporation_id&quot; =&gt; 123_456_789,
      &quot;alliance_id&quot; =&gt; 456_789_123,
      &quot;ship_type_id&quot; =&gt; 671,
      &quot;damage_taken&quot; =&gt; 2847,
      &quot;character&quot; =&gt; %{&quot;name&quot; =&gt; &quot;John Doe&quot;},
      &quot;corporation&quot; =&gt; %{&quot;name&quot; =&gt; &quot;Corp Name&quot;, &quot;ticker&quot; =&gt; &quot;[CORP]&quot;},
      &quot;alliance&quot; =&gt; %{&quot;name&quot; =&gt; &quot;Alliance Name&quot;, &quot;ticker&quot; =&gt; &quot;[ALLY]&quot;},
      &quot;ship&quot; =&gt; %{&quot;name&quot; =&gt; &quot;Rifter&quot;}
    },
    &quot;attackers&quot; =&gt; [
      %{
        &quot;character_id&quot; =&gt; 111_111_111,
        &quot;corporation_id&quot; =&gt; 222_222_222,
        &quot;alliance_id&quot; =&gt; 333_333_333,
        &quot;ship_type_id&quot; =&gt; 584,
        &quot;final_blow&quot; =&gt; true,
        &quot;damage_done&quot; =&gt; 2847,
        &quot;security_status&quot; =&gt; -1.5,
        &quot;weapon_type_id&quot; =&gt; 2185,
        &quot;character&quot; =&gt; %{&quot;name&quot; =&gt; &quot;Jane Doe&quot;},
        &quot;corporation&quot; =&gt; %{&quot;name&quot; =&gt; &quot;Attacker Corp&quot;, &quot;ticker&quot; =&gt; &quot;[ATK]&quot;},
        &quot;alliance&quot; =&gt; %{&quot;name&quot; =&gt; &quot;Attacker Alliance&quot;, &quot;ticker&quot; =&gt; &quot;[ATKR]&quot;}
      }
    ],
    &quot;zkb&quot; =&gt; %{
      &quot;awox&quot; =&gt; false,
      &quot;destroyedValue&quot; =&gt; 607_542.55,
      &quot;droppedValue&quot; =&gt; 0,
      &quot;fittedValue&quot; =&gt; 10_000,
      &quot;hash&quot; =&gt; &quot;3c7ad3e982520554aad200b6eac9c3773106f4ce&quot;,
      &quot;labels&quot; =&gt; [&quot;cat:6&quot;, &quot;#:1&quot;, &quot;pvp&quot;, &quot;loc:w-space&quot;],
      &quot;locationID&quot; =&gt; 40_423_641,
      &quot;npc&quot; =&gt; false,
      &quot;points&quot; =&gt; 1,
      &quot;solo&quot; =&gt; false,
      &quot;totalValue&quot; =&gt; 607_542.55
    }
  }

  describe &quot;killmail format validation&quot; do
    test &quot;validates expected killmail output format structure&quot; do
      # Test the expected format by creating a sample enriched killmail
      # This test ensures we always provide the expected structure

      # Simulate what the flattening should produce
      flattened = simulate_flattened_killmail(@enriched_killmail)

      # Verify the output matches the expected format
      required_root_fields = [
        &quot;killmail_id&quot;,
        &quot;kill_time&quot;,
        &quot;solar_system_id&quot;,
        &quot;victim&quot;,
        &quot;attackers&quot;,
        &quot;zkb&quot;,
        &quot;attacker_count&quot;
      ]

      for field &lt;- required_root_fields do
        assert Map.has_key?(flattened, field), &quot;Missing root field: #{field}&quot;
      end

      # Verify victim structure
      victim = flattened[&quot;victim&quot;]

      required_victim_fields = [
        &quot;character_id&quot;,
        &quot;character_name&quot;,
        &quot;name&quot;,
        &quot;corporation_id&quot;,
        &quot;corporation_name&quot;,
        &quot;corp_name&quot;,
        &quot;corporation_ticker&quot;,
        &quot;corp_ticker&quot;,
        &quot;alliance_id&quot;,
        &quot;alliance_name&quot;,
        &quot;alliance_ticker&quot;,
        &quot;ship_type_id&quot;,
        &quot;ship_name&quot;,
        &quot;ship_type_name&quot;,
        &quot;damage_taken&quot;
      ]

      for field &lt;- required_victim_fields do
        assert Map.has_key?(victim, field), &quot;Missing victim field: #{field}&quot;
      end

      # Verify attacker structure
      attacker = hd(flattened[&quot;attackers&quot;])

      required_attacker_fields = [
        &quot;character_id&quot;,
        &quot;character_name&quot;,
        &quot;name&quot;,
        &quot;corporation_id&quot;,
        &quot;corporation_name&quot;,
        &quot;corp_name&quot;,
        &quot;corporation_ticker&quot;,
        &quot;corp_ticker&quot;,
        &quot;alliance_id&quot;,
        &quot;alliance_name&quot;,
        &quot;alliance_ticker&quot;,
        &quot;ship_type_id&quot;,
        &quot;ship_name&quot;,
        &quot;ship_type_name&quot;,
        &quot;final_blow&quot;,
        &quot;damage_done&quot;,
        &quot;security_status&quot;,
        &quot;weapon_type_id&quot;
      ]

      for field &lt;- required_attacker_fields do
        assert Map.has_key?(attacker, field), &quot;Missing attacker field: #{field}&quot;
      end
    end

    test &quot;validates flattened victim data structure&quot; do
      flattened = simulate_flattened_killmail(@enriched_killmail)
      victim = flattened[&quot;victim&quot;]

      # Test original fields are preserved
      assert victim[&quot;character_id&quot;] == 987_654_321
      assert victim[&quot;corporation_id&quot;] == 123_456_789
      assert victim[&quot;alliance_id&quot;] == 456_789_123
      assert victim[&quot;ship_type_id&quot;] == 671
      assert victim[&quot;damage_taken&quot;] == 2847

      # Test flattened name fields are added
      assert victim[&quot;character_name&quot;] == &quot;John Doe&quot;
      # Alternative field name
      assert victim[&quot;name&quot;] == &quot;John Doe&quot;
      assert victim[&quot;corporation_name&quot;] == &quot;Corp Name&quot;
      # Alternative field name
      assert victim[&quot;corp_name&quot;] == &quot;Corp Name&quot;
      assert victim[&quot;corporation_ticker&quot;] == &quot;[CORP]&quot;
      # Alternative field name
      assert victim[&quot;corp_ticker&quot;] == &quot;[CORP]&quot;
      assert victim[&quot;alliance_name&quot;] == &quot;Alliance Name&quot;
      assert victim[&quot;alliance_ticker&quot;] == &quot;[ALLY]&quot;
      assert victim[&quot;ship_name&quot;] == &quot;Rifter&quot;
      # Alternative field name
      assert victim[&quot;ship_type_name&quot;] == &quot;Rifter&quot;
    end

    test &quot;validates flattened attacker data structure&quot; do
      flattened = simulate_flattened_killmail(@enriched_killmail)
      attacker = hd(flattened[&quot;attackers&quot;])

      # Test original fields are preserved
      assert attacker[&quot;character_id&quot;] == 111_111_111
      assert attacker[&quot;corporation_id&quot;] == 222_222_222
      assert attacker[&quot;alliance_id&quot;] == 333_333_333
      assert attacker[&quot;ship_type_id&quot;] == 584
      assert attacker[&quot;final_blow&quot;] == true
      assert attacker[&quot;damage_done&quot;] == 2847
      assert attacker[&quot;security_status&quot;] == -1.5
      assert attacker[&quot;weapon_type_id&quot;] == 2185

      # Test flattened name fields are added
      assert attacker[&quot;character_name&quot;] == &quot;Jane Doe&quot;
      # Alternative field name
      assert attacker[&quot;name&quot;] == &quot;Jane Doe&quot;
      assert attacker[&quot;corporation_name&quot;] == &quot;Attacker Corp&quot;
      # Alternative field name
      assert attacker[&quot;corp_name&quot;] == &quot;Attacker Corp&quot;
      assert attacker[&quot;corporation_ticker&quot;] == &quot;[ATK]&quot;
      # Alternative field name
      assert attacker[&quot;corp_ticker&quot;] == &quot;[ATK]&quot;
      assert attacker[&quot;alliance_name&quot;] == &quot;Attacker Alliance&quot;
      assert attacker[&quot;alliance_ticker&quot;] == &quot;[ATKR]&quot;
    end

    test &quot;handles missing enriched data gracefully&quot; do
      # Test with missing character data
      killmail_missing_character = put_in(@enriched_killmail, [&quot;victim&quot;, &quot;character&quot;], nil)
      flattened = simulate_flattened_killmail(killmail_missing_character)
      victim = flattened[&quot;victim&quot;]

      # Should have nil character name but other fields should work
      assert victim[&quot;character_name&quot;] == nil
      assert victim[&quot;name&quot;] == nil
      assert victim[&quot;corporation_name&quot;] == &quot;Corp Name&quot;
      assert victim[&quot;alliance_name&quot;] == &quot;Alliance Name&quot;
    end

    test &quot;handles multiple attackers correctly&quot; do
      multi_attacker_killmail = %{
        @enriched_killmail
        | &quot;attackers&quot; =&gt; [
            @enriched_killmail[&quot;attackers&quot;] |&gt; hd(),
            %{
              &quot;character_id&quot; =&gt; 999_999_999,
              &quot;corporation_id&quot; =&gt; 888_888_888,
              &quot;alliance_id&quot; =&gt; 777_777_777,
              &quot;ship_type_id&quot; =&gt; 123,
              &quot;final_blow&quot; =&gt; false,
              &quot;damage_done&quot; =&gt; 500,
              &quot;security_status&quot; =&gt; 2.0,
              &quot;weapon_type_id&quot; =&gt; 456,
              &quot;character&quot; =&gt; %{&quot;name&quot; =&gt; &quot;Third Attacker&quot;},
              &quot;corporation&quot; =&gt; %{&quot;name&quot; =&gt; &quot;Third Corp&quot;, &quot;ticker&quot; =&gt; &quot;[3RD]&quot;},
              &quot;alliance&quot; =&gt; %{&quot;name&quot; =&gt; &quot;Third Alliance&quot;, &quot;ticker&quot; =&gt; &quot;[3RD]&quot;}
            }
          ]
      }

      flattened = simulate_flattened_killmail(multi_attacker_killmail)

      # Should have 2 attackers and attacker_count should be 2
      assert length(flattened[&quot;attackers&quot;]) == 2
      assert flattened[&quot;attacker_count&quot;] == 2

      # Both attackers should have flattened name fields
      [first_attacker, second_attacker] = flattened[&quot;attackers&quot;]

      assert first_attacker[&quot;character_name&quot;] == &quot;Jane Doe&quot;
      assert first_attacker[&quot;corporation_name&quot;] == &quot;Attacker Corp&quot;

      assert second_attacker[&quot;character_name&quot;] == &quot;Third Attacker&quot;
      assert second_attacker[&quot;corporation_name&quot;] == &quot;Third Corp&quot;
    end
  end

  # Helper function to simulate the flattening that should happen in the enricher
  defp simulate_flattened_killmail(enriched_killmail) do
    enriched_killmail
    |&gt; flatten_victim_data()
    |&gt; flatten_attackers_data()
    |&gt; add_attacker_count()
  end

  defp flatten_victim_data(killmail) do
    victim = Map.get(killmail, &quot;victim&quot;, %{})

    flattened_victim =
      victim
      |&gt; add_character_name(get_in(victim, [&quot;character&quot;, &quot;name&quot;]))
      |&gt; add_corporation_info()
      |&gt; add_alliance_info()
      |&gt; add_ship_name()

    Map.put(killmail, &quot;victim&quot;, flattened_victim)
  end

  defp flatten_attackers_data(killmail) do
    attackers = Map.get(killmail, &quot;attackers&quot;, [])

    flattened_attackers =
      Enum.map(attackers, fn attacker -&gt;
        attacker
        |&gt; add_character_name(get_in(attacker, [&quot;character&quot;, &quot;name&quot;]))
        |&gt; add_corporation_info()
        |&gt; add_alliance_info()
        |&gt; add_ship_name_for_attacker()
      end)

    Map.put(killmail, &quot;attackers&quot;, flattened_attackers)
  end

  defp add_character_name(entity, character_name) do
    entity
    |&gt; Map.put(&quot;character_name&quot;, character_name)
    # Alternative field name
    |&gt; Map.put(&quot;name&quot;, character_name)
  end

  defp add_corporation_info(entity) do
    corp_name = get_in(entity, [&quot;corporation&quot;, &quot;name&quot;])
    corp_ticker = get_in(entity, [&quot;corporation&quot;, &quot;ticker&quot;])

    entity
    |&gt; Map.put(&quot;corporation_name&quot;, corp_name)
    # Alternative field name
    |&gt; Map.put(&quot;corp_name&quot;, corp_name)
    |&gt; Map.put(&quot;corporation_ticker&quot;, corp_ticker)
    # Alternative field name
    |&gt; Map.put(&quot;corp_ticker&quot;, corp_ticker)
  end

  defp add_alliance_info(entity) do
    alliance_name = get_in(entity, [&quot;alliance&quot;, &quot;name&quot;])
    alliance_ticker = get_in(entity, [&quot;alliance&quot;, &quot;ticker&quot;])

    entity
    |&gt; Map.put(&quot;alliance_name&quot;, alliance_name)
    |&gt; Map.put(&quot;alliance_ticker&quot;, alliance_ticker)
  end

  defp add_ship_name(entity) do
    ship_name = get_in(entity, [&quot;ship&quot;, &quot;name&quot;])

    entity
    |&gt; Map.put(&quot;ship_name&quot;, ship_name)
    # Alternative field name
    |&gt; Map.put(&quot;ship_type_name&quot;, ship_name)
  end

  defp add_ship_name_for_attacker(attacker) do
    # For attackers, we don&apos;t have ship enrichment in our test data
    # so we&apos;ll just add nil values to match the expected structure
    attacker
    |&gt; Map.put(&quot;ship_name&quot;, nil)
    # Alternative field name
    |&gt; Map.put(&quot;ship_type_name&quot;, nil)
  end

  defp add_attacker_count(killmail) do
    attackers = Map.get(killmail, &quot;attackers&quot;, [])
    attacker_count = length(attackers)

    Map.put(killmail, &quot;attacker_count&quot;, attacker_count)
  end
end</file><file path=".coderabbit.yaml"># yaml-language-server: $schema=https://coderabbit.ai/integrations/schema.v2.json

language: &quot;en-US&quot;
early_access: true  
reviews:
  profile: &quot;assertive&quot;
  request_changes_workflow: true 
  high_level_summary: true
  poem: true                      
  review_status: true              
  collapse_walkthrough: false      
  path_filters:
    - &quot;!node_modules/**&quot;   # Ignore dependencies
    - &quot;!dist/**&quot;           # Ignore build output
    - &quot;!**/*.min.js&quot;       # Ignore minified files
    - &quot;!**/*.bundle.js&quot;    # Ignore bundled assets
    - &quot;!.notes/**&quot;
    - &quot;!.cursor/**&quot;

  path_instructions:
    # Global project guidelines (apply to all files)
    - path: &quot;**/*&quot;
      instructions: |
        **General Code Quality** – Ensure the code follows global best practices:
        - Keep functions and modules small and focused (single responsibility).
        - Use consistent naming conventions and meaningful identifiers for clarity.
        - Look for unused code or files that can be removed
        - Avoid duplicate code – refactor common logic into reusable functions.
        - Maintain code readability (proper indentation, avoid deep nesting of code).
        - Write comments where necessary to explain intent, but keep code self-explanatory.
        - Use early exit strategy, avoid else use pattern matching

  auto_review:
    enabled: true        # Enable automatic AI review on pull requests
    drafts: false        # Skip reviews on draft PRs (only review ready PRs)
    base_branches: [&quot;main&quot;, &quot;develop&quot;]  # Only run auto-reviews for PRs targeting these branches (adjust to your workflow)

chat:
  auto_reply: true  # Enable the AI to answer follow-up questions in PR comments</file><file path=".coveralls.exs"># Coveralls configuration for WandererKills
[
  # Files and patterns to skip during coverage
  skip_files: [
    # Test support files
    &quot;test/support/&quot;,

    # Generated files
    &quot;_build/&quot;,
    &quot;deps/&quot;,

    # Application entry point (usually simple and well-tested through integration)
    &quot;lib/wanderer_kills/application.ex&quot;,

    # Mock modules used in tests
    &quot;test/support/mocks.ex&quot;
  ],

  # Coverage threshold - fail if coverage drops below this percentage
  minimum_coverage: 80,

  # Whether to halt the suite if coverage is below threshold
  halt_on_failure: false,

  # Output directory for HTML coverage reports
  output_dir: &quot;cover/&quot;,

  # Template for HTML reports
  template_path: &quot;cover/excoveralls.html.eex&quot;,

  # Exclude modules from coverage
  exclude_modules: [
    # Test helper modules
    ~r/.*\.TestHelpers/,
    ~r/.*Test$/,

    # Mock modules
    ~r/.*\.Mock$/,
    ~r/.*Mock$/
  ],

  # Custom stop words - lines with these comments will be excluded
  stop_words: [
    &quot;# coveralls-ignore-start&quot;,
    &quot;# coveralls-ignore-stop&quot;,
    &quot;# coveralls-ignore-next-line&quot;
  ]
]</file><file path=".dockerignore"># Git and version control
.git/
.gitignore

# Build artifacts
_build/
deps/
.elixir_ls/

# Documentation
doc/
*.md
!README.md

# Development files
.devcontainer/
.vscode/
.github/

# Test and coverage
cover/
.coveralls.exs

# Logs and temporary files
*.log
tmp/

# OS files
.DS_Store
Thumbs.db

# IDE files
*.swp
*.swo
*~</file><file path=".formatter.exs"># Used by &quot;mix format&quot;
[
  inputs: [&quot;{mix,.formatter}.exs&quot;, &quot;{config,lib,test}/**/*.{ex,exs}&quot;]
]</file><file path="DOCKER.md"># Docker Configuration

This project uses a consolidated Docker setup to minimize duplication and ensure consistency between development and production environments.

## Files Overview

### Production (`Dockerfile` + `docker-compose.yml`)

- **Purpose**: Production-ready container with optimized build
- **Base Image**: `hexpm/elixir:1.18.3-erlang-25.3-debian-slim`
- **Build**: Multi-stage build for smaller final image
- **Runtime**: Debian slim with only necessary runtime dependencies

### Development (`.devcontainer/`)

- **Purpose**: Development environment with full tooling
- **Base Image**: Same as production for consistency
- **Features**: Additional development tools (vim, jq, net-tools, etc.)
- **Volumes**: Source code mounted for live editing

## Common Patterns

Both configurations share:

- **Base Image**: `hexpm/elixir:1.18.3-erlang-25.3-debian-slim`
- **Core Dependencies**: `build-essential`, `git`, `curl`, `ca-certificates`
- **Elixir Setup**: `mix local.hex --force &amp;&amp; mix local.rebar --force`
- **Package Management**: `apt-get` with `--no-install-recommends` and cleanup

## Usage

### Production

```bash
# Build and run production container
docker-compose up --build

# Or build manually
docker build -t wanderer-kills .
docker run -p 4004:4004 wanderer-kills
```

### Development

Use VS Code with the Dev Containers extension, or:

```bash
# Run development environment
cd .devcontainer
docker-compose up --build
```

## Maintenance

When updating Docker configurations:

1. Keep base images consistent between production and development
2. Use the same package installation patterns
3. Update both Dockerfiles if changing core dependencies
4. Test both production and development builds
5. Update this documentation if adding new patterns</file><file path="repomix.config.json">{
  &quot;output&quot;: {
    &quot;filePath&quot;: &quot;repomix-output.xml&quot;,
    &quot;style&quot;: &quot;xml&quot;,
    &quot;parsableStyle&quot;: true,
    &quot;compress&quot;: false,
    &quot;fileSummary&quot;: true,
    &quot;directoryStructure&quot;: true,
    &quot;removeComments&quot;: false,
    &quot;removeEmptyLines&quot;: false,
    &quot;showLineNumbers&quot;: false,
    &quot;copyToClipboard&quot;: true,
    &quot;topFilesLength&quot;: 5,
    &quot;includeEmptyDirectories&quot;: false
  },
  &quot;include&quot;: [
    &quot;**/*&quot;
  ],
  &quot;ignore&quot;: {
    &quot;useGitignore&quot;: true,
    &quot;useDefaultPatterns&quot;: true,
    &quot;customPatterns&quot;: [
      &quot;priv/**/*&quot;,
      &quot;**/*.svg&quot;,
      &quot;.notes/**/*&quot;,
      &quot;.cursor/**/*&quot;,
      &quot;_build/**/*&quot;,
      &quot;feedback.md&quot;,
      &quot;test.results&quot;
    ]
  },
  &quot;security&quot;: {
    &quot;enableSecurityCheck&quot;: true
  },
  &quot;tokenCount&quot;: {
    &quot;encoding&quot;: &quot;o200k_base&quot;
  }
}</file><file path=".devcontainer/devcontainer.json">{
  &quot;name&quot;: &quot;wanderer-kills-dev&quot;,
  &quot;dockerComposeFile&quot;: [&quot;./docker-compose.yml&quot;],
  &quot;customizations&quot;: {
    &quot;vscode&quot;: {
      &quot;extensions&quot;: [
        &quot;jakebecker.elixir-ls&quot;,
        &quot;JakeBecker.elixir-ls&quot;,
        &quot;dbaeumer.vscode-eslint&quot;,
        &quot;esbenp.prettier-vscode&quot;
      ],
      &quot;settings&quot;: {
        &quot;editor.formatOnSave&quot;: true,
        &quot;search.exclude&quot;: {
          &quot;**/doc&quot;: true
        },
        &quot;elixirLS.dialyzerEnabled&quot;: false
      }
    }
  },
  &quot;service&quot;: &quot;wanderer-kills&quot;,
  &quot;workspaceFolder&quot;: &quot;/app&quot;,
  &quot;shutdownAction&quot;: &quot;stopCompose&quot;,
  &quot;features&quot;: {
    &quot;ghcr.io/devcontainers/features/common-utils:2&quot;: {
      &quot;networkArgs&quot;: [&quot;--add-host=host.docker.internal:host-gateway&quot;]
    }
  },
  &quot;forwardPorts&quot;: [4004],
  &quot;portsAttributes&quot;: {
    &quot;4004&quot;: {
      &quot;label&quot;: &quot;Wanderer Kills API&quot;,
      &quot;onAutoForward&quot;: &quot;notify&quot;
    }
  }
}</file><file path=".devcontainer/docker-compose.yml">version: &quot;3.8&quot;

services:
  wanderer-kills:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      PORT: 4004
      MIX_ENV: dev
      WEB_APP_URL: &quot;http://localhost:4004&quot;
      ERL_AFLAGS: &quot;-kernel shell_history enabled&quot;
    ports:
      - &quot;4004:4004&quot;
    networks:
      - default
    volumes:
      # Mount source code for development
      - ..:/app:delegated
      # Mount git configuration for development workflow
      - ~/.gitconfig:/root/.gitconfig
      - ~/.gitignore:/root/.gitignore
      - ~/.ssh:/root/.ssh
      # Cache Elixir artifacts for faster rebuilds
      - elixir-artifacts:/opt/elixir-artifacts
    # Keep container running for development
    command: sleep infinity

volumes:
  elixir-artifacts: {}

networks:
  default:
    name: wanderer-kills-network</file><file path=".devcontainer/Dockerfile">FROM elixir:1.18.3-otp-27-slim

# Install development and build dependencies
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
    build-essential \
    git \
    curl \
    ca-certificates \
    sudo \
    make \
    bash \
    jq \
    vim \
    net-tools \
    procps \
    &amp;&amp; apt-get clean \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

# Setup Elixir tools
RUN mix local.hex --force &amp;&amp; mix local.rebar --force

WORKDIR /app</file><file path=".github/workflows/ci.yml">name: CI/CD

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    name: Test
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis
        ports:
          - 6379:6379
        options: &gt;-
          --health-cmd &quot;redis-cli ping&quot;
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v3

      - name: Set up Elixir
        uses: erlef/setup-beam@v1
        with:
          elixir-version: &quot;1.18.4&quot;
          otp-version: &quot;27.0&quot;

      - name: Restore dependencies cache
        uses: actions/cache@v3
        with:
          path: |
            deps
            _build
          key: ${{ runner.os }}-mix-${{ hashFiles(&apos;**/mix.lock&apos;) }}
          restore-keys: ${{ runner.os }}-mix-

      - name: Install dependencies
        run: mix deps.get

      - name: Check formatting
        run: mix format --check-formatted

      - name: Run Credo
        run: mix credo

      - name: Run Dialyzer
        run: mix dialyzer

      - name: Run tests
        run: mix test

  docker:
    name: Build and Push Docker Image
    needs: test
    runs-on: ubuntu-latest
    if: github.event_name == &apos;push&apos; &amp;&amp; github.ref == &apos;refs/heads/main&apos;

    steps:
      - uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: wanderer-industries/wanderer-kills
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,format=short

      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  release:
    name: Create Release
    needs: docker
    runs-on: ubuntu-latest
    if: github.event_name == &apos;push&apos; &amp;&amp; github.ref == &apos;refs/heads/main&apos;

    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Get version
        id: get_version
        run: echo &quot;VERSION=$(git describe --tags --abbrev=0)&quot; &gt;&gt; $GITHUB_OUTPUT

      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          name: Release ${{ steps.get_version.outputs.VERSION }}
          tag_name: ${{ steps.get_version.outputs.VERSION }}
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}</file><file path="config/config.exs">import Config

config :wanderer_kills,
  port: String.to_integer(System.get_env(&quot;PORT&quot;) || &quot;4004&quot;),

  # Cache configuration
  cache_killmails_ttl: 3600,
  cache_system_ttl: 1800,
  cache_esi_ttl: 3600,
  cache_esi_killmail_ttl: 86_400,
  cache_system_recent_fetch_threshold: 5,

  # Parser configuration
  parser_cutoff_seconds: 3_600,
  parser_summary_interval_ms: 60_000,

  # Enricher configuration
  enricher_max_concurrency: 10,
  enricher_task_timeout_ms: 30_000,
  enricher_min_attackers_for_parallel: 3,

  # Batch processing configuration
  concurrency_batch_size: 100,

  # Service URLs
  esi_base_url: &quot;https://esi.evetech.net/latest&quot;,
  zkb_base_url: &quot;https://zkillboard.com/api&quot;,

  # HTTP client configuration
  http_client: WandererKills.Http.Client,

  # Request timeout configuration (missing from original config)
  esi_request_timeout_ms: 30_000,
  zkb_request_timeout_ms: 15_000,
  http_request_timeout_ms: 10_000,
  default_request_timeout_ms: 10_000,

  # Batch concurrency configuration (missing from original config)
  esi_batch_concurrency: 10,
  zkb_batch_concurrency: 5,
  default_batch_concurrency: 5,

  # Retry configuration
  retry_http_max_retries: 3,
  retry_http_base_delay: 1000,
  retry_http_max_delay: 30_000,
  retry_redisq_max_retries: 5,
  retry_redisq_base_delay: 500,

  # RedisQ stream configuration
  redisq_base_url: &quot;https://zkillredisq.stream/listen.php&quot;,
  redisq_fast_interval_ms: 1_000,
  redisq_idle_interval_ms: 5_000,
  redisq_initial_backoff_ms: 1_000,
  redisq_max_backoff_ms: 30_000,
  redisq_backoff_factor: 2,
  redisq_task_timeout_ms: 10_000,

  # Killmail store configuration
  killmail_store_gc_interval_ms: 60_000,
  killmail_store_max_events_per_system: 10_000,

  # Telemetry configuration
  telemetry_enabled_metrics: [:cache, :api, :circuit, :event],
  telemetry_sampling_rate: 1.0,
  # 7 days in seconds
  telemetry_retention_period: 604_800,

  # Service startup configuration
  start_preloader: true,
  start_redisq: true

# Cachex default configuration
config :cachex, :default_ttl, :timer.hours(24)

# Configure the logger
config :logger,
  level: :info,
  format: &quot;$time $metadata[$level] $message\n&quot;,
  metadata: :all,
  backends: [:console]

config :logger, :console,
  format: &quot;$time $metadata[$level] $message\n&quot;,
  metadata: [
    # Standard Elixir metadata
    :request_id,
    :application,
    :module,
    :function,
    :line,

    # Core application metadata
    :system_id,
    :killmail_id,
    :operation,
    :step,
    :status,
    :error,
    :duration,
    :source,
    :reason,

    # HTTP and API metadata
    :url,
    :response_time,
    :method,
    :service,
    :endpoint,

    # EVE Online entity metadata
    :character_id,
    :corporation_id,
    :alliance_id,
    :type_id,
    :solar_system_id,
    :ship_type_id,

    # Cache metadata
    :cache,
    :cache_key,
    :cache_type,
    :ttl,

    # Processing metadata
    :killmail_count,
    :count,
    :result,
    :data_source,

    # Retry and timeout metadata
    :attempt,
    :max_attempts,
    :remaining_attempts,
    :delay_ms,
    :timeout,
    :request_type,
    :raw_count,
    :parsed_count,
    :enriched_count,
    :since_hours,
    :provided_id,
    :types,
    :groups,
    :file,
    :path,
    :pass_type,
    :hours,
    :limit,
    :max_concurrency,
    :purpose,
    :format,
    :percentage,
    :description,
    :unit,
    :value,
    :count,
    :total,
    :processed,
    :skipped,
    :error,
    :total_killmails_analyzed,
    :format_distribution,
    :system_distribution,
    :ship_distribution,
    :character_distribution,
    :corporation_distribution,
    :alliance_distribution,
    :ship_type_distribution,
    :purpose,
    :sample_index,
    :sample_size,
    :sample_type,
    :sample_value,
    :sample_unit,
    :sample_value,
    :sample_structure,
    :data_type,
    :raw_keys,
    :has_full_data,
    :needs_esi_fetch,
    :byte_size,
    :tasks,
    :group_ids,
    :error_count,
    :total_groups,
    :success_count,
    :type_count,
    :cutoff_time,
    :killmail_sample,
    :required_fields,
    :missing_fields,
    :available_keys,
    :killmail_sample,
    :raw_structure,
    :parsed_structure,
    :enriched_structure,
    :killmail_id,
    :system_id,
    :ship_type_id,
    :character_id,
    :killmail_keys,
    :kill_count,
    :hash,
    :has_solar_system_id,
    :has_kill_count,
    :has_hash,
    :has_killmail_id,
    :has_system_id,
    :has_ship_type_id,
    :has_character_id,
    :has_victim,
    :has_attackers,
    :has_zkb,
    :killmail_keys,
    :parser_type,
    :killmail_hash,
    :raw_structure,
    :recommendation,
    :structure,
    :kill_time,
    :cutoff,
    :subscriber_id,
    :system_ids,
    :callback_url,
    :subscription_id,
    :status,
    :error,
    :system_count,
    :has_callback,
    :error_count,
    :success_count,
    :total_subscriptions,
    :active_subscriptions,
    :removed_count,
    :requested_systems,
    :successful_systems,
    :failed_systems,
    :total_systems,
    :system_ids,
    :callback_url,
    :subscription_id,
    :status,
    :kills_count,
    :pubsub_name,
    :pubsub_topic,
    :pubsub_message,
    :pubsub_metadata,
    :pubsub_payload,
    :pubsub_headers,
    :pubsub_timestamp,
    :total_kills,
    :filtered_kills,
    :subscriber_count,
    :total_cached_kills,
    :cache_error,
    :returned_kills,
    :unexpected_response,
    :cached_count,
    :total_kills_sent
  ]

# Import environment specific config
import_config &quot;#{config_env()}.exs&quot;

# Phoenix PubSub configuration
config :wanderer_kills, WandererKills.PubSub, adapter: Phoenix.PubSub.PG</file><file path="config/dev.exs">import Config

# Configure the logger for development
config :logger,
  level: :info,
  format: &quot;$time $metadata[$level] $message\n&quot;,
  metadata: :all

# Console backend (existing behavior)
config :logger, :console,
  format: &quot;$time $metadata[$level] $message\n&quot;,
  metadata: :all

# File backend for debugging
config :logger, :file,
  path: &quot;/app/logs/wanderer_kills_debug.log&quot;,
  level: :info,
  format: &quot;$time $metadata[$level] $message\n&quot;,
  metadata: :all</file><file path="config/test.exs">import Config

# Configure the application for testing
config :wanderer_kills,
  # Disable external services in tests
  start_preloader: false,
  start_redisq: false,

  # Disable ETS supervisor in tests (managed manually)
  start_ets_supervisor: false,

  # Fast cache expiry for tests (flattened structure)
  cache_killmails_ttl: 1,
  cache_system_ttl: 1,
  cache_esi_ttl: 1,

  # Short timeouts for faster test runs
  retry_http_max_retries: 1,
  retry_http_base_delay: 100,
  retry_redisq_max_retries: 1,
  retry_redisq_base_delay: 100,

  # Fast intervals for tests
  redisq_fast_interval_ms: 100,
  redisq_idle_interval_ms: 100,
  redisq_task_timeout_ms: 1_000,

  # Short timeouts for other services
  enricher_task_timeout_ms: 1_000,
  parser_summary_interval_ms: 100,
  killmail_store_gc_interval_ms: 100,

  # Mock clients for testing
  http_client: WandererKills.Http.Client.Mock,
  zkb_client: WandererKills.Zkb.Client.Mock,
  esi_client: WandererKills.ESI.Client.Mock,

  # Use test cache names
  killmails_cache_name: :wanderer_test_killmails_cache,
  system_cache_name: :wanderer_test_system_cache,
  esi_cache_name: :wanderer_test_esi_cache,

  # Disable telemetry in tests
  telemetry_enabled_metrics: [],
  telemetry_sampling_rate: 0.0

# ESI cache configuration removed - now using Cache.Helper directly

# Configure Cachex for tests
config :cachex, :default_ttl, :timer.minutes(1)

# Configure Mox - use global mode
config :mox, global: true

# Logger configuration for tests
config :logger, level: :warning</file><file path="lib/wanderer_kills/http/client.ex">defmodule WandererKills.Http.Client do
  @moduledoc &quot;&quot;&quot;
  Core HTTP client that handles rate limiting, retries, and common HTTP functionality.

  This module provides a robust HTTP client implementation that handles:
    - Rate limiting and backoff
    - Automatic retries with exponential backoff
    - JSON response parsing
    - Error handling and logging
    - Custom error types for different failure scenarios
    - Telemetry for monitoring HTTP calls

  ## Usage

      # Basic GET request with rate limiting
      {:ok, response} = WandererKills.Http.Client.get_with_rate_limit(&quot;https://api.example.com/data&quot;)

      # GET request with custom options
      opts = [
        params: [query: &quot;value&quot;],
        headers: [{&quot;authorization&quot;, &quot;Bearer token&quot;}],
        timeout: 5000
      ]
      {:ok, response} = WandererKills.Http.Client.get_with_rate_limit(&quot;https://api.example.com/data&quot;, opts)

  ## Error Handling

  The module defines several custom error types (in `WandererKills.Infrastructure.Error`):
    - `ConnectionError` - Raised when a connection fails
    - `TimeoutError` - Raised when a request times out
    - `RateLimitError` - Raised when rate limit is exceeded

  All functions return either `{:ok, result}` or `{:error, reason}` tuples.

  ## Telemetry

  The module emits the following telemetry events:

  - `[:wanderer_kills, :http, :request, :start]` - When a request starts
    - Metadata: `%{method: &quot;GET&quot;, url: url}`
  - `[:wanderer_kills, :http, :request, :stop]` - When a request completes
    - Metadata: `%{method: &quot;GET&quot;, url: url, status_code: status}` on success
    - Metadata: `%{method: &quot;GET&quot;, url: url, error: reason}` on failure
  &quot;&quot;&quot;

  @behaviour WandererKills.Behaviours.HttpClient

  require Logger
  alias WandererKills.Infrastructure.Error.{ConnectionError, TimeoutError, RateLimitError}
  alias WandererKills.Infrastructure.{Config, Error, Retry}
  alias WandererKills.Http.ClientProvider
  alias WandererKills.Observability.Telemetry

  @type url :: String.t()
  @type headers :: [{String.t(), String.t()}]
  @type opts :: keyword()
  @type response :: {:ok, map()} | {:error, term()}

  # Get the configured HTTP client implementation
  defp http_client do
    WandererKills.Infrastructure.Config.app().http_client
  end

  # Real HTTP implementation using Req
  def real_get(url, headers, raw, into) do
    Req.get(url, headers: headers, raw: raw, into: into)
  end

  # Implementation callbacks (not part of behaviour)

  # ============================================================================
  # HttpClient Behaviour Implementation
  # ============================================================================

  @impl true
  @doc &quot;&quot;&quot;
  Makes a GET request.

  This is a simplified version that delegates to get_with_rate_limit/2.
  &quot;&quot;&quot;
  @spec get(url(), headers(), opts()) :: response()
  def get(url, headers \\ [], options \\ []) do
    opts = Keyword.merge(options, headers: headers)
    get_with_rate_limit(url, opts)
  end

  @impl true

  # ============================================================================
  # Main Implementation
  # ============================================================================

  @doc &quot;&quot;&quot;
  Makes a GET request with rate limiting and retries.

  ## Options
    - `:params` - Query parameters (default: [])
    - `:headers` - HTTP headers (default: [])
    - `:timeout` - Request timeout in milliseconds (default: 30_000)
    - `:recv_timeout` - Receive timeout in milliseconds (default: 60_000)
    - `:raw` - If true, returns raw response body without JSON parsing (default: false)
    - `:into` - Optional module to decode the response into (default: nil)
    - `:retries` - Number of retry attempts (default: 3)

  ## Returns
    - `{:ok, response}` - On success, response is either a map (parsed JSON) or raw body
    - `{:error, reason}` - On failure, reason can be:
      - `:not_found` - HTTP 404
      - `:rate_limited` - HTTP 429 (after exhausting retries)
      - `&quot;HTTP status&quot;` - Other HTTP errors
      - Other error terms for network/parsing failures

  ## Examples

  ```elixir
  # Basic request
  {:ok, response} = get_with_rate_limit(&quot;https://api.example.com/data&quot;)

  # With options
  {:ok, response} = get_with_rate_limit(&quot;https://api.example.com/data&quot;,
    params: [query: &quot;value&quot;],
    headers: [{&quot;authorization&quot;, &quot;Bearer token&quot;}],
    timeout: 5_000
  )
  ```
  &quot;&quot;&quot;
  @spec get_with_rate_limit(url(), opts()) :: response()
  def get_with_rate_limit(url, opts \\ []) do
    # Check if we should use a mock client
    case http_client() do
      __MODULE__ -&gt;
        # Use the real implementation
        do_get_with_rate_limit(url, opts)

      mock_client -&gt;
        # Use mock implementation directly
        mock_client.get_with_rate_limit(url, opts)
    end
  end

  # The real implementation moved to a private function
  defp do_get_with_rate_limit(url, opts) do
    headers = Keyword.get(opts, :headers, [])
    raw = Keyword.get(opts, :raw, false)
    into = Keyword.get(opts, :into)

    # Merge default headers with custom headers
    merged_headers = ClientProvider.default_headers() ++ headers

    fetch_fun = fn -&gt;
      case do_get(url, merged_headers, raw, into) do
        {:ok, response} -&gt;
          response

        {:error, :rate_limited} -&gt;
          # Turn a 429 into a retryable exception
          raise RateLimitError, message: &quot;HTTP 429 Rate Limit for #{url}&quot;

        {:error, %TimeoutError{} = err} -&gt;
          raise err

        {:error, %ConnectionError{} = err} -&gt;
          raise err

        {:error, other_reason} -&gt;
          # Non-retriable: short-circuit
          throw({:error, other_reason})
      end
    end

    result =
      try do
        {:ok, response} = Retry.retry_with_backoff(fetch_fun)
        {:ok, response}
      catch
        {:error, reason} -&gt;
          {:error, reason}
      end

    result
  end

  @spec do_get(url(), headers(), boolean(), module() | nil) :: {:ok, term()} | {:error, term()}
  defp do_get(url, headers, raw, into) do
    start_time = System.monotonic_time()

    Telemetry.http_request_start(&quot;GET&quot;, url)

    result =
      case real_get(url, headers, raw, into) do
        {:ok, %{status: status} = resp} -&gt;
          handle_status_code(status, resp)

        {:error, %{reason: :timeout}} -&gt;
          {:error, %TimeoutError{message: &quot;Request to #{url} timed out&quot;}}

        {:error, %{reason: :econnrefused}} -&gt;
          {:error, %ConnectionError{message: &quot;Connection refused for #{url}&quot;}}

        {:error, reason} -&gt;
          {:error, reason}
      end

    duration = System.monotonic_time() - start_time

    case result do
      {:ok, %{status: status}} -&gt;
        Telemetry.http_request_stop(&quot;GET&quot;, url, duration, status)

      {:error, reason} -&gt;
        Telemetry.http_request_error(&quot;GET&quot;, url, duration, reason)
    end

    result
  end

  @doc &quot;&quot;&quot;
  Centralized HTTP status code handling.

  This function provides unified status code handling for all HTTP clients
  in the application, using configuration-driven status code mappings.

  ## Parameters
  - `status` - HTTP status code
  - `response` - HTTP response map (optional, defaults to empty map)

  ## Returns
  - `{:ok, response}` - For successful status codes (200-299)
  - `{:error, :not_found}` - For 404 status
  - `{:error, :rate_limited}` - For 429 status
  - `{:error, &quot;HTTP {status}&quot;}` - For other error status codes

  ## Examples

  ```elixir
  # Success case
  {:ok, response} = handle_status_code(200, %{body: &quot;data&quot;})

  # Not found
  {:error, :not_found} = handle_status_code(404)

  # Rate limited
  {:error, :rate_limited} = handle_status_code(429)

  # Other errors
  {:error, &quot;HTTP 500&quot;} = handle_status_code(500)
  ```
  &quot;&quot;&quot;
  @spec handle_status_code(integer(), map()) :: {:ok, map()} | {:error, term()}
  def handle_status_code(status, resp \\ %{})

  # Success status codes (200-299)
  def handle_status_code(status, resp) when status &gt;= 200 and status &lt; 300 do
    {:ok, resp}
  end

  # Not found
  def handle_status_code(404, _resp) do
    {:error, :not_found}
  end

  # Rate limited
  def handle_status_code(429, _resp) do
    {:error, :rate_limited}
  end

  # Retryable client errors (400-499, excluding 404 and 429)
  def handle_status_code(status, _resp)
      when status &gt;= 400 and status &lt; 500 and status not in [404, 429] do
    {:error, &quot;HTTP #{status}&quot;}
  end

  # Server errors (500-599) - typically retryable
  def handle_status_code(status, _resp) when status &gt;= 500 and status &lt; 600 do
    {:error, &quot;HTTP #{status}&quot;}
  end

  # Any other status code
  def handle_status_code(status, _resp) do
    {:error, &quot;HTTP #{status}&quot;}
  end

  @spec retriable_error?(term()) :: boolean()
  def retriable_error?(error), do: Retry.retriable_http_error?(error)

  # ============================================================================
  # Consolidated Utility Functions (from Http.Util)
  # ============================================================================

  @doc &quot;&quot;&quot;
  Standard request with telemetry and error handling.

  Provides consistent request patterns with automatic telemetry,
  logging, and error handling across all HTTP clients.
  &quot;&quot;&quot;
  @spec request_with_telemetry(url(), atom(), keyword()) :: response()
  def request_with_telemetry(url, service, opts \\ []) do
    operation = Keyword.get(opts, :operation, :http_request)
    request_opts = ClientProvider.build_request_opts(opts)

    Logger.debug(&quot;Starting HTTP request&quot;,
      url: url,
      service: service,
      operation: operation
    )

    case get_with_rate_limit(url, request_opts) do
      {:ok, response} -&gt;
        Logger.debug(&quot;HTTP request successful&quot;,
          url: url,
          service: service,
          operation: operation,
          status: Map.get(response, :status)
        )

        {:ok, response}

      {:error, reason} -&gt;
        Logger.error(&quot;HTTP request failed&quot;,
          url: url,
          service: service,
          operation: operation,
          error: inspect(reason)
        )

        {:error, reason}
    end
  end

  @doc &quot;&quot;&quot;
  Parse JSON response with error handling.

  Provides consistent JSON parsing across all HTTP clients.
  &quot;&quot;&quot;
  @spec parse_json_response(map()) :: {:ok, term()} | {:error, term()}
  def parse_json_response(%{status: 200, body: body}) when is_map(body) or is_list(body) do
    {:ok, body}
  end

  def parse_json_response(%{status: 200, body: body}) when is_binary(body) do
    case Jason.decode(body) do
      {:ok, parsed} -&gt;
        {:ok, parsed}

      {:error, reason} -&gt;
        {:error, Error.parsing_error(:invalid_json, &quot;Invalid JSON response&quot;, %{reason: reason})}
    end
  end

  def parse_json_response(%{status: 404}) do
    {:error, :not_found}
  end

  def parse_json_response(%{status: 429}) do
    {:error, :rate_limited}
  end

  def parse_json_response(%{status: status}) when status &gt;= 500 do
    {:error, Error.http_error(:server_error, &quot;Server error&quot;, false, %{status: status})}
  end

  def parse_json_response(%{status: status}) do
    {:error, Error.http_error(:http_error, &quot;HTTP error&quot;, false, %{status: status})}
  end

  @doc &quot;&quot;&quot;
  Retry an operation with specific service configuration.
  &quot;&quot;&quot;
  @spec retry_operation((-&gt; term()), atom(), keyword()) :: {:ok, term()} | {:error, term()}
  def retry_operation(fun, service, opts \\ []) do
    operation_name = Keyword.get(opts, :operation_name, &quot;#{service} request&quot;)
    max_retries = Keyword.get(opts, :max_retries, Config.retry().http_max_retries)

    Retry.retry_http_operation(fun,
      operation_name: operation_name,
      max_retries: max_retries
    )
  end

  @doc &quot;&quot;&quot;
  Validate response format and structure.

  Provides consistent validation across different API responses.
  &quot;&quot;&quot;
  @spec validate_response_structure(term(), list()) :: {:ok, term()} | {:error, term()}
  def validate_response_structure(data, required_fields) when is_map(data) do
    missing_fields =
      required_fields
      |&gt; Enum.reject(&amp;Map.has_key?(data, &amp;1))

    case missing_fields do
      [] -&gt;
        {:ok, data}

      missing -&gt;
        {:error,
         Error.validation_error(:missing_fields, &quot;Missing required fields&quot;, %{missing: missing})}
    end
  end

  def validate_response_structure(data, _required_fields) when is_list(data) do
    {:ok, data}
  end

  def validate_response_structure(data, _required_fields) do
    {:error,
     Error.validation_error(:invalid_format, &quot;Invalid response format&quot;, %{type: typeof(data)})}
  end

  # ============================================================================
  # Private Helper Functions
  # ============================================================================

  defp typeof(data) when is_map(data), do: :map
  defp typeof(data) when is_list(data), do: :list
  defp typeof(data) when is_binary(data), do: :string
  defp typeof(data) when is_integer(data), do: :integer
  defp typeof(data) when is_float(data), do: :float
  defp typeof(data) when is_boolean(data), do: :boolean
  defp typeof(data) when is_atom(data), do: :atom
  defp typeof(_), do: :unknown
end</file><file path="lib/wanderer_kills.ex">defmodule WandererKills do
  @moduledoc &quot;&quot;&quot;
  WandererKills is a standalone service for retrieving and caching EVE Online killmails from zKillboard.

  ## Features

  * Fetches killmails from zKillboard API
  * Caches killmails and related data
  * Provides HTTP API endpoints for accessing killmail data
  * Supports system-specific killmail queries
  * Includes ship type information enrichment
  &quot;&quot;&quot;

  @doc &quot;&quot;&quot;
  Returns the application version.
  &quot;&quot;&quot;
  def version do
    case Application.spec(:wanderer_kills) do
      nil -&gt; &quot;0.1.0&quot;
      spec -&gt; to_string(spec[:vsn])
    end
  end

  @doc &quot;&quot;&quot;
  Returns the application name.
  &quot;&quot;&quot;
  def app_name do
    case Application.spec(:wanderer_kills) do
      nil -&gt; :wanderer_kills
      spec -&gt; spec[:app] || :wanderer_kills
    end
  end
end</file><file path="test/wanderer_kills_test.exs">defmodule WandererKillsTest do
  use ExUnit.Case
  doctest WandererKills

  test &quot;version returns a string&quot; do
    assert is_binary(WandererKills.version())
  end

  test &quot;app_name returns :wanderer_kills&quot; do
    assert WandererKills.app_name() == :wanderer_kills
  end
end</file><file path=".gitignore"># The directory Mix will write compiled artifacts to.
/_build/
cleanup.md
# If you run &quot;mix test --cover&quot;, coverage assets end up here.
/cover/

# The directory Mix downloads your dependencies sources to.
/deps/

# Where third-party dependencies like ExDoc output generated docs.
/doc/

# Ignore .fetch files in case you like to edit your project deps locally.
/.fetch

# If the VM crashes, it generates a dump, let&apos;s ignore it too.
erl_crash.dump

# Also ignore archive artifacts (built via &quot;mix archive.build&quot;).
*.ez

# Ignore package tarball (built via &quot;mix hex.build&quot;).
wanderer_kills-*.tar

logs/
logs.txt
# Temporary files, for example, from tests.
/tmp/</file><file path="docker-compose.yml">version: &quot;3.8&quot;

services:
  wanderer-kills:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      PORT: 4004
      MIX_ENV: prod
    ports:
      - &quot;4004:4004&quot;
    command: [&quot;bin/wanderer_kills&quot;, &quot;start&quot;]
    restart: unless-stopped</file><file path="Dockerfile">FROM elixir:1.18.3-otp-27-slim AS build

# Install build dependencies (consistent with devcontainer)
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
    build-essential \
    git \
    curl \
    ca-certificates \
    &amp;&amp; apt-get clean \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Setup Elixir tools and dependencies
COPY mix.exs mix.lock ./
RUN mix local.hex --force &amp;&amp; mix local.rebar --force
RUN MIX_ENV=prod mix deps.get --only prod &amp;&amp; MIX_ENV=prod mix deps.compile

COPY lib lib
COPY config config

RUN MIX_ENV=prod mix compile

FROM debian:stable-slim AS app
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
    ca-certificates \
    &amp;&amp; apt-get clean \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

WORKDIR /app
COPY --from=build /app/_build/prod/rel/wanderer_kills ./

ENV REPLACE_OS_VARS=true \
    MIX_ENV=prod

EXPOSE 4004
CMD [&quot;bin/wanderer_kills&quot;, &quot;start&quot;]</file><file path="README.md"># WandererKills

A standalone service for retrieving and caching EVE Online killmails from zKillboard.

## Development Setup

### Using Docker Development Container

The project includes a development container configuration for a consistent development environment. To use it:

1. Install [Docker](https://docs.docker.com/get-docker/) and [VS Code](https://code.visualstudio.com/)
2. Install the [Remote - Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) extension in VS Code
3. Clone this repository
4. Open the project in VS Code
5. When prompted, click &quot;Reopen in Container&quot; or use the command palette (F1) and select &quot;Remote-Containers: Reopen in Container&quot;

The development container includes:

- Elixir 1.14.0
- OTP 25.0
- Redis for caching
- All required build tools

### Data Mounting

The service requires access to several data directories:

1. **Cache Directory**

   ```bash
   # Mount the cache directory for persistent caching
   docker run -v /path/to/cache:/app/cache wanderer-kills
   ```

2. **Log Directory**

   ```bash
   # Mount the log directory for persistent logs
   docker run -v /path/to/logs:/app/logs wanderer-kills
   ```

3. **Configuration Directory**
   ```bash
   # Mount a custom configuration directory
   docker run -v /path/to/config:/app/config wanderer-kills
   ```

### Ship-Type Data Bootstrap

The service requires ship type data for proper operation. To bootstrap the data:

1. **Automatic Bootstrap**

   ```bash
   # The service will automatically download and process ship type data on first run
   mix run --no-halt
   ```

2. **Manual Bootstrap**

   ```bash
   # Download and process ship type data manually
   mix run -e &quot;WandererKills.Data.ShipTypeUpdater.update_all_ship_types()&quot;
   ```

3. **Verify Data**
   ```bash
   # Check if ship type data is properly loaded
   mix run -e &quot;IO.inspect(WandererKills.Data.ShipTypeInfo.get_ship_type(670))&quot;
   ```

## Configuration

The service can be configured through environment variables or a config file:

```elixir
# config/config.exs
config :wanderer_kills,
  port: String.to_integer(System.get_env(&quot;PORT&quot;) || &quot;4004&quot;),
  cache: %{
    killmails: [name: :killmails_cache, ttl: :timer.hours(24)],
    system: [name: :system_cache, ttl: :timer.hours(1)],
    esi: [name: :esi_cache, ttl: :timer.hours(48)]
  }
```

## API Endpoints

- `GET /api/v1/killmails/:system_id` - Get killmails for a system
- `GET /api/v1/systems/:system_id/count` - Get kill count for a system
- `GET /api/v1/ships/:type_id` - Get ship type information

## Development

### Running Tests

```bash
mix test
```

### Code Quality

```bash
# Format code
mix format

# Run Credo
mix credo

# Run Dialyzer
mix dialyzer
```

### Docker Development

```bash
# Build the development image
docker build -t wanderer-kills-dev -f Dockerfile.dev .

# Run the development container
docker run -it --rm \
  -v $(pwd):/app \
  -p 4004:4004 \
  wanderer-kills-dev
```

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m &apos;Add some amazing feature&apos;`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Logging

The application uses a standardized logging system with the following log levels:

- `:debug` - Detailed information for debugging purposes

  - Cache operations
  - Task completions
  - Request/response details
  - System state changes

- `:info` - General operational information

  - Successful API requests
  - Cache misses
  - System startup/shutdown
  - Background job completions

- `:warning` - Unexpected but handled situations

  - Rate limiting
  - Cache errors
  - Invalid input data
  - Retry attempts

- `:error` - Errors that affect operation but don&apos;t crash the system
  - API failures
  - Database errors
  - Task failures
  - Invalid state transitions

Each log entry includes:

- Request ID (for HTTP requests)
- Module name
- Operation context
- Relevant metadata

To configure logging levels, set the `:logger` configuration in your environment:

```elixir
config :logger,
  level: :info,
  metadata: [:request_id, :module, :function]
```</file><file path="lib/wanderer_kills/application.ex"># lib/wanderer_kills/application.ex

defmodule WandererKills.Application do
  @moduledoc &quot;&quot;&quot;
  OTP Application entry point for WandererKills.

  Supervises:
    1. A Task.Supervisor for background jobs
    2. Cachex instances for different cache namespaces
    3. The preloader supervisor tree (conditionally)
    4. The HTTP endpoint (Plug.Cowboy)
    5. Observability/monitoring processes
    6. The Telemetry.Poller for periodic measurements
  &quot;&quot;&quot;

  use Application
  require Logger
  alias WandererKills.Infrastructure.Config
  import Cachex.Spec

  @impl true
  def start(_type, _args) do
    # 1) Initialize ETS for our KillStore
    WandererKills.KillStore.init_tables!()

    # 2) Attach telemetry handlers
    WandererKills.Observability.Telemetry.attach_handlers()

    # 3) Build children list
    children =
      ([
         {Task.Supervisor, name: WandererKills.TaskSupervisor},
         {Phoenix.PubSub, name: WandererKills.PubSub},
         {WandererKills.SubscriptionManager, [pubsub_name: WandererKills.PubSub]}
       ] ++
         cache_children() ++
         [
           WandererKills.Observability.Monitoring,
           {Plug.Cowboy,
            scheme: :http,
            plug: WandererKillsWeb.Api,
            options: [port: Config.app().port, ip: {0, 0, 0, 0}]},
           {:telemetry_poller, measurements: telemetry_measurements(), period: :timer.seconds(10)}
         ])
      |&gt; maybe_preloader()
      |&gt; maybe_redisq()

    # 4) Start the supervisor
    opts = [strategy: :one_for_one, name: WandererKills.Supervisor]

    case Supervisor.start_link(children, opts) do
      {:ok, pid} -&gt;
        start_ship_type_update()
        {:ok, pid}

      error -&gt;
        error
    end
  end

  # Create a single Cachex instance with namespace support
  defp cache_children do
    # Use a reasonable default TTL - we&apos;ll set specific TTLs per key when needed
    default_ttl_ms = Config.cache().esi_ttl * 1_000

    opts = [
      default_ttl: default_ttl_ms,
      expiration:
        expiration(
          interval: :timer.seconds(60),
          default: default_ttl_ms,
          lazy: true
        )
    ]

    [
      {Cachex, [:wanderer_cache, opts]}
    ]
  end

  defp telemetry_measurements do
    [
      {WandererKills.Observability.Monitoring, :measure_http_requests, []},
      {WandererKills.Observability.Monitoring, :measure_cache_operations, []},
      {WandererKills.Observability.Monitoring, :measure_fetch_operations, []},
      {WandererKills.Observability.Monitoring, :measure_system_resources, []}
    ]
  end

  defp maybe_preloader(children) do
    if Config.start_preloader?() do
      children ++ [WandererKills.Preloader.Supervisor]
    else
      children
    end
  end

  defp maybe_redisq(children) do
    if Config.start_redisq?() do
      children ++ [WandererKills.RedisQ]
    else
      children
    end
  end

  @spec start_ship_type_update() :: :ok
  defp start_ship_type_update do
    Task.start(fn -&gt;
      WandererKills.ShipTypes.Info.warm_cache()

      case WandererKills.ShipTypes.Updater.update_ship_types() do
        {:error, reason} -&gt;
          Logger.error(&quot;Failed to update ship types: #{inspect(reason)}&quot;)

        _ -&gt;
          :ok
      end
    end)

    :ok
  end
end</file><file path="lib/wanderer_kills/preloader.ex">defmodule WandererKills.Preloader do
  @moduledoc &quot;&quot;&quot;
  Preloader subsystem for killmail data.

  This module contains both the supervisor and worker components for preloading
  killmail data for active systems. The supervisor manages the worker process
  lifecycle, while the worker handles the actual preloading logic.
  &quot;&quot;&quot;

  defmodule Supervisor do
    @moduledoc &quot;&quot;&quot;
    Supervisor for the Preloader subsystem.
    Manages the lifecycle of the Preloader and RedisQ processes.
    &quot;&quot;&quot;

    use Elixir.Supervisor

    # No @impl here, since Supervisor only defines init/1 as a callback.
    def start_link(opts) do
      Elixir.Supervisor.start_link(__MODULE__, opts, name: __MODULE__)
    end

    @impl true
    @spec init(any()) ::
            {:ok,
             {%{
                :strategy =&gt; :one_for_one,
                :intensity =&gt; non_neg_integer(),
                :period =&gt; pos_integer(),
                :auto_shutdown =&gt; :all_significant | :any_significant | :never
              }, [Elixir.Supervisor.child_spec()]}}
    def init(_opts) do
      # Build children list based on configuration
      children = []

      # Always include preloader worker
      preloader_worker_spec = %{
        id: WandererKills.Preloader.Worker,
        start: {WandererKills.Preloader.Worker, :start_link, [[]]},
        type: :worker,
        restart: :permanent,
        shutdown: 5_000
      }

      children = [preloader_worker_spec | children]

      # RedisQ module was removed during cleanup - no longer needed
      # All RedisQ functionality is handled through the ZKB client now

      # Reverse to maintain proper order
      children = Enum.reverse(children)

      # Supervisor flags with better fault tolerance
      flags = %{
        strategy: :one_for_one,
        # Allow up to 3 restarts
        intensity: 3,
        # Within 60 seconds
        period: 60,
        auto_shutdown: :any_significant
      }

      {:ok, {flags, children}}
    end
  end

  defmodule Worker do
    @moduledoc &quot;&quot;&quot;
    Preloads killmail data for systems.

    On startup:
      1. Runs a one-off quick preload (last 1h, limit 5).
      2. Exposes `run_preload_now/0` for an expanded preload (last 24h, limit 100).

    The preloader maintains a list of active systems based on API requests,
    with a 24-hour TTL for each system.
    &quot;&quot;&quot;

    use GenServer
    require Logger

    alias WandererKills.Cache.Helper

    @type pass_type :: :quick | :expanded
    @type fetch_result :: :ok | {:error, term()}

    @passes %{
      quick: %{hours: 1, limit: 5},
      expanded: %{hours: 24, limit: 100}
    }

    @default_max_concurrency 2

    ## Public API

    @doc false
    @spec child_spec(keyword()) :: Elixir.Supervisor.child_spec()
    def child_spec(opts) do
      %{
        id: __MODULE__,
        start: {__MODULE__, :start_link, [opts]},
        type: :worker,
        restart: :permanent,
        shutdown: 5_000
      }
    end

    @doc &quot;&quot;&quot;
    Starts the KillsPreloader GenServer.

    Options:
      - `:max_concurrency` (integer, default: #{@default_max_concurrency})
    &quot;&quot;&quot;
    @spec start_link(keyword()) :: GenServer.on_start()
    def start_link(opts \\ []) do
      GenServer.start_link(__MODULE__, opts, name: __MODULE__)
    end

    @doc &quot;&quot;&quot;
    Triggers an expanded preload pass (last 24h, limit 100).
    &quot;&quot;&quot;
    @spec run_preload_now() :: :ok
    def run_preload_now do
      GenServer.cast(__MODULE__, :run_expanded_pass)
    end

    @doc &quot;&quot;&quot;
    Adds a system to the active systems list and triggers a preload.
    &quot;&quot;&quot;
    @spec add_system(integer()) :: :ok
    def add_system(system_id) when is_integer(system_id) do
      Logger.info(&quot;Adding system to active list&quot;,
        system_id: system_id,
        operation: :add_system,
        step: :start
      )

      case Helper.system_add_active(system_id) do
        {:ok, :added} -&gt;
          Logger.info(&quot;Successfully added system to active list&quot;,
            system_id: system_id,
            operation: :add_system,
            status: :success
          )

          :ok

        {:ok, :already_exists} -&gt;
          Logger.info(&quot;System already in active list&quot;,
            system_id: system_id,
            operation: :add_system,
            status: :already_exists
          )

          :ok

        {:error, reason} -&gt;
          Logger.error(&quot;Failed to add system to active list&quot;,
            system_id: system_id,
            operation: :add_system,
            error: reason,
            status: :error
          )

          {:error, reason}
      end
    end

    ## GenServer callbacks

    @impl true
    def init(opts) do
      max_concurrency = Keyword.get(opts, :max_concurrency, @default_max_concurrency)

      Logger.info(&quot;Preloader initialized - waiting for subscribers&quot;)
      {:ok, %{max_concurrency: max_concurrency}}
    end

    @impl true
    def handle_cast(:run_expanded_pass, %{max_concurrency: _max} = state) do
      case Helper.system_get_active_systems() do
        {:ok, systems} when is_list(systems) -&gt;
          Logger.info(&quot;Starting preload pass for #{length(systems)} systems&quot;)

          for system_id &lt;- systems do
            Logger.debug(&quot;Processing system in preload pass&quot;, system_id: system_id)
            # Add any system-specific processing here
          end

          {:noreply, state}

        {:error, reason} -&gt;
          Logger.error(&quot;Failed to get active systems: #{inspect(reason)}&quot;)
          {:noreply, state}
      end
    end

    @impl true
    def handle_info({:DOWN, _ref, :process, _pid, reason}, state) do
      # Only log actual crashes, not normal exits or expected errors
      case reason do
        :normal -&gt; :ok
        :no_active_systems -&gt; :ok
        _ -&gt; Logger.error(&quot;[Preloader] Preload task crashed: #{inspect(reason)}&quot;)
      end

      {:noreply, state}
    end

    ## Internal

    @doc &quot;&quot;&quot;
    Spawns a new pass task under the task supervisor.
    &quot;&quot;&quot;
    def spawn_pass(pass_type, max_concurrency) do
      task =
        Task.Supervisor.async_nolink(
          WandererKills.TaskSupervisor,
          fn -&gt; do_pass(pass_type, max_concurrency) end,
          shutdown: :brutal_kill
        )

      # Monitor the task to handle failures
      Process.monitor(task.pid)
      task
    end

    # Perform a preload pass
    defp do_pass(pass_type, max_concurrency) do
      %{hours: hours, limit: limit} = Map.get(@passes, pass_type)

      Logger.info(&quot;Starting #{pass_type} preload pass&quot;,
        hours: hours,
        limit: limit,
        max_concurrency: max_concurrency
      )

      case Helper.system_get_active_systems() do
        {:ok, systems} when is_list(systems) and length(systems) &gt; 0 -&gt;
          Logger.info(&quot;Processing #{length(systems)} active systems&quot;)

          # Take only the limit number of systems for this pass
          systems_to_process = Enum.take(systems, limit)

          # Process systems with limited concurrency
          systems_to_process
          |&gt; Task.async_stream(
            fn system_id -&gt; preload_system(system_id, pass_type) end,
            max_concurrency: max_concurrency,
            timeout: 30_000,
            on_timeout: :kill_task
          )
          |&gt; Enum.each(fn
            {:ok, result} -&gt;
              Logger.debug(&quot;System preload completed&quot;, result: result)

            {:exit, reason} -&gt;
              Logger.warning(&quot;System preload task exited&quot;, reason: reason)
          end)

          Logger.info(&quot;Completed #{pass_type} preload pass&quot;)
          :ok

        {:ok, []} -&gt;
          Logger.info(&quot;No active systems to preload&quot;)
          {:error, :no_active_systems}

        {:error, reason} -&gt;
          Logger.error(&quot;Failed to get active systems for preload&quot;, reason: reason)
          {:error, reason}
      end
    end

    # Preload data for a specific system
    defp preload_system(system_id, pass_type) do
      Logger.debug(&quot;Preloading system&quot;,
        system_id: system_id,
        pass_type: pass_type
      )

      # Fetch kills for this system during preload
      case fetch_and_cache_system_kills(system_id, pass_type) do
        {:ok, kills_count} -&gt;
          Logger.debug(&quot;System preload successful&quot;,
            system_id: system_id,
            kills_count: kills_count
          )

          # Broadcast kill count update after successful preload
          if kills_count &gt; 0 do
            WandererKills.SubscriptionManager.broadcast_kill_count_update(system_id, kills_count)

            Logger.debug(&quot;Broadcasted kill count update from preloader&quot;,
              system_id: system_id,
              count: kills_count
            )
          end

          {:ok, system_id}

        {:error, reason} -&gt;
          Logger.warning(&quot;System preload failed&quot;,
            system_id: system_id,
            reason: reason
          )

          {:error, {system_id, reason}}
      end
    end

    # Fetch and cache kills for a system during preload
    defp fetch_and_cache_system_kills(system_id, pass_type) do
      %{hours: hours, limit: limit} = Map.get(@passes, pass_type)

      case WandererKills.Killmails.ZkbClient.fetch_system_killmails(system_id, limit, hours) do
        {:ok, kills} when is_list(kills) -&gt;
          # Cache the kills (extract killmail IDs first)
          killmail_ids =
            Enum.map(kills, fn kill -&gt; Map.get(kill, &quot;killID&quot;) || Map.get(kill, &quot;killmail_id&quot;) end)
            |&gt; Enum.filter(&amp;(&amp;1 != nil))

          case Helper.system_put_killmails(system_id, killmail_ids) do
            {:ok, _} -&gt;
              Logger.debug(&quot;Cached #{length(kills)} kills for system&quot;,
                system_id: system_id,
                count: length(kills)
              )

              {:ok, length(kills)}

            {:error, reason} -&gt;
              Logger.warning(&quot;Failed to cache kills for system&quot;,
                system_id: system_id,
                error: reason
              )

              {:error, reason}
          end

        {:error, reason} -&gt;
          Logger.warning(&quot;Failed to fetch kills for system&quot;,
            system_id: system_id,
            error: reason
          )

          {:error, reason}
      end
    end
  end
end</file><file path="test/test_helper.exs"># Start test-specific cache instances
:ets.new(:killmails_cache_test, [:named_table, :public, :set])
:ets.new(:system_cache_test, [:named_table, :public, :set])
:ets.new(:esi_cache_test, [:named_table, :public, :set])

# Ensure mox is available
Application.ensure_all_started(:mox)

# Start ExUnit first
ExUnit.start()

# Define mocks
Mox.defmock(WandererKills.Http.Client.Mock,
  for: WandererKills.Behaviours.HttpClient
)

Mox.defmock(WandererKills.Zkb.Client.Mock, for: WandererKills.Killmails.ZkbClientBehaviour)

# Mock for ESI client
Mox.defmock(EsiClientMock, for: WandererKills.Behaviours.ESIClient)

# Start the application for testing
Application.ensure_all_started(:wanderer_kills)

# Create a test case module that provides common setup for all tests
defmodule WandererKills.TestCase do
  use ExUnit.CaseTemplate

  setup do
    # Clear any existing processes and caches
    WandererKills.TestHelpers.clear_all_caches()
    :ok
  end
end

# Set up global mocks - do not stub with real implementation to allow proper mocking in tests
# Mox.stub_with(WandererKills.Http.Client.Mock, WandererKills.Http.Client)
# Mox.stub_with(WandererKills.Zkb.Client.Mock, WandererKills.Killmails.ZkbClient)

# Configure ExUnit to run tests sequentially
ExUnit.configure(parallel: false)

# Set the enricher for tests
ExUnit.configure(enricher: WandererKills.MockEnricher)

# Note: Cache clearing functionality is now available via WandererKills.TestHelpers.clear_all_caches()</file><file path="mix.exs">defmodule WandererKills.MixProject do
  use Mix.Project

  def project do
    [
      app: :wanderer_kills,
      version: &quot;0.1.0&quot;,
      elixir: &quot;~&gt; 1.18&quot;,
      start_permanent: Mix.env() == :prod,
      deps: deps(),
      description:
        &quot;A standalone service for retrieving and caching EVE Online killmails from zKillboard&quot;,
      package: package(),
      elixirc_paths: elixirc_paths(Mix.env()),
      aliases: aliases(),

      # Coverage configuration
      test_coverage: [tool: ExCoveralls],
      preferred_cli_env: [
        coveralls: :test,
        &quot;coveralls.detail&quot;: :test,
        &quot;coveralls.post&quot;: :test,
        &quot;coveralls.html&quot;: :test,
        &quot;coveralls.json&quot;: :test,
        &quot;coveralls.xml&quot;: :test
      ]
    ]
  end

  # The OTP application entrypoint:
  def application do
    [
      extra_applications: [
        :logger,
        :telemetry_poller
      ],
      mod: {WandererKills.Application, []}
    ]
  end

  # Specifies which paths to compile per environment.
  defp elixirc_paths(:test), do: [&quot;lib&quot;, &quot;test/support&quot;]
  defp elixirc_paths(_), do: [&quot;lib&quot;]

  defp deps do
    [
      # HTTP server and routing
      {:plug_cowboy, &quot;~&gt; 2.7&quot;},

      # JSON parsing
      {:jason, &quot;~&gt; 1.4&quot;},

      # Caching
      {:cachex, &quot;~&gt; 4.1&quot;},

      # HTTP client with retry support
      {:req, &quot;~&gt; 0.5&quot;},
      {:backoff, &quot;~&gt; 1.1&quot;},

      # CSV parsing
      {:nimble_csv, &quot;~&gt; 1.2&quot;},

      # Telemetry
      {:telemetry_poller, &quot;~&gt; 1.2&quot;},
      {:uuid, &quot;~&gt; 1.1&quot;},

      # Phoenix PubSub for real-time killmail distribution
      {:phoenix_pubsub, &quot;~&gt; 2.1&quot;},

      # Development and test tools
      {:credo, &quot;~&gt; 1.7.6&quot;, only: [:dev, :test], runtime: false},
      {:dialyxir, &quot;~&gt; 1.4.3&quot;, only: [:dev], runtime: false},
      {:mox, &quot;~&gt; 1.2.0&quot;, only: :test},

      # Code coverage
      {:excoveralls, &quot;~&gt; 0.18&quot;, only: :test}
    ]
  end

  defp package do
    [
      name: &quot;wanderer_kills&quot;,
      licenses: [&quot;MIT&quot;],
      links: %{&quot;GitHub&quot; =&gt; &quot;https://github.com/guarzo/wanderer_kills&quot;}
    ]
  end

  defp aliases do
    [
      test: [&quot;test&quot;],
      check: [
        &quot;format --check-formatted&quot;,
        &quot;credo --strict&quot;,
        &quot;dialyzer&quot;
      ],
      &quot;test.coverage&quot;: [&quot;coveralls.html&quot;],
      &quot;test.coverage.ci&quot;: [&quot;coveralls.json&quot;]
    ]
  end
end</file></files></repomix>